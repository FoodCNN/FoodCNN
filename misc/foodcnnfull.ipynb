{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restarted base (Python 3.12.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3458f026-8297-47cc-ae0c-96505f314f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# %% [markdown]\n",
    "# # Food Nutrition Prediction Model\n",
    "# This notebook implements a CNN-based model for predicting nutritional values from food images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f17e09-cb0b-45c0-b7fc-39e0551fb1de",
   "metadata": {},
   "source": [
    " ## 1. Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72fca2a7-d0ab-4e8b-8140-8c0e8e71f707",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import multiprocessing\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    multiprocessing.set_start_method('spawn', force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8415ac60-6dad-4ecb-a79a-7c130d331eef",
   "metadata": {},
   "source": [
    " ## 2. Configuration and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb77e731-56ee-43ff-b2c2-66fedc477bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_WORKERS = min(int(os.cpu_count() * 0.8), 1)\n",
    "PIN_MEMORY = torch.cuda.is_available()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "LOCAL_BASE_DIR = \"/Data/aa/FoodCNN/datasets/nutrition5k\"\n",
    "IMAGERY_DIR_LOCAL_FULL = os.path.join(LOCAL_BASE_DIR, \"imagery/realsense_overhead\")\n",
    "METADATA_FILE_CAFE1 = os.path.join(LOCAL_BASE_DIR, \"metadata/dish_metadata_cafe1.csv\")\n",
    "METADATA_FILE_CAFE2 = os.path.join(LOCAL_BASE_DIR, \"metadata/dish_metadata_cafe2.csv\")\n",
    "RGB_IMAGE_FILENAME = \"rgb.png\"\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-2\n",
    "NUM_EPOCHS = 50\n",
    "TARGET_COLUMNS = [\"calories\", \"weight\", \"fat\", \"carbs\", \"protein\"]\n",
    "RANDOM_STATE = 42\n",
    "N_SPLITS = 5  # For cross-validation\n",
    "CONFIDENCE_LEVEL = 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df51e9b-979d-4d36-99de-f98bdd74e088",
   "metadata": {},
   "source": [
    " ## 3. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd68b04b-bb60-43ec-90d6-0d1ea0d68db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_nutrition_csv(file_path):\n",
    "    dishes = []\n",
    "    ingredients_list = []\n",
    "\n",
    "    with open(file_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\",\")\n",
    "            if not parts[0].startswith(\"dish_\"):\n",
    "                continue\n",
    "\n",
    "            dish_id = parts[0]\n",
    "            dish_calories = float(parts[1])\n",
    "            dish_weight = float(parts[2])\n",
    "            dish_fat = float(parts[3])\n",
    "            dish_carbs = float(parts[4])\n",
    "            dish_protein = float(parts[5])\n",
    "\n",
    "            dishes.append(\n",
    "                {\n",
    "                    \"dish_id\": dish_id,\n",
    "                    \"calories\": dish_calories,\n",
    "                    \"weight\": dish_weight,\n",
    "                    \"fat\": dish_fat,\n",
    "                    \"carbs\": dish_carbs,\n",
    "                    \"protein\": dish_protein,\n",
    "                }\n",
    "            )\n",
    "\n",
    "            ingredient_data = parts[6:]\n",
    "            i = 0\n",
    "            while i < len(ingredient_data) - 6:\n",
    "                if not ingredient_data[i].startswith(\"ingr_\"):\n",
    "                    break\n",
    "\n",
    "                ingredients_list.append(\n",
    "                    {\n",
    "                        \"dish_id\": dish_id,\n",
    "                        \"ingredient_id\": ingredient_data[i],\n",
    "                        \"ingredient_name\": ingredient_data[i + 1],\n",
    "                        \"amount\": float(ingredient_data[i + 2]),\n",
    "                        \"calories\": float(ingredient_data[i + 3]),\n",
    "                        \"fat\": float(ingredient_data[i + 4]),\n",
    "                        \"carbs\": float(ingredient_data[i + 5]),\n",
    "                        \"protein\": float(ingredient_data[i + 6]),\n",
    "                    }\n",
    "                )\n",
    "                i += 7\n",
    "\n",
    "    dish_df = pd.DataFrame(dishes)\n",
    "    ingredient_df = pd.DataFrame(ingredients_list)\n",
    "\n",
    "    return dish_df, ingredient_df\n",
    "\n",
    "class NutritionDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dish_ids,\n",
    "        labels,\n",
    "        imagery_dir,\n",
    "        rgb_filename,\n",
    "        transform=None,\n",
    "        gpu_cache=False,\n",
    "    ):\n",
    "        self.dish_ids = dish_ids\n",
    "        self.labels = labels\n",
    "        self.imagery_dir = imagery_dir\n",
    "        self.rgb_filename = rgb_filename\n",
    "        self.transform = transform\n",
    "        self.gpu_cache = gpu_cache and torch.cuda.is_available()\n",
    "        \n",
    "        self.image_cache = {}\n",
    "        \n",
    "        if self.gpu_cache:\n",
    "            print(f\"Caching {len(self.dish_ids)} images to GPU VRAM...\")\n",
    "            for dish_id in tqdm(self.dish_ids, desc=\"Caching images to GPU VRAM\"):\n",
    "                self._load_and_cache_image(dish_id)\n",
    "            print(\"Image caching to GPU VRAM completed\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dish_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        dish_id = self.dish_ids[idx]\n",
    "        \n",
    "        if self.gpu_cache and dish_id in self.image_cache:\n",
    "            image_tensor = self.image_cache[dish_id].clone()\n",
    "        else:\n",
    "            image = self._load_and_cache_image(dish_id)\n",
    "            image_tensor = self.transform(image) if self.transform else image\n",
    "        \n",
    "        label_tensor = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        \n",
    "        return image_tensor, label_tensor\n",
    "    \n",
    "    def _load_and_cache_image(self, dish_id):\n",
    "        img_path = os.path.join(self.imagery_dir, dish_id, self.rgb_filename)\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            \n",
    "            if self.gpu_cache:\n",
    "                image_tensor = self.transform(image) if self.transform else transforms.ToTensor()(image)\n",
    "                with torch.no_grad():\n",
    "                    self.image_cache[dish_id] = image_tensor.to(DEVICE, non_blocking=True)\n",
    "                return image\n",
    "            \n",
    "            return image\n",
    "        except FileNotFoundError:\n",
    "            print(f\"ERROR: Image not found at {img_path} for dish_id {dish_id}\")\n",
    "            dummy_img = Image.new(\"RGB\", (224, 224), color=(0, 0, 0))\n",
    "            \n",
    "            if self.gpu_cache:\n",
    "                dummy_tensor = self.transform(dummy_img) if self.transform else transforms.ToTensor()(dummy_img)\n",
    "                with torch.no_grad():\n",
    "                    self.image_cache[dish_id] = dummy_tensor.to(DEVICE, non_blocking=True)\n",
    "                \n",
    "            return dummy_img\n",
    "\n",
    "def check_memory_for_caching(num_images, image_size=224):\n",
    "    try:\n",
    "        gpu_caching_viable = False\n",
    "        if torch.cuda.is_available():\n",
    "            tensor_size_mb = num_images * image_size * image_size * 3 * 4 / (1024 * 1024)\n",
    "            \n",
    "            available_gpu_memory_mb = torch.cuda.get_device_properties(0).total_memory / (1024 * 1024)\n",
    "            free_gpu_memory_mb = available_gpu_memory_mb - (torch.cuda.memory_allocated(0) / (1024 * 1024))\n",
    "            \n",
    "            usable_gpu_memory_mb = free_gpu_memory_mb * 0.7\n",
    "            \n",
    "            gpu_caching_viable = usable_gpu_memory_mb > tensor_size_mb\n",
    "            \n",
    "            print(f\"Available GPU memory: {free_gpu_memory_mb:.2f} MB\")\n",
    "            print(f\"Estimated GPU cache size: {tensor_size_mb:.2f} MB\")\n",
    "            \n",
    "        if gpu_caching_viable:\n",
    "            print(\"✓ Sufficient GPU memory available, enabling GPU VRAM caching\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"✗ Insufficient GPU memory for image caching\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error checking memory: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b595f6e-2593-4a1d-b4f5-3ec515dadaa8",
   "metadata": {},
   "source": [
    " ## 4. Model Architecture Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9eba5ec2-bcc4-44b4-8ecf-ceda4a3b8980",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineCNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_outputs):\n",
    "        super(BaselineCNN, self).__init__()\n",
    "        self.name = \"Baseline CNN\"\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        \n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "        )\n",
    "\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, num_outputs),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "    \n",
    "class SimpleBaseline(nn.Module):\n",
    "    \"\"\"Simple baseline that predicts mean values.\"\"\"\n",
    "    def __init__(self, num_outputs, mean_values):\n",
    "        super(SimpleBaseline, self).__init__()\n",
    "        self.name = \"Mean Baseline\"\n",
    "        self.register_buffer('mean_values', torch.tensor(mean_values, dtype=torch.float32))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        return self.mean_values.unsqueeze(0).repeat(batch_size, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb72078d-46fa-4668-91e4-09131df4a149",
   "metadata": {},
   "source": [
    " ## 5. Training and Evaluation Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b10a379b-0f3a-4263-a1a9-9124843ead35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        test_loader,  # Added test loader\n",
    "        device,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        num_epochs=NUM_EPOCHS,\n",
    "        patience=5,\n",
    "    ):\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.device = device\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.models_results = {}\n",
    "        self.patience = patience\n",
    "        \n",
    "    def compute_comprehensive_metrics(self, predictions, ground_truth):\n",
    "        \"\"\"Compute comprehensive evaluation metrics with confidence intervals.\"\"\"\n",
    "        n_samples = len(predictions)\n",
    "        metrics = {}\n",
    "        \n",
    "        for i, target in enumerate(TARGET_COLUMNS):\n",
    "            pred_col = predictions[:, i]\n",
    "            true_col = ground_truth[:, i]\n",
    "            \n",
    "            # Basic metrics\n",
    "            mae = mean_absolute_error(true_col, pred_col)\n",
    "            rmse = np.sqrt(mean_squared_error(true_col, pred_col))\n",
    "            r2 = r2_score(true_col, pred_col)\n",
    "            \n",
    "            # Mean Absolute Percentage Error (MAPE)\n",
    "            mape = np.mean(np.abs((true_col - pred_col) / np.maximum(np.abs(true_col), 1e-8))) * 100\n",
    "            \n",
    "            # Bootstrap confidence intervals for MAE\n",
    "            bootstrap_maes = []\n",
    "            n_bootstrap = 1000\n",
    "            for _ in range(n_bootstrap):\n",
    "                indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "                bootstrap_mae = mean_absolute_error(true_col[indices], pred_col[indices])\n",
    "                bootstrap_maes.append(bootstrap_mae)\n",
    "            \n",
    "            alpha = 1 - CONFIDENCE_LEVEL\n",
    "            mae_ci_lower = np.percentile(bootstrap_maes, (alpha/2) * 100)\n",
    "            mae_ci_upper = np.percentile(bootstrap_maes, (1 - alpha/2) * 100)\n",
    "            \n",
    "            metrics[target] = {\n",
    "                'mae': mae,\n",
    "                'rmse': rmse,\n",
    "                'r2': r2,\n",
    "                'mape': mape,\n",
    "                'mae_ci_lower': mae_ci_lower,\n",
    "                'mae_ci_upper': mae_ci_upper\n",
    "            }\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def statistical_significance_test(self, model1_preds, model2_preds, ground_truth):\n",
    "        \"\"\"Perform statistical significance test between two models.\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        for i, target in enumerate(TARGET_COLUMNS):\n",
    "            true_col = ground_truth[:, i]\n",
    "            \n",
    "            # Compute absolute errors for both models\n",
    "            errors1 = np.abs(model1_preds[:, i] - true_col)\n",
    "            errors2 = np.abs(model2_preds[:, i] - true_col)\n",
    "            \n",
    "            # Paired t-test on absolute errors\n",
    "            t_stat, p_value = stats.ttest_rel(errors1, errors2)\n",
    "            \n",
    "            # Effect size (Cohen's d)\n",
    "            diff = errors1 - errors2\n",
    "            pooled_std = np.sqrt((np.var(errors1, ddof=1) + np.var(errors2, ddof=1)) / 2)\n",
    "            cohens_d = np.mean(diff) / pooled_std if pooled_std > 0 else 0\n",
    "            \n",
    "            results[target] = {\n",
    "                't_statistic': t_stat,\n",
    "                'p_value': p_value,\n",
    "                'cohens_d': cohens_d,\n",
    "                'significant': p_value < 0.05,\n",
    "                'mean_diff': np.mean(diff)\n",
    "            }\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def train_and_evaluate(self, model):\n",
    "        model_name = model.name\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Training {model_name}\")\n",
    "        print(f\"{'='*50}\\n\")\n",
    "\n",
    "        model = model.to(self.device)\n",
    "        \n",
    "        # Skip training for baseline models\n",
    "        if 'Baseline' in model_name:\n",
    "            train_losses, val_losses = [], []\n",
    "        else:\n",
    "            criterion = nn.MSELoss()\n",
    "            optimizer = optim.Adam(model.parameters(), lr=self.learning_rate)\n",
    "            scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer, mode=\"min\", factor=0.5, patience=3, verbose=True\n",
    "            )\n",
    "            train_losses, val_losses = self._train_model(\n",
    "                model, criterion, optimizer, scheduler\n",
    "            )\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        val_predictions, val_ground_truth, _ = self._evaluate_model(model, self.val_loader)\n",
    "        val_metrics = self.compute_comprehensive_metrics(val_predictions, val_ground_truth)\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        test_predictions, test_ground_truth, _ = self._evaluate_model(model, self.test_loader)\n",
    "        test_metrics = self.compute_comprehensive_metrics(test_predictions, test_ground_truth)\n",
    "\n",
    "        self.models_results[model_name] = {\n",
    "            \"model\": model,\n",
    "            \"train_losses\": train_losses,\n",
    "            \"val_losses\": val_losses,\n",
    "            \"val_metrics\": val_metrics,\n",
    "            \"test_metrics\": test_metrics,\n",
    "            \"val_predictions\": val_predictions,\n",
    "            \"test_predictions\": test_predictions,\n",
    "            \"val_ground_truth\": val_ground_truth,\n",
    "            \"test_ground_truth\": test_ground_truth\n",
    "        }\n",
    "\n",
    "        self._display_comprehensive_results(model_name, val_metrics, test_metrics)\n",
    "        if 'Baseline' not in model_name:\n",
    "            self._save_model(model, model_name)\n",
    "\n",
    "        return self.models_results[model_name]\n",
    "\n",
    "    def train_multiple_models(self, models):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Training {len(models)} model architectures\")\n",
    "        print(f\"{'='*50}\")\n",
    "\n",
    "        model_pbar = tqdm(\n",
    "            models, desc=\"Training models\", leave=True, dynamic_ncols=True\n",
    "        )\n",
    "        for model in model_pbar:\n",
    "            model_pbar.set_description(f\"Training {model.name}\")\n",
    "            self.train_and_evaluate(model)\n",
    "\n",
    "        return self.models_results\n",
    "\n",
    "    def _train_model(self, model, criterion, optimizer, scheduler):\n",
    "        best_val_loss = float(\"inf\")\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        no_improve_count = 0\n",
    "\n",
    "        for epoch in range(self.num_epochs):\n",
    "            model.train()\n",
    "            train_loss = 0.0\n",
    "\n",
    "            train_pbar = tqdm(\n",
    "                self.train_loader,\n",
    "                desc=f\"Epoch {epoch+1}/{self.num_epochs} [Train]\",\n",
    "                leave=False,\n",
    "                dynamic_ncols=True,\n",
    "            )\n",
    "\n",
    "            for inputs, labels in train_pbar:\n",
    "                inputs = inputs.to(self.device, non_blocking=PIN_MEMORY)\n",
    "                labels = labels.to(self.device, non_blocking=PIN_MEMORY)\n",
    "\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                batch_loss = loss.item() * inputs.size(0)\n",
    "                train_loss += batch_loss\n",
    "                train_pbar.set_postfix({\"loss\": f\"{batch_loss/inputs.size(0):.4f}\"})\n",
    "\n",
    "            train_loss = train_loss / len(self.train_loader.dataset)\n",
    "            train_losses.append(train_loss)\n",
    "\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            val_pbar = tqdm(\n",
    "                self.val_loader,\n",
    "                desc=f\"Epoch {epoch+1}/{self.num_epochs} [Valid]\",\n",
    "                leave=False,\n",
    "                dynamic_ncols=True,\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in val_pbar:\n",
    "                    inputs = inputs.to(self.device, non_blocking=PIN_MEMORY)\n",
    "                    labels = labels.to(self.device, non_blocking=PIN_MEMORY)\n",
    "\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    batch_loss = loss.item() * inputs.size(0)\n",
    "                    val_loss += batch_loss\n",
    "                    val_pbar.set_postfix({\"loss\": f\"{batch_loss/inputs.size(0):.4f}\"})\n",
    "\n",
    "            val_loss = val_loss / len(self.val_loader.dataset)\n",
    "            val_losses.append(val_loss)\n",
    "\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "            if epoch > 0:\n",
    "                train_improve = train_losses[epoch - 1] - train_loss\n",
    "                val_improve = val_losses[epoch - 1] - val_loss\n",
    "                train_indicator = (\n",
    "                    f\"↓ {train_improve:.4f}\"\n",
    "                    if train_improve > 0\n",
    "                    else f\"↑ {-train_improve:.4f}\"\n",
    "                )\n",
    "                val_indicator = (\n",
    "                    f\"↓ {val_improve:.4f}\"\n",
    "                    if val_improve > 0\n",
    "                    else f\"↑ {-val_improve:.4f}\"\n",
    "                )\n",
    "            else:\n",
    "                train_indicator = \"---\"\n",
    "                val_indicator = \"---\"\n",
    "\n",
    "            is_best = val_loss < best_val_loss\n",
    "\n",
    "            if is_best:\n",
    "                best_val_loss = val_loss\n",
    "                self._save_model(model, f\"{model.name}_best\")\n",
    "                no_improve_count = 0\n",
    "                best_indicator = \"✓ BEST\"\n",
    "            else:\n",
    "                no_improve_count += 1\n",
    "                best_indicator = \"\"\n",
    "\n",
    "            early_stop_msg = (\n",
    "                f\"| Early stop: {no_improve_count}/{self.patience}\"\n",
    "                if no_improve_count > 0\n",
    "                else \"\"\n",
    "            )\n",
    "\n",
    "            print(\n",
    "                f\"Epoch {epoch+1}/{self.num_epochs} | Train Loss: {train_loss:.4f} ({train_indicator}) | Val Loss: {val_loss:.4f} ({val_indicator}) {best_indicator} {early_stop_msg}\"\n",
    "            )\n",
    "\n",
    "            if no_improve_count >= self.patience:\n",
    "                print(\n",
    "                    f\"Early stopping triggered after {epoch+1} epochs. No improvement for {self.patience} epochs.\"\n",
    "                )\n",
    "                break\n",
    "\n",
    "        return train_losses, val_losses\n",
    "\n",
    "    def _evaluate_model(self, model, data_loader):\n",
    "        model.eval()\n",
    "        all_predictions = []\n",
    "        all_ground_truth = []\n",
    "\n",
    "        eval_pbar = tqdm(\n",
    "            data_loader, desc=\"Evaluating\", leave=False, dynamic_ncols=True\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in eval_pbar:\n",
    "                inputs = inputs.to(self.device, non_blocking=PIN_MEMORY)\n",
    "                targets = targets.to(self.device, non_blocking=PIN_MEMORY)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                all_predictions.append(outputs.cpu().numpy())\n",
    "                all_ground_truth.append(targets.cpu().numpy())\n",
    "\n",
    "        all_predictions = np.vstack(all_predictions)\n",
    "        all_ground_truth = np.vstack(all_ground_truth)\n",
    "        mae_values = np.mean(np.abs(all_predictions - all_ground_truth), axis=0)\n",
    "\n",
    "        return all_predictions, all_ground_truth, mae_values\n",
    "\n",
    "    def _display_comprehensive_results(self, model_name, val_metrics, test_metrics):\n",
    "        \"\"\"Display comprehensive results including confidence intervals.\"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Results for {model_name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Create comprehensive results table\n",
    "        results_data = []\n",
    "        for target in TARGET_COLUMNS:\n",
    "            val_m = val_metrics[target]\n",
    "            test_m = test_metrics[target]\n",
    "            \n",
    "            results_data.append({\n",
    "                'Target': target,\n",
    "                'Val_MAE': f\"{val_m['mae']:.3f}\",\n",
    "                'Val_MAE_CI': f\"[{val_m['mae_ci_lower']:.3f}, {val_m['mae_ci_upper']:.3f}]\",\n",
    "                'Test_MAE': f\"{test_m['mae']:.3f}\",\n",
    "                'Test_MAE_CI': f\"[{test_m['mae_ci_lower']:.3f}, {test_m['mae_ci_upper']:.3f}]\",\n",
    "                'Test_RMSE': f\"{test_m['rmse']:.3f}\",\n",
    "                'Test_R2': f\"{test_m['r2']:.3f}\",\n",
    "                'Test_MAPE': f\"{test_m['mape']:.1f}%\"\n",
    "            })\n",
    "        \n",
    "        results_df = pd.DataFrame(results_data)\n",
    "        print(\"\\nDetailed Metrics:\")\n",
    "        print(results_df.to_string(index=False))\n",
    "        \n",
    "        # Summary statistics\n",
    "        val_mae_overall = np.mean([val_metrics[t]['mae'] for t in TARGET_COLUMNS])\n",
    "        test_mae_overall = np.mean([test_metrics[t]['mae'] for t in TARGET_COLUMNS])\n",
    "        test_r2_overall = np.mean([test_metrics[t]['r2'] for t in TARGET_COLUMNS])\n",
    "        \n",
    "        print(f\"\\nSummary:\")\n",
    "        print(f\"Validation MAE (overall): {val_mae_overall:.3f}\")\n",
    "        print(f\"Test MAE (overall): {test_mae_overall:.3f}\")\n",
    "        print(f\"Test R² (overall): {test_r2_overall:.3f}\")\n",
    "\n",
    "    def compare_models_statistically(self):\n",
    "        \"\"\"Compare models with statistical significance tests.\"\"\"\n",
    "        model_names = list(self.models_results.keys())\n",
    "        \n",
    "        if len(model_names) < 2:\n",
    "            print(\"Need at least 2 models for comparison.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"Statistical Significance Tests (Test Set)\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        for i in range(len(model_names)):\n",
    "            for j in range(i+1, len(model_names)):\n",
    "                model1 = model_names[i]\n",
    "                model2 = model_names[j]\n",
    "                \n",
    "                preds1 = self.models_results[model1]['test_predictions']\n",
    "                preds2 = self.models_results[model2]['test_predictions']\n",
    "                ground_truth = self.models_results[model1]['test_ground_truth']\n",
    "                \n",
    "                sig_results = self.statistical_significance_test(preds1, preds2, ground_truth)\n",
    "                \n",
    "                print(f\"\\n{model1} vs {model2}:\")\n",
    "                for target in TARGET_COLUMNS:\n",
    "                    result = sig_results[target]\n",
    "                    significance = \"***\" if result['p_value'] < 0.001 else \"**\" if result['p_value'] < 0.01 else \"*\" if result['p_value'] < 0.05 else \"ns\"\n",
    "                    print(f\"  {target}: p={result['p_value']:.4f} {significance}, d={result['cohens_d']:.3f}, Δ={result['mean_diff']:.3f}\")\n",
    "\n",
    "    def compare_models(self):\n",
    "        self._compare_losses()\n",
    "        self._compare_mae()\n",
    "        self._rank_models()\n",
    "\n",
    "    def _compare_losses(self):\n",
    "        plt.figure(figsize=(15, 6))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        for name, results in self.models_results.items():\n",
    "            plt.plot(results[\"train_losses\"], label=name)\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Training Loss Comparison\")\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        for name, results in self.models_results.items():\n",
    "            plt.plot(results[\"val_losses\"], label=name)\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Validation Loss Comparison\")\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"model_comparison_loss.png\")\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "    def _compare_mae(self):\n",
    "        model_names = list(self.models_results.keys())\n",
    "        model_maes = [results[\"mae_values\"] for results in self.models_results.values()]\n",
    "        \n",
    "        comparison_df = pd.DataFrame({\"Target\": TARGET_COLUMNS})\n",
    "        \n",
    "        for i, name in enumerate(model_names):\n",
    "            comparison_df[name] = model_maes[i]\n",
    "        \n",
    "        overall_row = pd.DataFrame({\"Target\": [\"Overall MAE\"]})\n",
    "        for name in model_names:\n",
    "            overall_row[name] = [self.models_results[name][\"overall_mae\"]]\n",
    "        \n",
    "        comparison_df = pd.concat([comparison_df, overall_row], ignore_index=True)\n",
    "        \n",
    "        print(\"MAE Comparison across models:\")\n",
    "        print(comparison_df)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        x = np.arange(len(TARGET_COLUMNS))\n",
    "        width = 0.8 / len(model_names) if len(model_names) > 0 else 0.4\n",
    "        \n",
    "        for i, name in enumerate(model_names):\n",
    "            offset = (i - len(model_names) / 2 + 0.5) * width\n",
    "            plt.bar(x + offset, model_maes[i], width, label=name)\n",
    "        \n",
    "        plt.xlabel(\"Target\")\n",
    "        plt.ylabel(\"Mean Absolute Error\")\n",
    "        plt.title(\"MAE Comparison Between Models\")\n",
    "        plt.xticks(x, TARGET_COLUMNS)\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"model_comparison_mae.png\")\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "    def _rank_models(self):\n",
    "        ranked_models = sorted(\n",
    "            self.models_results.items(), key=lambda x: x[1][\"overall_mae\"]\n",
    "        )\n",
    "        \n",
    "        print(\"\\nModel Ranking (based on overall MAE):\")\n",
    "        for i, (name, results) in enumerate(ranked_models):\n",
    "            print(f\"{i+1}. {name}: {results['overall_mae']:.2f}\")\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        names = [name for name, _ in ranked_models]\n",
    "        maes = [results[\"overall_mae\"] for _, results in ranked_models]\n",
    "        \n",
    "        plt.bar(names, maes)\n",
    "        plt.xlabel(\"Model\")\n",
    "        plt.ylabel(\"Overall MAE\")\n",
    "        plt.title(\"Model Ranking by Overall MAE\")\n",
    "        plt.xticks(rotation=45, ha=\"right\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"model_ranking.png\")\n",
    "        plt.show()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e64cb68-66a1-438e-b372-37a52bc2ca80",
   "metadata": {},
   "source": [
    " ## 6. Visualization Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78c2ebfb-6562-4ac5-8b0f-cf2e2e6fa288",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(model, data_loader, device, num_samples=5):\n",
    "    model.eval()\n",
    "    \n",
    "    dataiter = iter(data_loader)\n",
    "    images, targets = next(dataiter)\n",
    "    \n",
    "    images = images[:num_samples]\n",
    "    targets = targets[:num_samples]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        predictions = model(images.to(device)).cpu().numpy()\n",
    "    \n",
    "    for i in range(len(images)):\n",
    "        img_tensor = images[i].cpu() if images[i].device.type != 'cpu' else images[i]\n",
    "        \n",
    "        img = img_tensor.permute(1, 2, 0).numpy()\n",
    "        \n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        img = std * img + mean\n",
    "        img = np.clip(img, 0, 1)\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        ax1.imshow(img)\n",
    "        ax1.set_title(\"Food Image\")\n",
    "        ax1.axis(\"off\")\n",
    "        \n",
    "        target = targets[i].cpu().numpy() if targets[i].device.type != 'cpu' else targets[i].numpy()\n",
    "        pred = predictions[i]\n",
    "        \n",
    "        abs_errors = np.abs(pred - target).round(1)\n",
    "        error_pcts = np.zeros_like(target)\n",
    "        \n",
    "        for j in range(len(target)):\n",
    "            if abs(target[j]) > 1e-7:\n",
    "                error_pcts[j] = (abs_errors[j] / abs(target[j]) * 100).round(1)\n",
    "            else:\n",
    "                if abs(pred[j]) < 1e-5:\n",
    "                    error_pcts[j] = 0\n",
    "                else:\n",
    "                    error_pcts[j] = 100\n",
    "        \n",
    "        comparison = pd.DataFrame(\n",
    "            {\n",
    "                \"Target\": TARGET_COLUMNS,\n",
    "                \"Ground Truth\": target.round(1),\n",
    "                \"Prediction\": pred.round(1),\n",
    "                \"Absolute Error\": abs_errors,\n",
    "                \"Error %\": error_pcts,\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        ax2.axis(\"tight\")\n",
    "        ax2.axis(\"off\")\n",
    "        table = ax2.table(\n",
    "            cellText=comparison.values,\n",
    "            colLabels=comparison.columns,\n",
    "            rowLabels=None,\n",
    "            cellLoc=\"center\",\n",
    "            loc=\"center\",\n",
    "        )\n",
    "        table.auto_set_font_size(False)\n",
    "        table.set_fontsize(9)\n",
    "        table.scale(1.2, 1.5)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "def plot_model_comparison_with_ci(results):\n",
    "    \"\"\"Plot model comparison with confidence intervals.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    model_names = list(results.keys())\n",
    "    colors = plt.cm.Set1(np.linspace(0, 1, len(model_names)))\n",
    "    \n",
    "    for i, target in enumerate(TARGET_COLUMNS):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        x_pos = np.arange(len(model_names))\n",
    "        maes = []\n",
    "        ci_lowers = []\n",
    "        ci_uppers = []\n",
    "        \n",
    "        for model_name in model_names:\n",
    "            metrics = results[model_name]['test_metrics'][target]\n",
    "            maes.append(metrics['mae'])\n",
    "            ci_lowers.append(metrics['mae_ci_lower'])\n",
    "            ci_uppers.append(metrics['mae_ci_upper'])\n",
    "        \n",
    "        # Error bars\n",
    "        yerr_lower = np.array(maes) - np.array(ci_lowers)\n",
    "        yerr_upper = np.array(ci_uppers) - np.array(maes)\n",
    "        \n",
    "        bars = ax.bar(x_pos, maes, yerr=[yerr_lower, yerr_upper], \n",
    "                     capsize=5, color=colors, alpha=0.7)\n",
    "        \n",
    "        ax.set_title(f'{target.title()} - MAE with 95% CI')\n",
    "        ax.set_xlabel('Model')\n",
    "        ax.set_ylabel('Mean Absolute Error')\n",
    "        ax.set_xticks(x_pos)\n",
    "        ax.set_xticklabels(model_names, rotation=45, ha='right')\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for j, (bar, mae) in enumerate(zip(bars, maes)):\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + yerr_upper[j] + 0.01,\n",
    "                   f'{mae:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    # Remove empty subplot\n",
    "    if len(TARGET_COLUMNS) < len(axes):\n",
    "        axes[-1].remove()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('model_comparison_with_ci.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88650a79-6a22-4628-bcde-796ca033821b",
   "metadata": {},
   "source": [
    " ## 7. Main Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40491ddc-bb97-4765-ba88-7830a9b91fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 3490\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'stratified_split_by_bins' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 209\u001b[39m\n\u001b[32m    206\u001b[39m     visualize_predictions(best_model, test_loader, DEVICE, num_samples=\u001b[32m3\u001b[39m)\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m209\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 37\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# Improved data splitting with stratification\u001b[39;00m\n\u001b[32m     36\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTotal samples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(filtered_metadata_df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m train_df, val_df, test_df = \u001b[43mstratified_split_by_bins\u001b[49m(\n\u001b[32m     38\u001b[39m     filtered_metadata_df, TARGET_COLUMNS, \n\u001b[32m     39\u001b[39m     test_size=\u001b[32m0.2\u001b[39m, val_size=\u001b[32m0.2\u001b[39m, random_state=RANDOM_STATE\n\u001b[32m     40\u001b[39m )\n\u001b[32m     42\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTrain samples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_df)/\u001b[38;5;28mlen\u001b[39m(filtered_metadata_df)*\u001b[32m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m%)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     43\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mValidation samples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(val_df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(val_df)/\u001b[38;5;28mlen\u001b[39m(filtered_metadata_df)*\u001b[32m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m%)\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'stratified_split_by_bins' is not defined"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Set random seeds for reproducibility\n",
    "    torch.manual_seed(RANDOM_STATE)\n",
    "    np.random.seed(RANDOM_STATE)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(RANDOM_STATE)\n",
    "    \n",
    "    # Data loading and preparation\n",
    "    dish_df_cafe1, ingredient_df_cafe1 = parse_nutrition_csv(METADATA_FILE_CAFE1)\n",
    "    dish_df_cafe2, ingredient_df_cafe2 = parse_nutrition_csv(METADATA_FILE_CAFE2)\n",
    "\n",
    "    dish_metadata_df = pd.concat([dish_df_cafe1, dish_df_cafe2], ignore_index=True)\n",
    "    ingredient_metadata_df = pd.concat(\n",
    "        [ingredient_df_cafe1, ingredient_df_cafe2], ignore_index=True\n",
    "    )\n",
    "\n",
    "    available_dish_ids_in_imagery = []\n",
    "    for dish_id_folder in os.listdir(IMAGERY_DIR_LOCAL_FULL):\n",
    "        rgb_path = os.path.join(IMAGERY_DIR_LOCAL_FULL, dish_id_folder, RGB_IMAGE_FILENAME)\n",
    "        if os.path.isdir(\n",
    "            os.path.join(IMAGERY_DIR_LOCAL_FULL, dish_id_folder)\n",
    "        ) and os.path.exists(rgb_path):\n",
    "            available_dish_ids_in_imagery.append(dish_id_folder)\n",
    "\n",
    "    filtered_metadata_df = dish_metadata_df[\n",
    "        dish_metadata_df[\"dish_id\"].isin(available_dish_ids_in_imagery)\n",
    "    ].copy()\n",
    "\n",
    "    for col in TARGET_COLUMNS:\n",
    "        filtered_metadata_df[col] = pd.to_numeric(\n",
    "            filtered_metadata_df[col], errors=\"coerce\"\n",
    "        )\n",
    "    filtered_metadata_df.dropna(subset=TARGET_COLUMNS, inplace=True)\n",
    "\n",
    "    # Improved data splitting with stratification\n",
    "    print(f\"Total samples: {len(filtered_metadata_df)}\")\n",
    "    train_df, val_df, test_df = stratified_split_by_bins(\n",
    "        filtered_metadata_df, TARGET_COLUMNS, \n",
    "        test_size=0.2, val_size=0.2, random_state=RANDOM_STATE\n",
    "    )\n",
    "    \n",
    "    print(f\"Train samples: {len(train_df)} ({len(train_df)/len(filtered_metadata_df)*100:.1f}%)\")\n",
    "    print(f\"Validation samples: {len(val_df)} ({len(val_df)/len(filtered_metadata_df)*100:.1f}%)\")\n",
    "    print(f\"Test samples: {len(test_df)} ({len(test_df)/len(filtered_metadata_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Extract data for datasets\n",
    "    train_dish_ids = train_df[\"dish_id\"].tolist()\n",
    "    val_dish_ids = val_df[\"dish_id\"].tolist()\n",
    "    test_dish_ids = test_df[\"dish_id\"].tolist()\n",
    "    \n",
    "    train_labels = train_df[TARGET_COLUMNS].values.astype(np.float32)\n",
    "    val_labels = val_df[TARGET_COLUMNS].values.astype(np.float32)\n",
    "    test_labels = test_df[TARGET_COLUMNS].values.astype(np.float32)\n",
    "    \n",
    "    # Compute normalization statistics from training set only\n",
    "    train_mean = np.mean(train_labels, axis=0)\n",
    "    train_std = np.std(train_labels, axis=0)\n",
    "    \n",
    "    print(f\"\\nTarget statistics (training set):\")\n",
    "    stats_df = pd.DataFrame({\n",
    "        'Target': TARGET_COLUMNS,\n",
    "        'Mean': train_mean,\n",
    "        'Std': train_std,\n",
    "        'Min': np.min(train_labels, axis=0),\n",
    "        'Max': np.max(train_labels, axis=0)\n",
    "    })\n",
    "    print(stats_df)\n",
    "\n",
    "    data_transforms = {\n",
    "        \"train\": transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.RandomRotation(15),\n",
    "                transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
    "                ),\n",
    "            ]\n",
    "        ),\n",
    "        \"val\": transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
    "                ),\n",
    "            ]\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        can_gpu_cache = check_memory_for_caching(len(train_dish_ids + val_dish_ids + test_dish_ids))\n",
    "    except:\n",
    "        can_gpu_cache = False\n",
    "        print(\"Error checking memory. Disabling GPU image caching.\")\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = NutritionDataset(\n",
    "        dish_ids=train_dish_ids,\n",
    "        labels=train_labels,\n",
    "        imagery_dir=IMAGERY_DIR_LOCAL_FULL,\n",
    "        rgb_filename=RGB_IMAGE_FILENAME,\n",
    "        transform=data_transforms[\"train\"],\n",
    "        gpu_cache=can_gpu_cache,\n",
    "    )\n",
    "    \n",
    "    val_dataset = NutritionDataset(\n",
    "        dish_ids=val_dish_ids,\n",
    "        labels=val_labels,\n",
    "        imagery_dir=IMAGERY_DIR_LOCAL_FULL,\n",
    "        rgb_filename=RGB_IMAGE_FILENAME,\n",
    "        transform=data_transforms[\"val\"],\n",
    "        gpu_cache=can_gpu_cache,\n",
    "    )\n",
    "    \n",
    "    test_dataset = NutritionDataset(\n",
    "        dish_ids=test_dish_ids,\n",
    "        labels=test_labels,\n",
    "        imagery_dir=IMAGERY_DIR_LOCAL_FULL,\n",
    "        rgb_filename=RGB_IMAGE_FILENAME,\n",
    "        transform=data_transforms[\"val\"],\n",
    "        gpu_cache=can_gpu_cache,\n",
    "    )\n",
    "\n",
    "    if can_gpu_cache:\n",
    "        loader_workers = 0\n",
    "        print(\"Using 0 workers for DataLoader due to GPU caching\")\n",
    "    else:\n",
    "        loader_workers = NUM_WORKERS\n",
    "        print(f\"Using {loader_workers} workers for DataLoader\")\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=True, \n",
    "        num_workers=loader_workers,\n",
    "        pin_memory=PIN_MEMORY and not can_gpu_cache,\n",
    "        persistent_workers=loader_workers > 0,\n",
    "        prefetch_factor=2 if loader_workers > 0 else None,\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=False, \n",
    "        num_workers=loader_workers,\n",
    "        pin_memory=PIN_MEMORY and not can_gpu_cache,\n",
    "        persistent_workers=loader_workers > 0,\n",
    "        prefetch_factor=2 if loader_workers > 0 else None,\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=loader_workers,\n",
    "        pin_memory=PIN_MEMORY and not can_gpu_cache,\n",
    "        persistent_workers=loader_workers > 0,\n",
    "        prefetch_factor=2 if loader_workers > 0 else None,\n",
    "    )\n",
    "\n",
    "    # Define models including baselines\n",
    "    MODELS_TO_TEST = [\n",
    "        SimpleBaseline(num_outputs=len(TARGET_COLUMNS), mean_values=train_mean),\n",
    "        BaselineCNN(num_outputs=len(TARGET_COLUMNS)),\n",
    "    ]\n",
    "\n",
    "    trainer = ModelTrainer(\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader,\n",
    "        device=DEVICE,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        num_epochs=NUM_EPOCHS,\n",
    "        patience=7,\n",
    "    )\n",
    "\n",
    "    results = trainer.train_multiple_models(MODELS_TO_TEST)\n",
    "\n",
    "    print(\"\\nComparing model performance:\")\n",
    "    trainer.compare_models()\n",
    "    \n",
    "    print(\"\\nPerforming statistical significance tests:\")\n",
    "    trainer.compare_models_statistically()\n",
    "    \n",
    "    # Enhanced visualizations\n",
    "    plot_model_comparison_with_ci(results)\n",
    "\n",
    "    # Select best model based on validation performance\n",
    "    best_model_name = min(results.items(), \n",
    "                         key=lambda x: np.mean([x[1]['val_metrics'][t]['mae'] for t in TARGET_COLUMNS]))[0]\n",
    "    best_model = results[best_model_name][\"model\"]\n",
    "\n",
    "    print(f\"\\nBest model (validation): {best_model_name}\")\n",
    "    print(f\"Final test performance:\")\n",
    "    test_metrics = results[best_model_name]['test_metrics']\n",
    "    for target in TARGET_COLUMNS:\n",
    "        m = test_metrics[target]\n",
    "        print(f\"  {target}: MAE={m['mae']:.3f} [CI: {m['mae_ci_lower']:.3f}-{m['mae_ci_upper']:.3f}], R²={m['r2']:.3f}\")\n",
    "    \n",
    "    # Visualize predictions on test set\n",
    "    print(f\"\\nVisualizing predictions for {best_model_name} on test set:\")\n",
    "    visualize_predictions(best_model, test_loader, DEVICE, num_samples=3)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
