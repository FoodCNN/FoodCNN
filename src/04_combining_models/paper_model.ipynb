{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# DEVICE = torch.device(\"mps\" if torch.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOCAL_BASE_DIR = \"/Users/georgiikuznetsov/Desktop/coding/CNN_nutrition/nutrition5k\"\n",
    "LOCAL_BASE_DIR = \"/users/eleves-b/2023/georgii.kuznetsov/CNN_nutrition/nutrition5k\"\n",
    "\n",
    "IMAGERY_DIR = os.path.join(LOCAL_BASE_DIR, \"imagery/realsense_overhead\")\n",
    "METADATA_FILE_CAFE1 = os.path.join(LOCAL_BASE_DIR, \"metadata/dish_metadata_cafe1.csv\")\n",
    "METADATA_FILE_CAFE2 = os.path.join(LOCAL_BASE_DIR, \"metadata/dish_metadata_cafe2.csv\")\n",
    "\n",
    "assert(os.path.exists(LOCAL_BASE_DIR))\n",
    "assert(os.path.exists(IMAGERY_DIR))\n",
    "assert(os.path.exists(METADATA_FILE_CAFE1))\n",
    "assert(os.path.exists(METADATA_FILE_CAFE2))\n",
    "\n",
    "RGB_IMAGE_FILENAME = \"rgb.png\" \n",
    "\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_EPOCHS = 10\n",
    "TARGET_COLUMNS = ['calories', 'weight', 'fat', 'carbs', 'protein']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Simplified Multi-Task CNN Model\n",
    "class SimpleNutritionNet(nn.Module):\n",
    "    def __init__(self, num_outputs=5, backbone='resnet34', pretrained=True):\n",
    "        super(SimpleNutritionNet, self).__init__()\n",
    "        \n",
    "        # Use a pre-trained ResNet as backbone (simpler than InceptionV2)\n",
    "        if backbone == 'resnet34':\n",
    "            self.backbone = models.resnet34(pretrained=pretrained)\n",
    "            num_features = 512\n",
    "        elif backbone == 'resnet50':\n",
    "            self.backbone = models.resnet50(pretrained=pretrained)\n",
    "            num_features = 2048\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported backbone: {backbone}\")\n",
    "        \n",
    "        # Remove the final classification layer\n",
    "        self.backbone = nn.Sequential(*list(self.backbone.children())[:-1])\n",
    "        \n",
    "        # Multi-task heads - one for each nutritional component\n",
    "        self.shared_fc = nn.Sequential(\n",
    "            nn.Linear(num_features, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        # Individual heads for each target\n",
    "        self.calorie_head = nn.Linear(128, 1)\n",
    "        self.weight_head = nn.Linear(128, 1)\n",
    "        self.fat_head = nn.Linear(128, 1)\n",
    "        self.carb_head = nn.Linear(128, 1)\n",
    "        self.protein_head = nn.Linear(128, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Extract features using backbone\n",
    "        features = self.backbone(x)\n",
    "        features = features.view(features.size(0), -1)\n",
    "        \n",
    "        # Shared layers\n",
    "        shared = self.shared_fc(features)\n",
    "        \n",
    "        # Task-specific predictions\n",
    "        calories = self.calorie_head(shared)\n",
    "        weight = self.weight_head(shared)\n",
    "        fat = self.fat_head(shared)\n",
    "        carbs = self.carb_head(shared)\n",
    "        protein = self.protein_head(shared)\n",
    "        \n",
    "        # Stack predictions in the order of TARGET_COLUMNS\n",
    "        return torch.cat([calories, weight, fat, carbs, protein], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskLoss(nn.Module):\n",
    "    def __init__(self, task_weights=None):\n",
    "        super(MultiTaskLoss, self).__init__()\n",
    "        if task_weights is None:\n",
    "            self.task_weights = torch.ones(5)\n",
    "        else:\n",
    "            self.task_weights = torch.tensor(task_weights, dtype=torch.float32)\n",
    "    \n",
    "    def forward(self, predictions, targets):\n",
    "        # Calculate MAE for each task\n",
    "        losses = torch.abs(predictions - targets)\n",
    "        \n",
    "        # Ensure task weights are on the same device\n",
    "        if self.task_weights.device != predictions.device:\n",
    "            self.task_weights = self.task_weights.to(predictions.device)\n",
    "        \n",
    "        # Weight the losses\n",
    "        weighted_losses = losses * self.task_weights\n",
    "        \n",
    "        # Return mean loss\n",
    "        return weighted_losses.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_nutrition_csv(file_path):\n",
    "    dishes = []\n",
    "    ingredients_list = []\n",
    "    \n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(',')\n",
    "            if not parts[0].startswith('dish_'):\n",
    "                continue\n",
    "                \n",
    "            dish_id = parts[0]\n",
    "            dish_calories = float(parts[1])\n",
    "            dish_weight = float(parts[2])\n",
    "            dish_fat = float(parts[3])\n",
    "            dish_carbs = float(parts[4])\n",
    "            dish_protein = float(parts[5])\n",
    "            \n",
    "            dishes.append({\n",
    "                'dish_id': dish_id,\n",
    "                'calories': dish_calories,\n",
    "                'weight': dish_weight,\n",
    "                'fat': dish_fat,\n",
    "                'carbs': dish_carbs,\n",
    "                'protein': dish_protein\n",
    "            })\n",
    "            \n",
    "            # Extract ingredients (in groups of 7 fields)\n",
    "            ingredient_data = parts[6:]\n",
    "            i = 0\n",
    "            while i < len(ingredient_data) - 6:\n",
    "                if not ingredient_data[i].startswith('ingr_'):\n",
    "                    break\n",
    "                    \n",
    "                ingredients_list.append({\n",
    "                    'dish_id': dish_id,\n",
    "                    'ingredient_id': ingredient_data[i],\n",
    "                    'ingredient_name': ingredient_data[i+1],\n",
    "                    'amount': float(ingredient_data[i+2]),\n",
    "                    'calories': float(ingredient_data[i+3]),\n",
    "                    'fat': float(ingredient_data[i+4]),\n",
    "                    'carbs': float(ingredient_data[i+5]),\n",
    "                    'protein': float(ingredient_data[i+6])\n",
    "                })\n",
    "                i += 7\n",
    "    \n",
    "    # Create two separate dataframes\n",
    "    dish_df = pd.DataFrame(dishes)\n",
    "    ingredient_df = pd.DataFrame(ingredients_list)\n",
    "    \n",
    "    return dish_df, ingredient_df\n",
    "\n",
    "# Load and combine metadata\n",
    "dish_df_cafe1, _ = parse_nutrition_csv(METADATA_FILE_CAFE1)\n",
    "dish_df_cafe2, _ = parse_nutrition_csv(METADATA_FILE_CAFE2)\n",
    "dish_metadata_df = pd.concat([dish_df_cafe1, dish_df_cafe2], ignore_index=True)\n",
    "\n",
    "# Filter for dishes with available images\n",
    "available_dishes = [d for d in os.listdir(IMAGERY_DIR) \n",
    "                   if os.path.isdir(os.path.join(IMAGERY_DIR, d)) and \n",
    "                   os.path.exists(os.path.join(IMAGERY_DIR, d, \"rgb.png\"))]\n",
    "filtered_metadata = dish_metadata_df[dish_metadata_df['dish_id'].isin(available_dishes)]\n",
    "\n",
    "# Remove any rows with NaN values in target columns\n",
    "filtered_metadata = filtered_metadata.dropna(subset=TARGET_COLUMNS)\n",
    "\n",
    "print(f\"Found {len(filtered_metadata)} dishes with both metadata and images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load and combine metadata\n",
    "dish_df_cafe1, _ = parse_nutrition_csv(METADATA_FILE_CAFE1)\n",
    "dish_df_cafe2, _ = parse_nutrition_csv(METADATA_FILE_CAFE2)\n",
    "dish_metadata_df = pd.concat([dish_df_cafe1, dish_df_cafe2], ignore_index=True)\n",
    "\n",
    "# Filter for dishes with available images\n",
    "available_dishes = [d for d in os.listdir(IMAGERY_DIR) \n",
    "                   if os.path.exists(os.path.join(IMAGERY_DIR, d, \"rgb.png\"))]\n",
    "filtered_metadata = dish_metadata_df[dish_metadata_df['dish_id'].isin(available_dishes)]\n",
    "\n",
    "print(f\"Found {len(filtered_metadata)} dishes with both metadata and images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NutritionDataset(Dataset):\n",
    "    def __init__(self, dish_ids, labels, imagery_dir, transform=None):\n",
    "        self.dish_ids = dish_ids\n",
    "        self.labels = labels\n",
    "        self.imagery_dir = imagery_dir\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dish_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        dish_id = self.dish_ids[idx]\n",
    "        \n",
    "        # Load RGB image\n",
    "        img_path = os.path.join(self.imagery_dir, dish_id, \"rgb.png\")\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # Get labels\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "# Define transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare datasets and dataloaders - Fixed for macOS\n",
    "dish_ids = filtered_metadata['dish_id'].tolist()\n",
    "labels = filtered_metadata[TARGET_COLUMNS].values.astype(np.float32)\n",
    "\n",
    "# Split data\n",
    "train_ids, val_ids, train_labels, val_labels = train_test_split(\n",
    "    dish_ids, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = NutritionDataset(train_ids, train_labels, IMAGERY_DIR, train_transform)\n",
    "val_dataset = NutritionDataset(val_ids, val_labels, IMAGERY_DIR, val_transform)\n",
    "\n",
    "# Create dataloaders - Set num_workers=0 to avoid multiprocessing issues\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Initialize model, loss, and optimizer\n",
    "model = SimpleNutritionNet(num_outputs=5, backbone='resnet34').to(DEVICE)\n",
    "\n",
    "# Use multi-task loss with task-specific weights\n",
    "# Weights can be adjusted based on the scale of each target\n",
    "task_weights = [1.0, 1.0, 1.0, 1.0, 1.0]  # Equal weights initially\n",
    "criterion = MultiTaskLoss(task_weights)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Enhanced training and validation functions with progress tracking\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    batch_losses = []\n",
    "    \n",
    "    # Create progress bar\n",
    "    pbar = tqdm(loader, desc='Training', leave=False)\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(pbar):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_loss = loss.item()\n",
    "        total_loss += batch_loss\n",
    "        batch_losses.append(batch_loss)\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{batch_loss:.4f}',\n",
    "            'avg_loss': f'{np.mean(batch_losses):.4f}'\n",
    "        })\n",
    "    \n",
    "    pbar.close()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    # Create progress bar\n",
    "    pbar = tqdm(loader, desc='Validating', leave=False)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in pbar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            batch_loss = loss.item()\n",
    "            total_loss += batch_loss\n",
    "            \n",
    "            all_predictions.append(outputs.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({'loss': f'{batch_loss:.4f}'})\n",
    "    \n",
    "    pbar.close()\n",
    "    \n",
    "    predictions = np.concatenate(all_predictions)\n",
    "    labels = np.concatenate(all_labels)\n",
    "    \n",
    "    # Calculate percentage errors\n",
    "    percentage_errors = {}\n",
    "    for i, col in enumerate(TARGET_COLUMNS):\n",
    "        mae = mean_absolute_error(labels[:, i], predictions[:, i])\n",
    "        mean_val = labels[:, i].mean()\n",
    "        percentage_error = (mae / mean_val) * 100 if mean_val != 0 else 0\n",
    "        percentage_errors[col] = percentage_error\n",
    "    \n",
    "    return total_loss / len(loader), percentage_errors, predictions, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Verbose Training loop with detailed progress tracking\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "history = {'train_loss': [], 'val_loss': [], 'percentage_errors': [], 'lr': []}\n",
    "\n",
    "# Print training configuration\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Model: SimpleNutritionNet with {model.backbone.__class__.__name__} backbone\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"Number of Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"Training Samples: {len(train_dataset)}\")\n",
    "print(f\"Validation Samples: {len(val_dataset)}\")\n",
    "print(f\"Target Columns: {TARGET_COLUMNS}\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "try:\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        # Get current learning rate\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        print(f\"\\nEPOCH {epoch+1}/{NUM_EPOCHS} | LR: {current_lr:.6f}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # Train\n",
    "        print(\"Training phase:\")\n",
    "        train_loss = train_epoch(model, train_loader, criterion, optimizer, DEVICE)\n",
    "        \n",
    "        # Validate\n",
    "        print(\"\\nValidation phase:\")\n",
    "        val_loss, percentage_errors, predictions, labels = validate(\n",
    "            model, val_loader, criterion, DEVICE\n",
    "        )\n",
    "        \n",
    "        # Calculate epoch time\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        \n",
    "        # Update scheduler\n",
    "        old_lr = optimizer.param_groups[0]['lr']\n",
    "        scheduler.step(val_loss)\n",
    "        new_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # Save history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['percentage_errors'].append(percentage_errors)\n",
    "        history['lr'].append(current_lr)\n",
    "        \n",
    "        # Print detailed results\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"EPOCH {epoch+1} RESULTS:\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}\")\n",
    "        print(f\"Epoch Time: {epoch_time:.2f} seconds\")\n",
    "        \n",
    "        # Calculate improvement\n",
    "        if epoch > 0:\n",
    "            train_improvement = (history['train_loss'][-2] - train_loss) / history['train_loss'][-2] * 100\n",
    "            val_improvement = (history['val_loss'][-2] - val_loss) / history['val_loss'][-2] * 100\n",
    "            print(f\"Train Loss Change: {train_improvement:+.2f}%\")\n",
    "            print(f\"Val Loss Change: {val_improvement:+.2f}%\")\n",
    "        \n",
    "        print(\"\\nPERCENTAGE ERRORS BY NUTRIENT:\")\n",
    "        print(\"-\" * 40)\n",
    "        for nutrient, error in percentage_errors.items():\n",
    "            # Show trend if we have history\n",
    "            if len(history['percentage_errors']) > 1:\n",
    "                prev_error = history['percentage_errors'][-2][nutrient]\n",
    "                change = error - prev_error\n",
    "                print(f\"  {nutrient:12s}: {error:6.2f}% ({change:+.2f}%)\")\n",
    "            else:\n",
    "                print(f\"  {nutrient:12s}: {error:6.2f}%\")\n",
    "        \n",
    "        # Calculate average percentage error\n",
    "        avg_percentage_error = np.mean(list(percentage_errors.values()))\n",
    "        print(f\"  {'Average':12s}: {avg_percentage_error:6.2f}%\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            improvement_pct = (best_val_loss - val_loss) / best_val_loss * 100 if best_val_loss != float('inf') else 100\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_nutrition_model.pth')\n",
    "            print(f\"\\n✓ NEW BEST MODEL SAVED! (Improvement: {improvement_pct:.2f}%)\")\n",
    "        else:\n",
    "            epochs_since_best = epoch - history['val_loss'].index(min(history['val_loss']))\n",
    "            print(f\"\\n  No improvement for {epochs_since_best} epoch(s)\")\n",
    "        \n",
    "        # Check if learning rate changed\n",
    "        if old_lr != new_lr:\n",
    "            print(f\"\\n⚡ Learning rate reduced: {old_lr:.6f} → {new_lr:.6f}\")\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n\\n⚠️  Training interrupted by user!\")\n",
    "    print(f\"Completed {epoch}/{NUM_EPOCHS} epochs\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n\\n❌ Error during training: {e}\")\n",
    "    print(f\"Failed at epoch: {epoch+1}\")\n",
    "    raise\n",
    "\n",
    "finally:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TRAINING SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    if history['train_loss']:\n",
    "        print(f\"Final Train Loss: {history['train_loss'][-1]:.4f}\")\n",
    "        print(f\"Final Val Loss: {history['val_loss'][-1]:.4f}\")\n",
    "        print(f\"Best Val Loss: {best_val_loss:.4f}\")\n",
    "        print(f\"Total Epochs Completed: {len(history['train_loss'])}\")\n",
    "        \n",
    "        # Final percentage errors\n",
    "        if history['percentage_errors']:\n",
    "            print(\"\\nFinal Percentage Errors:\")\n",
    "            final_errors = history['percentage_errors'][-1]\n",
    "            for nutrient, error in final_errors.items():\n",
    "                print(f\"  {nutrient}: {error:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10-continue\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss plot\n",
    "ax1.plot(history['train_loss'], label='Train Loss')\n",
    "ax1.plot(history['val_loss'], label='Val Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training and Validation Loss')\n",
    "ax1.legend()\n",
    "\n",
    "# Percentage error plot\n",
    "percentage_df = pd.DataFrame(history['percentage_errors'])\n",
    "for col in percentage_df.columns:\n",
    "    ax2.plot(percentage_df[col], label=col)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Percentage Error (%)')\n",
    "ax2.set_title('Percentage Errors by Nutrient')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Load best model and evaluate performance\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load('best_nutrition_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Get predictions on validation set\n",
    "print(\"Evaluating model on validation set...\")\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "all_dish_ids = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (images, labels) in enumerate(tqdm(val_loader, desc='Predicting')):\n",
    "        images = images.to(DEVICE)\n",
    "        outputs = model(images)\n",
    "        \n",
    "        all_predictions.append(outputs.cpu().numpy())\n",
    "        all_labels.append(labels.numpy())\n",
    "        \n",
    "        # Get dish IDs for this batch\n",
    "        batch_start = i * BATCH_SIZE\n",
    "        batch_end = min(batch_start + BATCH_SIZE, len(val_ids))\n",
    "        all_dish_ids.extend(val_ids[batch_start:batch_end])\n",
    "\n",
    "predictions = np.concatenate(all_predictions)\n",
    "labels = np.concatenate(all_labels)\n",
    "\n",
    "# Create DataFrame for easier analysis\n",
    "results_df = pd.DataFrame({\n",
    "    'dish_id': all_dish_ids[:len(predictions)],\n",
    "    'calories_pred': predictions[:, 0],\n",
    "    'calories_true': labels[:, 0],\n",
    "    'weight_pred': predictions[:, 1],\n",
    "    'weight_true': labels[:, 1],\n",
    "    'fat_pred': predictions[:, 2],\n",
    "    'fat_true': labels[:, 2],\n",
    "    'carbs_pred': predictions[:, 3],\n",
    "    'carbs_true': labels[:, 3],\n",
    "    'protein_pred': predictions[:, 4],\n",
    "    'protein_true': labels[:, 4]\n",
    "})\n",
    "\n",
    "print(f\"\\nPredictions completed for {len(results_df)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Calculate detailed metrics\n",
    "def calculate_metrics(true_values, pred_values, name):\n",
    "    mae = mean_absolute_error(true_values, pred_values)\n",
    "    mse = mean_squared_error(true_values, pred_values)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(true_values, pred_values)\n",
    "    \n",
    "    # Percentage error\n",
    "    mean_true = np.mean(true_values)\n",
    "    percentage_error = (mae / mean_true) * 100 if mean_true != 0 else 0\n",
    "    \n",
    "    # Mean Absolute Percentage Error (MAPE) - per sample\n",
    "    mape = np.mean(np.abs((true_values - pred_values) / true_values)) * 100\n",
    "    \n",
    "    return {\n",
    "        'nutrient': name,\n",
    "        'MAE': mae,\n",
    "        'RMSE': rmse,\n",
    "        'R²': r2,\n",
    "        'Percentage Error': percentage_error,\n",
    "        'MAPE': mape,\n",
    "        'Mean True': mean_true,\n",
    "        'Mean Pred': np.mean(pred_values)\n",
    "    }\n",
    "\n",
    "# Calculate metrics for each nutrient\n",
    "metrics_list = []\n",
    "for nutrient in TARGET_COLUMNS:\n",
    "    true_col = f'{nutrient}_true'\n",
    "    pred_col = f'{nutrient}_pred'\n",
    "    metrics = calculate_metrics(\n",
    "        results_df[true_col].values,\n",
    "        results_df[pred_col].values,\n",
    "        nutrient\n",
    "    )\n",
    "    metrics_list.append(metrics)\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_list)\n",
    "\n",
    "# Display metrics table\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL PERFORMANCE METRICS\")\n",
    "print(\"=\"*80)\n",
    "print(metrics_df.to_string(index=False, float_format='%.3f'))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Highlight best and worst performing nutrients\n",
    "best_nutrient = metrics_df.loc[metrics_df['Percentage Error'].idxmin(), 'nutrient']\n",
    "worst_nutrient = metrics_df.loc[metrics_df['Percentage Error'].idxmax(), 'nutrient']\n",
    "print(f\"\\n✓ Best prediction: {best_nutrient} ({metrics_df.loc[metrics_df['nutrient']==best_nutrient, 'Percentage Error'].values[0]:.2f}% error)\")\n",
    "print(f\"✗ Worst prediction: {worst_nutrient} ({metrics_df.loc[metrics_df['nutrient']==worst_nutrient, 'Percentage Error'].values[0]:.2f}% error)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Visualize predictions vs actual values\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, nutrient in enumerate(TARGET_COLUMNS):\n",
    "    ax = axes[i]\n",
    "    true_col = f'{nutrient}_true'\n",
    "    pred_col = f'{nutrient}_pred'\n",
    "    \n",
    "    # Get data\n",
    "    x = results_df[true_col].values\n",
    "    y = results_df[pred_col].values\n",
    "    \n",
    "    # Create scatter plot\n",
    "    ax.scatter(x, y, alpha=0.5, s=50, edgecolors='k', linewidth=0.5)\n",
    "    \n",
    "    # Add perfect prediction line\n",
    "    min_val = min(x.min(), y.min())\n",
    "    max_val = max(x.max(), y.max())\n",
    "    ax.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect Prediction')\n",
    "    \n",
    "    # Add trend line\n",
    "    z = np.polyfit(x, y, 1)\n",
    "    p = np.poly1d(z)\n",
    "    ax.plot(x, p(x), \"b-\", alpha=0.8, label=f'Trend: y={z[0]:.2f}x+{z[1]:.2f}')\n",
    "    \n",
    "    # Calculate R²\n",
    "    r2 = r2_score(x, y)\n",
    "    \n",
    "    # Labels and title\n",
    "    ax.set_xlabel(f'True {nutrient.capitalize()}')\n",
    "    ax.set_ylabel(f'Predicted {nutrient.capitalize()}')\n",
    "    ax.set_title(f'{nutrient.capitalize()} (R² = {r2:.3f})')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Predictions vs True Values', fontsize=16, y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Error distribution analysis\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, nutrient in enumerate(TARGET_COLUMNS):\n",
    "    ax = axes[i]\n",
    "    true_col = f'{nutrient}_true'\n",
    "    pred_col = f'{nutrient}_pred'\n",
    "    \n",
    "    # Calculate errors\n",
    "    errors = results_df[pred_col] - results_df[true_col]\n",
    "    relative_errors = (errors / results_df[true_col]) * 100\n",
    "    \n",
    "    # Remove outliers for better visualization\n",
    "    q1 = relative_errors.quantile(0.25)\n",
    "    q3 = relative_errors.quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "    filtered_errors = relative_errors[(relative_errors >= lower_bound) & (relative_errors <= upper_bound)]\n",
    "    \n",
    "    # Create histogram\n",
    "    ax.hist(filtered_errors, bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    ax.axvline(x=0, color='red', linestyle='--', linewidth=2, label='Zero Error')\n",
    "    ax.axvline(x=filtered_errors.mean(), color='green', linestyle='-', linewidth=2, \n",
    "               label=f'Mean: {filtered_errors.mean():.1f}%')\n",
    "    \n",
    "    # Labels\n",
    "    ax.set_xlabel('Relative Error (%)')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_title(f'{nutrient.capitalize()} Error Distribution')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: Show sample predictions with images\n",
    "def show_predictions_with_images(n_samples=6):\n",
    "    # Get random samples\n",
    "    sample_indices = np.random.choice(len(results_df), n_samples, replace=False)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, ax in enumerate(axes):\n",
    "        if idx >= n_samples:\n",
    "            ax.axis('off')\n",
    "            continue\n",
    "            \n",
    "        sample_idx = sample_indices[idx]\n",
    "        dish_id = results_df.iloc[sample_idx]['dish_id']\n",
    "        \n",
    "        # Load and display image\n",
    "        img_path = os.path.join(IMAGERY_DIR, dish_id, \"rgb.png\")\n",
    "        img = Image.open(img_path)\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "        \n",
    "        # Create prediction text\n",
    "        pred_text = \"Predicted:\\n\"\n",
    "        true_text = \"Actual:\\n\"\n",
    "        \n",
    "        for nutrient in TARGET_COLUMNS:\n",
    "            pred_val = results_df.iloc[sample_idx][f'{nutrient}_pred']\n",
    "            true_val = results_df.iloc[sample_idx][f'{nutrient}_true']\n",
    "            error = abs(pred_val - true_val) / true_val * 100\n",
    "            \n",
    "            if nutrient == 'calories':\n",
    "                pred_text += f\"Cal: {pred_val:.0f}\\n\"\n",
    "                true_text += f\"Cal: {true_val:.0f} ({error:.1f}%)\\n\"\n",
    "            elif nutrient == 'weight':\n",
    "                pred_text += f\"Weight: {pred_val:.0f}g\\n\"\n",
    "                true_text += f\"Weight: {true_val:.0f}g ({error:.1f}%)\\n\"\n",
    "            else:\n",
    "                pred_text += f\"{nutrient.capitalize()}: {pred_val:.1f}g\\n\"\n",
    "                true_text += f\"{nutrient.capitalize()}: {true_val:.1f}g ({error:.1f}%)\\n\"\n",
    "        \n",
    "        # Add text to image\n",
    "        ax.text(0.02, 0.98, pred_text, transform=ax.transAxes, \n",
    "                verticalalignment='top', bbox=dict(boxstyle='round', \n",
    "                facecolor='lightblue', alpha=0.8), fontsize=9)\n",
    "        ax.text(0.98, 0.98, true_text, transform=ax.transAxes, \n",
    "                verticalalignment='top', horizontalalignment='right',\n",
    "                bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8), fontsize=9)\n",
    "        \n",
    "        ax.set_title(f\"Dish: {dish_id}\", fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('Sample Predictions vs Ground Truth', fontsize=14, y=1.02)\n",
    "    plt.show()\n",
    "\n",
    "show_predictions_with_images(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16: Analyze prediction quality by nutritional characteristics\n",
    "# Analyze errors by meal size\n",
    "results_df['meal_size'] = pd.cut(results_df['weight_true'], \n",
    "                                  bins=[0, 100, 200, 300, float('inf')],\n",
    "                                  labels=['Small (<100g)', 'Medium (100-200g)', \n",
    "                                         'Large (200-300g)', 'Very Large (>300g)'])\n",
    "\n",
    "# Calculate percentage errors\n",
    "for nutrient in TARGET_COLUMNS:\n",
    "    results_df[f'{nutrient}_error_pct'] = abs(\n",
    "        (results_df[f'{nutrient}_pred'] - results_df[f'{nutrient}_true']) / \n",
    "        results_df[f'{nutrient}_true']\n",
    "    ) * 100\n",
    "\n",
    "# Analyze by meal size\n",
    "print(\"\\nERROR ANALYSIS BY MEAL SIZE\")\n",
    "print(\"=\"*60)\n",
    "error_by_size = results_df.groupby('meal_size')[[f'{n}_error_pct' for n in TARGET_COLUMNS]].mean()\n",
    "print(error_by_size.round(2))\n",
    "\n",
    "# Analyze by calorie density\n",
    "results_df['calorie_density'] = results_df['calories_true'] / results_df['weight_true']\n",
    "results_df['density_category'] = pd.cut(results_df['calorie_density'],\n",
    "                                        bins=[0, 1, 2, 3, float('inf')],\n",
    "                                        labels=['Low (<1)', 'Medium (1-2)', \n",
    "                                               'High (2-3)', 'Very High (>3)'])\n",
    "\n",
    "print(\"\\n\\nERROR ANALYSIS BY CALORIE DENSITY\")\n",
    "print(\"=\"*60)\n",
    "error_by_density = results_df.groupby('density_category')[[f'{n}_error_pct' for n in TARGET_COLUMNS]].mean()\n",
    "print(error_by_density.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 17: Create confusion matrix for macronutrient ratios\n",
    "# Calculate macronutrient percentages\n",
    "def calculate_macro_percentages(df, suffix):\n",
    "    total_macros = df[f'fat_{suffix}'] + df[f'carbs_{suffix}'] + df[f'protein_{suffix}']\n",
    "    df[f'fat_pct_{suffix}'] = (df[f'fat_{suffix}'] / total_macros) * 100\n",
    "    df[f'carbs_pct_{suffix}'] = (df[f'carbs_{suffix}'] / total_macros) * 100\n",
    "    df[f'protein_pct_{suffix}'] = (df[f'protein_{suffix}'] / total_macros) * 100\n",
    "    return df\n",
    "\n",
    "results_df = calculate_macro_percentages(results_df, 'true')\n",
    "results_df = calculate_macro_percentages(results_df, 'pred')\n",
    "\n",
    "# Visualize macronutrient prediction accuracy\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for i, macro in enumerate(['fat', 'carbs', 'protein']):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Create 2D histogram\n",
    "    h = ax.hist2d(results_df[f'{macro}_pct_true'], \n",
    "                  results_df[f'{macro}_pct_pred'],\n",
    "                  bins=20, cmap='Blues')\n",
    "    \n",
    "    # Add colorbar\n",
    "    plt.colorbar(h[3], ax=ax, label='Count')\n",
    "    \n",
    "    # Add perfect prediction line\n",
    "    ax.plot([0, 100], [0, 100], 'r--', lw=2)\n",
    "    \n",
    "    # Labels\n",
    "    ax.set_xlabel(f'True {macro.capitalize()} %')\n",
    "    ax.set_ylabel(f'Predicted {macro.capitalize()} %')\n",
    "    ax.set_title(f'{macro.capitalize()} Percentage Prediction')\n",
    "    ax.set_xlim(0, 100)\n",
    "    ax.set_ylim(0, 100)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 18: Save detailed results and create summary report\n",
    "# Save detailed predictions\n",
    "results_df.to_csv('nutrition_predictions_detailed.csv', index=False)\n",
    "print(\"Detailed predictions saved to 'nutrition_predictions_detailed.csv'\")\n",
    "\n",
    "# Create summary report\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Overall performance\n",
    "overall_mae = np.mean([metrics_df.loc[metrics_df['nutrient']==n, 'Percentage Error'].values[0] \n",
    "                      for n in TARGET_COLUMNS])\n",
    "print(f\"\\nOverall Mean Percentage Error: {overall_mae:.2f}%\")\n",
    "\n",
    "# Performance compared to paper\n",
    "print(\"\\nComparison to Nutrition5k Paper Results:\")\n",
    "print(\"-\"*40)\n",
    "paper_results = {\n",
    "    'calories': 26.1,\n",
    "    'weight': 18.8,\n",
    "    'fat': 34.2,\n",
    "    'carbs': 31.9,\n",
    "    'protein': 29.5\n",
    "}\n",
    "\n",
    "for nutrient in TARGET_COLUMNS:\n",
    "    our_error = metrics_df.loc[metrics_df['nutrient']==nutrient, 'Percentage Error'].values[0]\n",
    "    paper_error = paper_results.get(nutrient, 0)\n",
    "    diff = our_error - paper_error\n",
    "    \n",
    "    if diff < 0:\n",
    "        print(f\"{nutrient:10s}: {our_error:5.1f}% (Paper: {paper_error:.1f}%) ✓ Better by {abs(diff):.1f}%\")\n",
    "    else:\n",
    "        print(f\"{nutrient:10s}: {our_error:5.1f}% (Paper: {paper_error:.1f}%) ✗ Worse by {diff:.1f}%\")\n",
    "\n",
    "# Nutritionist comparison (from paper: 41% error for professionals)\n",
    "print(f\"\\n{'Nutritionist (Professional)':10s}: ~41% average error\")\n",
    "print(f\"{'Our Model':10s}: {overall_mae:.1f}% average error\")\n",
    "if overall_mae < 41:\n",
    "    print(f\"\\n✓ Model performs {41-overall_mae:.1f}% better than professional nutritionists!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
