{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import argparse\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Train nutrition estimation models.\")\n",
    "    parser.add_argument('--model_name', type=str, required=True, \n",
    "                        choices=['SimpleConvNet', 'DeepConvNet', 'MobileNetLike', 'ResNetFromScratch', 'ResNetPretrained'],\n",
    "                        help=\"Name of the model to train.\")\n",
    "    parser.add_argument('--base_dir', type=str, default=\"/users/eleves-b/2023/georgii.kuznetsov/CNN_nutrition/nutrition5k\",\n",
    "                        help=\"Base directory for the dataset.\")\n",
    "    parser.add_argument('--output_dir', type=str, default=\".\",\n",
    "                        help=\"Directory to save model checkpoints and history.\")\n",
    "    parser.add_argument('--epochs', type=int, default=100, help=\"Number of training epochs.\")\n",
    "    parser.add_argument('--batch_size', type=int, default=32, help=\"Batch size for training.\")\n",
    "    parser.add_argument('--lr', type=float, default=1e-3, help=\"Learning rate.\")\n",
    "    parser.add_argument('--num_workers', type=int, default=0, help=\"Number of workers for DataLoader.\")\n",
    "    parser.add_argument('--no_plots', action='store_true', help=\"Disable plotting (for headless execution).\")\n",
    "    parser.add_argument('--seed', type=int, default=42, help=\"Random seed for reproducibility.\")\n",
    "    \n",
    "    # Add a specific argument for choosing pre-trained or not for ResNet models\n",
    "    # This could be inferred from model_name but making it explicit can be clearer\n",
    "    # For now, ResNetPretrained implies pretrained=True, ResNetFromScratch implies pretrained=False\n",
    "\n",
    "    return parser.parse_args()\n",
    "\n",
    "# Global constants based on typical usage, some will be overridden by args\n",
    "RGB_IMAGE_FILENAME = \"rgb.png\" \n",
    "TARGET_COLUMNS = ['calories_per_100g', 'fat_per_100g', 'carbs_per_100g', 'protein_per_100g']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# This cell would typically be run after parsing args in a script context\n",
    "# For notebook execution, you might define args manually for testing:\n",
    "# class Args:\n",
    "#     model_name = 'DeepConvNet'\n",
    "#     base_dir = \"/users/eleves-b/2023/georgii.kuznetsov/CNN_nutrition/nutrition5k\"\n",
    "#     output_dir = \".\"\n",
    "#     epochs = 10 # For quick test\n",
    "#     batch_size = 32\n",
    "#     lr = 1e-3\n",
    "#     num_workers = 0\n",
    "#     no_plots = False\n",
    "#     seed = 42\n",
    "# args = Args() # Uncomment for notebook testing\n",
    "\n",
    "# If running as a script, args will be populated by parse_args()\n",
    "# args = parse_args() # This line will be in the __main__ block\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# DEVICE = torch.device(\"mps\" if torch.mps.is_available() else \"cpu\") # If you prefer mps\n",
    "\n",
    "# To be initialized in main script flow after args are parsed\n",
    "# LOCAL_BASE_DIR = args.base_dir \n",
    "# IMAGERY_DIR = os.path.join(LOCAL_BASE_DIR, \"imagery/realsense_overhead\")\n",
    "# METADATA_FILE_CAFE1 = os.path.join(LOCAL_BASE_DIR, \"metadata/dish_metadata_cafe1.csv\")\n",
    "# METADATA_FILE_CAFE2 = os.path.join(LOCAL_BASE_DIR, \"metadata/dish_metadata_cafe2.csv\")\n",
    "\n",
    "# BATCH_SIZE = args.batch_size\n",
    "# LEARNING_RATE = args.lr\n",
    "# NUM_EPOCHS = args.epochs\n",
    "\n",
    "# Ensure output directory exists\n",
    "# if not os.path.exists(args.output_dir):\n",
    "#    os.makedirs(args.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Convolutional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConvNet(nn.Module):\n",
    "    \"\"\"Simple CNN from scratch\"\"\"\n",
    "    def __init__(self, num_outputs=4):\n",
    "        super(SimpleConvNet, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        # Calculate size after convolutions\n",
    "        # 224 -> 112 -> 56 -> 28 -> 14 (after 4 pooling layers)\n",
    "        self.fc1 = nn.Linear(256 * 14 * 14, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, num_outputs)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.pool(F.relu(self.bn4(self.conv4(x))))\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep convolutional network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepConvNet(nn.Module):\n",
    "    \"\"\"Deeper CNN with residual connections\"\"\"\n",
    "    def __init__(self, num_outputs=4):\n",
    "        super(DeepConvNet, self).__init__()\n",
    "        \n",
    "        # Initial conv\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.pool1 = nn.MaxPool2d(3, stride=2, padding=1)\n",
    "        \n",
    "        # Residual blocks\n",
    "        self.res_block1 = self._make_residual_block(64, 128)\n",
    "        self.res_block2 = self._make_residual_block(128, 256)\n",
    "        self.res_block3 = self._make_residual_block(256, 512)\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, num_outputs)\n",
    "        \n",
    "    def _make_residual_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
    "        \n",
    "        x = self.res_block1(x)\n",
    "        x = self.res_block2(x)\n",
    "        x = self.res_block3(x)\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mobile Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MobileNetLike(nn.Module):\n",
    "    \"\"\"Lightweight model inspired by MobileNet (depthwise separable convolutions)\"\"\"\n",
    "    def __init__(self, num_outputs=4):\n",
    "        super(MobileNetLike, self).__init__()\n",
    "        \n",
    "        def depthwise_separable_conv(in_channels, out_channels, stride=1):\n",
    "            return nn.Sequential(\n",
    "                # Depthwise\n",
    "                nn.Conv2d(in_channels, in_channels, 3, stride=stride, \n",
    "                         padding=1, groups=in_channels),\n",
    "                nn.BatchNorm2d(in_channels),\n",
    "                nn.ReLU(inplace=True),\n",
    "                # Pointwise\n",
    "                nn.Conv2d(in_channels, out_channels, 1),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, stride=2, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.dw_conv2 = depthwise_separable_conv(32, 64, stride=2)\n",
    "        self.dw_conv3 = depthwise_separable_conv(64, 128, stride=2)\n",
    "        self.dw_conv4 = depthwise_separable_conv(128, 256, stride=2)\n",
    "        self.dw_conv5 = depthwise_separable_conv(256, 512, stride=2)\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, num_outputs)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.dw_conv2(x)\n",
    "        x = self.dw_conv3(x)\n",
    "        x = self.dw_conv4(x)\n",
    "        x = self.dw_conv5(x)\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetFromScratch(nn.Module):\n",
    "    \"\"\"ResNet-like architmecture without pre-training\"\"\"\n",
    "    def __init__(self, num_outputs=4, use_pretrained=False):\n",
    "        super(ResNetFromScratch, self).__init__()\n",
    "        # Use ResNet34 architecture but without pre-trained weights\n",
    "        self.backbone = models.resnet34(pretrained=use_pretrained)\n",
    "        num_features = self.backbone.fc.in_features\n",
    "        \n",
    "        # Replace the final layer\n",
    "        self.backbone.fc = nn.Sequential(\n",
    "            nn.Linear(num_features, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_outputs)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskLoss(nn.Module):\n",
    "    def __init__(self, task_weights=None):\n",
    "        super(MultiTaskLoss, self).__init__()\n",
    "        if task_weights is None:\n",
    "            # Number of target columns\n",
    "            self.task_weights = torch.ones(len(TARGET_COLUMNS)) \n",
    "        else:\n",
    "            self.task_weights = torch.tensor(task_weights, dtype=torch.float32)\n",
    "    \n",
    "    def forward(self, predictions, targets):\n",
    "        # Calculate MAE for each task\n",
    "        losses = torch.abs(predictions - targets) # Using L1Loss (MAE) as base\n",
    "        \n",
    "        # Ensure task weights are on the same device\n",
    "        if self.task_weights.device != predictions.device:\n",
    "            self.task_weights = self.task_weights.to(predictions.device)\n",
    "        \n",
    "        # Weight the losses\n",
    "        weighted_losses = losses * self.task_weights\n",
    "        \n",
    "        # Return mean loss\n",
    "        return weighted_losses.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def parse_nutrition_csv(file_path):\n",
    "    dishes = []\n",
    "    # ingredients_list = [] # Original code had this, but it wasn't used.\n",
    "    \n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(',')\n",
    "            if not parts[0].startswith('dish_'):\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                dish_id = parts[0]\n",
    "                dish_calories = float(parts[1])\n",
    "                dish_weight = float(parts[2])\n",
    "                dish_fat = float(parts[3])\n",
    "                dish_carbs = float(parts[4])\n",
    "                dish_protein = float(parts[5])\n",
    "            except (IndexError, ValueError) as e:\n",
    "                # print(f\"Skipping malformed line: {line.strip()} - Error: {e}\")\n",
    "                continue\n",
    "\n",
    "            if dish_weight == 0:\n",
    "                continue\n",
    "            \n",
    "            dishes.append({\n",
    "                'dish_id': dish_id,\n",
    "                'calories': dish_calories,\n",
    "                'weight': dish_weight,\n",
    "                'fat': dish_fat,\n",
    "                'carbs': dish_carbs,\n",
    "                'protein': dish_protein,\n",
    "                'calories_per_100g': (dish_calories / dish_weight) * 100,\n",
    "                'fat_per_100g': (dish_fat / dish_weight) * 100,\n",
    "                'carbs_per_100g': (dish_carbs / dish_weight) * 100,\n",
    "                'protein_per_100g': (dish_protein / dish_weight) * 100\n",
    "            })\n",
    "    \n",
    "    dish_df = pd.DataFrame(dishes)\n",
    "    # ingredient_df = pd.DataFrame(ingredients_list) if ingredients_list else pd.DataFrame() # Original\n",
    "    \n",
    "    return dish_df #, ingredient_df\n",
    "\n",
    "class NutritionDataset(Dataset):\n",
    "    def __init__(self, dish_ids, labels, imagery_dir, transform=None):\n",
    "        self.dish_ids = dish_ids\n",
    "        self.labels = labels\n",
    "        self.imagery_dir = imagery_dir\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dish_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        dish_id = self.dish_ids[idx]\n",
    "        \n",
    "        img_path = os.path.join(self.imagery_dir, dish_id, RGB_IMAGE_FILENAME)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "# Define transforms (kept global as they are standard)\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    batch_losses = []\n",
    "    \n",
    "    pbar = tqdm(loader, desc='Training', leave=False)\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(pbar):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_loss = loss.item()\n",
    "        total_loss += batch_loss\n",
    "        batch_losses.append(batch_loss)\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{batch_loss:.4f}',\n",
    "            'avg_loss': f'{np.mean(batch_losses):.4f}'\n",
    "        })\n",
    "    \n",
    "    pbar.close()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def validate(model, loader, criterion, device, target_columns_list):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    pbar = tqdm(loader, desc='Validating', leave=False)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels_batch in pbar: # Renamed labels to labels_batch\n",
    "            images, labels_batch = images.to(device), labels_batch.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels_batch)\n",
    "            \n",
    "            batch_loss = loss.item()\n",
    "            total_loss += batch_loss\n",
    "            \n",
    "            all_predictions.append(outputs.cpu().numpy())\n",
    "            all_labels.append(labels_batch.cpu().numpy())\n",
    "            \n",
    "            pbar.set_postfix({'loss': f'{batch_loss:.4f}'})\n",
    "    \n",
    "    pbar.close()\n",
    "    \n",
    "    predictions_np = np.concatenate(all_predictions) # Renamed\n",
    "    labels_np = np.concatenate(all_labels) # Renamed\n",
    "    \n",
    "    percentage_errors = {}\n",
    "    for i, col in enumerate(target_columns_list): # Use passed target_columns_list\n",
    "        mae = mean_absolute_error(labels_np[:, i], predictions_np[:, i])\n",
    "        # Handle cases where mean_val might be zero or very small, or labels_np is empty\n",
    "        if labels_np.shape[0] > 0 and labels_np[:, i].mean() != 0:\n",
    "            mean_val = labels_np[:, i].mean()\n",
    "            percentage_error = (mae / mean_val) * 100\n",
    "        else:\n",
    "            percentage_error = float('nan') # Or some other indicator for problematic calculation\n",
    "        percentage_errors[col] = percentage_error\n",
    "    \n",
    "    return total_loss / len(loader), percentage_errors, predictions_np, labels_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    batch_losses = []\n",
    "    \n",
    "    pbar = tqdm(loader, desc='Training', leave=False)\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(pbar):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_loss = loss.item()\n",
    "        total_loss += batch_loss\n",
    "        batch_losses.append(batch_loss)\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{batch_loss:.4f}',\n",
    "            'avg_loss': f'{np.mean(batch_losses):.4f}'\n",
    "        })\n",
    "    \n",
    "    pbar.close()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def validate(model, loader, criterion, device, target_columns_list):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    pbar = tqdm(loader, desc='Validating', leave=False)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels_batch in pbar: # Renamed labels to labels_batch\n",
    "            images, labels_batch = images.to(device), labels_batch.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels_batch)\n",
    "            \n",
    "            batch_loss = loss.item()\n",
    "            total_loss += batch_loss\n",
    "            \n",
    "            all_predictions.append(outputs.cpu().numpy())\n",
    "            all_labels.append(labels_batch.cpu().numpy())\n",
    "            \n",
    "            pbar.set_postfix({'loss': f'{batch_loss:.4f}'})\n",
    "    \n",
    "    pbar.close()\n",
    "    \n",
    "    predictions_np = np.concatenate(all_predictions) # Renamed\n",
    "    labels_np = np.concatenate(all_labels) # Renamed\n",
    "    \n",
    "    percentage_errors = {}\n",
    "    for i, col in enumerate(target_columns_list): # Use passed target_columns_list\n",
    "        mae = mean_absolute_error(labels_np[:, i], predictions_np[:, i])\n",
    "        # Handle cases where mean_val might be zero or very small, or labels_np is empty\n",
    "        if labels_np.shape[0] > 0 and labels_np[:, i].mean() != 0:\n",
    "            mean_val = labels_np[:, i].mean()\n",
    "            percentage_error = (mae / mean_val) * 100\n",
    "        else:\n",
    "            percentage_error = float('nan') # Or some other indicator for problematic calculation\n",
    "        percentage_errors[col] = percentage_error\n",
    "    \n",
    "    return total_loss / len(loader), percentage_errors, predictions_np, labels_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def calculate_metrics_per_100g(results_df_local): # Renamed to avoid conflict\n",
    "    metrics_list = []\n",
    "    for nutrient_col_prefix in ['calories', 'fat', 'carbs', 'protein']: # Iterate through base nutrient names\n",
    "        true_col_name = f'{nutrient_col_prefix}_per_100g_true'\n",
    "        pred_col_name = f'{nutrient_col_prefix}_per_100g_pred'\n",
    "\n",
    "        # Check if columns exist, important if TARGET_COLUMNS was changed\n",
    "        if true_col_name not in results_df_local.columns or pred_col_name not in results_df_local.columns:\n",
    "            print(f\"Warning: Columns for {nutrient_col_prefix} not found in results_df. Skipping metrics for it.\")\n",
    "            continue\n",
    "\n",
    "        per_100g_true = results_df_local[true_col_name].values\n",
    "        per_100g_pred = results_df_local[pred_col_name].values\n",
    "        \n",
    "        # Ensure there's data to calculate metrics on\n",
    "        if len(per_100g_true) == 0 or len(per_100g_pred) == 0:\n",
    "            print(f\"Warning: No data for {nutrient_col_prefix} to calculate metrics. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Drop NaNs for metric calculation if any exist from data issues\n",
    "        valid_indices = ~ (np.isnan(per_100g_true) | np.isnan(per_100g_pred))\n",
    "        per_100g_true_valid = per_100g_true[valid_indices]\n",
    "        per_100g_pred_valid = per_100g_pred[valid_indices]\n",
    "\n",
    "        if len(per_100g_true_valid) == 0: # Still no data after NaN removal\n",
    "             print(f\"Warning: No valid (non-NaN) data for {nutrient_col_prefix} to calculate metrics. Skipping.\")\n",
    "             continue\n",
    "\n",
    "\n",
    "        mae = mean_absolute_error(per_100g_true_valid, per_100g_pred_valid)\n",
    "        rmse = np.sqrt(mean_squared_error(per_100g_true_valid, per_100g_pred_valid))\n",
    "        r2 = r2_score(per_100g_true_valid, per_100g_pred_valid)\n",
    "        \n",
    "        mean_true = np.mean(per_100g_true_valid)\n",
    "        percentage_error = (mae / mean_true) * 100 if mean_true != 0 else float('nan')\n",
    "        \n",
    "        metrics_list.append({\n",
    "            'nutrient': f'{nutrient_col_prefix}_per_100g', # Use prefix\n",
    "            'MAE': mae,\n",
    "            'RMSE': rmse,\n",
    "            'R²': r2,\n",
    "            'Percentage Error': percentage_error,\n",
    "            'Mean True': mean_true,\n",
    "            'Mean Pred': np.mean(per_100g_pred_valid)\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(metrics_list)\n",
    "\n",
    "def plot_training_history(history, model_name_str, output_dir_str, target_columns_list): # Added arguments\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "    ax1.plot(history['train_loss'], label='Train Loss')\n",
    "    ax1.plot(history['val_loss'], label='Val Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title(f'Training and Validation Loss ({model_name_str})')\n",
    "    ax1.legend()\n",
    "\n",
    "    percentage_df = pd.DataFrame(history['percentage_errors'])\n",
    "    # Ensure columns in percentage_df match target_columns_list for plotting\n",
    "    for col in target_columns_list: # Iterate over expected target_columns\n",
    "        if col in percentage_df.columns:\n",
    "             ax2.plot(percentage_df[col], label=col.replace('_per_100g',''))\n",
    "        else:\n",
    "            print(f\"Warning: Column {col} not found in history['percentage_errors'] for plotting.\")\n",
    "\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Percentage Error (%)')\n",
    "    ax2.set_title(f'Validation Percentage Errors ({model_name_str})')\n",
    "    ax2.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir_str, f'training_history_{model_name_str}.png'))\n",
    "    plt.close(fig) # Close figure to free memory\n",
    "    print(f\"Training history plot saved to {os.path.join(output_dir_str, f'training_history_{model_name_str}.png')}\")\n",
    "\n",
    "\n",
    "def plot_predictions_vs_actual(results_df_local, model_name_str, output_dir_str, target_columns_list):\n",
    "    num_nutrients = len(target_columns_list)\n",
    "    ncols = 2 \n",
    "    nrows = (num_nutrients + ncols - 1) // ncols\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(7 * ncols, 6 * nrows))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, nutrient_full_name in enumerate(target_columns_list): # e.g. 'calories_per_100g'\n",
    "        ax = axes[i]\n",
    "        true_col = f'{nutrient_full_name}_true'\n",
    "        pred_col = f'{nutrient_full_name}_pred'\n",
    "\n",
    "        if true_col not in results_df_local.columns or pred_col not in results_df_local.columns:\n",
    "            print(f\"Warning: Columns '{true_col}' or '{pred_col}' not found. Skipping plot for '{nutrient_full_name}'.\")\n",
    "            ax.set_title(f\"{nutrient_full_name.replace('_per_100g', '').capitalize()} - Data Missing\")\n",
    "            ax.axis('off')\n",
    "            continue\n",
    "        \n",
    "        x_data = results_df_local[true_col].values\n",
    "        y_data = results_df_local[pred_col].values\n",
    "\n",
    "        valid_indices = ~ (np.isnan(x_data) | np.isnan(y_data))\n",
    "        x_plot, y_plot = x_data[valid_indices], y_data[valid_indices]\n",
    "\n",
    "        if len(x_plot) == 0:\n",
    "            ax.set_title(f\"{nutrient_full_name.replace('_per_100g', '').capitalize()} - No Valid Data\")\n",
    "            ax.axis('off')\n",
    "            continue\n",
    "            \n",
    "        ax.scatter(x_plot, y_plot, alpha=0.5, s=30, edgecolors='k', linewidth=0.5)\n",
    "        min_val = min(x_plot.min(), y_plot.min())\n",
    "        max_val = max(x_plot.max(), y_plot.max())\n",
    "        ax.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect Prediction')\n",
    "\n",
    "        if len(x_plot) > 1:\n",
    "            z = np.polyfit(x_plot, y_plot, 1)\n",
    "            p = np.poly1d(z)\n",
    "            trend_x = np.array([x_plot.min(), x_plot.max()])\n",
    "            ax.plot(trend_x, p(trend_x), \"b-\", alpha=0.8, label=f'Trend: y={z[0]:.2f}x+{z[1]:.2f}')\n",
    "            r2 = r2_score(x_plot, y_plot)\n",
    "            r2_text = f\"R² = {r2:.3f}\"\n",
    "        else:\n",
    "            r2_text = \"R² = N/A\"\n",
    "            ax.plot([], [], \"b-\", alpha=0.8, label='Trend: N/A')\n",
    "\n",
    "\n",
    "        display_nutrient_name = nutrient_full_name.replace('_per_100g', '').replace('_', ' ').capitalize()\n",
    "        ax.set_xlabel(f'True {display_nutrient_name} (per 100g)')\n",
    "        ax.set_ylabel(f'Predicted {display_nutrient_name} (per 100g)')\n",
    "        ax.set_title(f'{display_nutrient_name} (per 100g)\\n{r2_text}')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    for j in range(num_nutrients, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.suptitle(f'Predictions vs True Values ({model_name_str}, per 100g)', fontsize=16)\n",
    "    plt.savefig(os.path.join(output_dir_str, f'predictions_vs_actual_{model_name_str}.png'))\n",
    "    plt.close(fig)\n",
    "    print(f\"Predictions vs actual plot saved to {os.path.join(output_dir_str, f'predictions_vs_actual_{model_name_str}.png')}\")\n",
    "\n",
    "\n",
    "def show_sample_predictions_with_images(results_df_local, imagery_dir_str, model_name_str, output_dir_str, target_columns_list, n_samples=6):\n",
    "    if len(results_df_local) == 0:\n",
    "        print(\"No results to show samples from.\")\n",
    "        return\n",
    "    \n",
    "    # Ensure n_samples is not greater than available samples\n",
    "    n_samples = min(n_samples, len(results_df_local))\n",
    "    if n_samples == 0:\n",
    "        print(\"Not enough samples to display.\")\n",
    "        return\n",
    "\n",
    "    sample_indices = np.random.choice(len(results_df_local), n_samples, replace=False)\n",
    "    \n",
    "    ncols = 3\n",
    "    nrows = (n_samples + ncols - 1) // ncols\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(5 * ncols, 5 * nrows))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, ax_idx in enumerate(range(n_samples)): # Iterate only up to n_samples\n",
    "        ax = axes[ax_idx]\n",
    "        sample_df_idx = sample_indices[i] # Use i for sample_indices\n",
    "        \n",
    "        dish_id = results_df_local.iloc[sample_df_idx]['dish_id']\n",
    "        \n",
    "        img_path = os.path.join(imagery_dir_str, dish_id, RGB_IMAGE_FILENAME)\n",
    "        try:\n",
    "            img = Image.open(img_path)\n",
    "            ax.imshow(img)\n",
    "        except FileNotFoundError:\n",
    "            ax.text(0.5, 0.5, \"Image not found\", ha='center', va='center')\n",
    "            print(f\"Warning: Image not found for {dish_id} at {img_path}\")\n",
    "\n",
    "        ax.axis('off')\n",
    "        \n",
    "        pred_text = \"Pred:\\n\"\n",
    "        true_text = \"Actual (Err%):\\n\"\n",
    "        \n",
    "        for nutrient in target_columns_list: # e.g. 'calories_per_100g'\n",
    "            pred_val = results_df_local.iloc[sample_df_idx][f'{nutrient}_pred']\n",
    "            true_val = results_df_local.iloc[sample_df_idx][f'{nutrient}_true']\n",
    "            \n",
    "            # Make nutrient name shorter for display\n",
    "            short_nutrient_name = nutrient.replace('_per_100g', '').capitalize()[:4]\n",
    "\n",
    "            if true_val != 0:\n",
    "                error = abs(pred_val - true_val) / true_val * 100\n",
    "                error_str = f\"({error:.0f}%)\"\n",
    "            else:\n",
    "                error_str = \"(N/A)\"\n",
    "\n",
    "            pred_text += f\"{short_nutrient_name}: {pred_val:.0f}\\n\"\n",
    "            true_text += f\"{short_nutrient_name}: {true_val:.0f} {error_str}\\n\"\n",
    "        \n",
    "        ax.text(0.02, 0.02, pred_text, transform=ax.transAxes, \n",
    "                verticalalignment='bottom', bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.7), fontsize=8)\n",
    "        ax.text(0.98, 0.02, true_text, transform=ax.transAxes, \n",
    "                verticalalignment='bottom', horizontalalignment='right',\n",
    "                bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.7), fontsize=8)\n",
    "        \n",
    "        ax.set_title(f\"Dish: {dish_id[:15]}...\", fontsize=9) # Shorten dish_id if too long\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for j in range(n_samples, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.suptitle(f'Sample Predictions ({model_name_str})', fontsize=14)\n",
    "    plt.savefig(os.path.join(output_dir_str, f'sample_predictions_{model_name_str}.png'))\n",
    "    plt.close(fig)\n",
    "    print(f\"Sample predictions plot saved to {os.path.join(output_dir_str, f'sample_predictions_{model_name_str}.png')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def run_training_and_evaluation(args):\n",
    "    # Set seed for reproducibility\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "    # --- Path and Parameter Setup ---\n",
    "    LOCAL_BASE_DIR = args.base_dir\n",
    "    IMAGERY_DIR = os.path.join(LOCAL_BASE_DIR, \"imagery/realsense_overhead\")\n",
    "    METADATA_FILE_CAFE1 = os.path.join(LOCAL_BASE_DIR, \"metadata/dish_metadata_cafe1.csv\")\n",
    "    METADATA_FILE_CAFE2 = os.path.join(LOCAL_BASE_DIR, \"metadata/dish_metadata_cafe2.csv\")\n",
    "\n",
    "    assert(os.path.exists(LOCAL_BASE_DIR)), f\"Base directory not found: {LOCAL_BASE_DIR}\"\n",
    "    assert(os.path.exists(IMAGERY_DIR)), f\"Imagery directory not found: {IMAGERY_DIR}\"\n",
    "    assert(os.path.exists(METADATA_FILE_CAFE1)), f\"Metadata cafe1 not found: {METADATA_FILE_CAFE1}\"\n",
    "    assert(os.path.exists(METADATA_FILE_CAFE2)), f\"Metadata cafe2 not found: {METADATA_FILE_CAFE2}\"\n",
    "    \n",
    "    if not os.path.exists(args.output_dir):\n",
    "        os.makedirs(args.output_dir, exist_ok=True)\n",
    "\n",
    "    # --- Data Loading and Preprocessing ---\n",
    "    print(\"Loading and preprocessing data...\")\n",
    "    dish_df_cafe1 = parse_nutrition_csv(METADATA_FILE_CAFE1)\n",
    "    dish_df_cafe2 = parse_nutrition_csv(METADATA_FILE_CAFE2)\n",
    "    dish_metadata_df = pd.concat([dish_df_cafe1, dish_df_cafe2], ignore_index=True)\n",
    "\n",
    "    available_dishes = [d for d in os.listdir(IMAGERY_DIR) \n",
    "                       if os.path.isdir(os.path.join(IMAGERY_DIR, d)) and \n",
    "                       os.path.exists(os.path.join(IMAGERY_DIR, d, RGB_IMAGE_FILENAME))]\n",
    "    filtered_metadata = dish_metadata_df[dish_metadata_df['dish_id'].isin(available_dishes)]\n",
    "    filtered_metadata = filtered_metadata.replace([np.inf, -np.inf], np.nan)\n",
    "    filtered_metadata = filtered_metadata.dropna(subset=TARGET_COLUMNS + ['weight']) # Ensure weight is also not NaN\n",
    "\n",
    "    print(f\"Found {len(filtered_metadata)} dishes with metadata, images, and valid targets.\")\n",
    "    # print(\"\\nPer-100g statistics:\")\n",
    "    # print(filtered_metadata[TARGET_COLUMNS].describe())\n",
    "\n",
    "    dish_ids = filtered_metadata['dish_id'].tolist()\n",
    "    labels = filtered_metadata[TARGET_COLUMNS].values.astype(np.float32)\n",
    "\n",
    "    train_ids, val_ids, train_labels, val_labels = train_test_split(\n",
    "        dish_ids, labels, test_size=0.2, random_state=args.seed\n",
    "    )\n",
    "\n",
    "    train_dataset = NutritionDataset(train_ids, train_labels, IMAGERY_DIR, train_transform)\n",
    "    val_dataset = NutritionDataset(val_ids, val_labels, IMAGERY_DIR, val_transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.num_workers)\n",
    "    print(f\"Training samples: {len(train_dataset)}, Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "    # --- Model Initialization ---\n",
    "    num_outputs = len(TARGET_COLUMNS)\n",
    "    model_configs = {\n",
    "        'SimpleConvNet': SimpleConvNet(num_outputs=num_outputs),\n",
    "        'DeepConvNet': DeepConvNet(num_outputs=num_outputs),\n",
    "        'MobileNetLike': MobileNetLike(num_outputs=num_outputs),\n",
    "        'ResNetFromScratch': ResNetFromScratch(num_outputs=num_outputs, use_pretrained=False),\n",
    "        'ResNetPretrained': ResNetFromScratch(num_outputs=num_outputs, use_pretrained=True)\n",
    "    }\n",
    "    model = model_configs[args.model_name].to(DEVICE)\n",
    "    \n",
    "    print(f\"\\nSelected model: {args.model_name}\")\n",
    "    print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "    # criterion = MultiTaskLoss() # If using custom weighted loss\n",
    "    criterion = nn.L1Loss() # MAE loss, as in original training\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.5, verbose=False)\n",
    "\n",
    "    # --- Training Loop ---\n",
    "    MODEL_SAVE_PATH = os.path.join(args.output_dir, f'best_nutrition_model_{args.model_name}.pth')\n",
    "    history_path = os.path.join(args.output_dir, f'training_history_{args.model_name}.pkl')\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    history = {'train_loss': [], 'val_loss': [], 'percentage_errors': [], 'lr': []}\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"STARTING TRAINING: {args.model_name}\")\n",
    "    print(f\"Epochs: {args.epochs}, Batch Size: {args.batch_size}, LR: {args.lr}, Device: {DEVICE}\")\n",
    "    print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "    try:\n",
    "        for epoch in range(args.epochs):\n",
    "            epoch_start_time = time.time()\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            \n",
    "            print(f\"\\nEPOCH {epoch+1}/{args.epochs} | LR: {current_lr:.6f}\")\n",
    "            \n",
    "            train_loss = train_epoch(model, train_loader, criterion, optimizer, DEVICE)\n",
    "            val_loss, percentage_errors, _, _ = validate(model, val_loader, criterion, DEVICE, TARGET_COLUMNS)\n",
    "            \n",
    "            epoch_time = time.time() - epoch_start_time\n",
    "            scheduler.step(val_loss)\n",
    "            \n",
    "            history['train_loss'].append(train_loss)\n",
    "            history['val_loss'].append(val_loss)\n",
    "            history['percentage_errors'].append(percentage_errors)\n",
    "            history['lr'].append(current_lr)\n",
    "            \n",
    "            print(f\"Epoch {epoch+1} Summary: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Time: {epoch_time:.2f}s\")\n",
    "            avg_percentage_error = np.nanmean(list(percentage_errors.values())) # Use nanmean\n",
    "            print(f\"  Avg Val %Error: {avg_percentage_error:.2f}%\")\n",
    "            for nutrient, error in percentage_errors.items():\n",
    "                 print(f\"    {nutrient}: {error:.2f}%\")\n",
    "\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'scheduler_state_dict': scheduler.state_dict(),\n",
    "                    'best_val_loss': best_val_loss,\n",
    "                    'model_name': args.model_name,\n",
    "                    'target_columns': TARGET_COLUMNS,\n",
    "                }, MODEL_SAVE_PATH) # History is saved at the end\n",
    "                print(f\"  ✓ New best model saved to {MODEL_SAVE_PATH}\")\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\nTraining interrupted by user.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n\\nError during training: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        with open(history_path, 'wb') as f:\n",
    "            pickle.dump(history, f)\n",
    "        print(f\"\\nTraining history saved to: {history_path}\")\n",
    "        print(f\"Best validation loss for {args.model_name}: {best_val_loss:.4f}\")\n",
    "\n",
    "    # --- Evaluation and Plotting (if not disabled) ---\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"POST-TRAINING EVALUATION: {args.model_name}\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "    if os.path.exists(MODEL_SAVE_PATH):\n",
    "        print(f\"Loading best model from {MODEL_SAVE_PATH} for evaluation...\")\n",
    "        checkpoint = torch.load(MODEL_SAVE_PATH, map_location=DEVICE, weights_only=False)\n",
    "        # Re-initialize model to ensure correct architecture before loading state_dict\n",
    "        model_eval = model_configs[args.model_name].to(DEVICE) # Use the same config\n",
    "        model_eval.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model_eval.eval()\n",
    "\n",
    "        all_predictions_eval = []\n",
    "        all_labels_eval = []\n",
    "        all_dish_ids_eval = [] # For matching with weights\n",
    "        \n",
    "        # Re-create val_dataset/loader for consistent dish_id retrieval if needed\n",
    "        # Or ensure val_ids from training split is used correctly\n",
    "        current_val_idx = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels_batch in tqdm(val_loader, desc=\"Evaluating best model\"):\n",
    "                images = images.to(DEVICE)\n",
    "                outputs = model_eval(images)\n",
    "                all_predictions_eval.append(outputs.cpu().numpy())\n",
    "                all_labels_eval.append(labels_batch.cpu().numpy())\n",
    "                \n",
    "                batch_size_actual = images.size(0)\n",
    "                all_dish_ids_eval.extend(val_ids[current_val_idx : current_val_idx + batch_size_actual])\n",
    "                current_val_idx += batch_size_actual\n",
    "\n",
    "        predictions_np_eval = np.concatenate(all_predictions_eval)\n",
    "        labels_np_eval = np.concatenate(all_labels_eval)\n",
    "\n",
    "        # Create DataFrame for results\n",
    "        eval_results_list = []\n",
    "        for i in range(len(all_dish_ids_eval)):\n",
    "            dish_id = all_dish_ids_eval[i]\n",
    "            row = {'dish_id': dish_id}\n",
    "            # Get weight from filtered_metadata\n",
    "            weight = filtered_metadata.loc[filtered_metadata['dish_id'] == dish_id, 'weight'].iloc[0]\n",
    "            row['weight'] = weight\n",
    "            for j, col_name in enumerate(TARGET_COLUMNS):\n",
    "                row[f'{col_name}_pred'] = predictions_np_eval[i, j]\n",
    "                row[f'{col_name}_true'] = labels_np_eval[i, j]\n",
    "            eval_results_list.append(row)\n",
    "        \n",
    "        results_df_eval = pd.DataFrame(eval_results_list)\n",
    "\n",
    "        # Calculate absolute values\n",
    "        for nutrient_tc in TARGET_COLUMNS: # e.g. 'calories_per_100g'\n",
    "            base_nutrient_name = nutrient_tc.replace('_per_100g', '') # 'calories'\n",
    "            results_df_eval[f'{base_nutrient_name}_abs_pred'] = results_df_eval[f'{nutrient_tc}_pred'] * results_df_eval['weight'] / 100\n",
    "            results_df_eval[f'{base_nutrient_name}_abs_true'] = results_df_eval[f'{nutrient_tc}_true'] * results_df_eval['weight'] / 100\n",
    "        \n",
    "        results_df_eval.to_csv(os.path.join(args.output_dir, f'evaluation_results_{args.model_name}.csv'), index=False)\n",
    "        print(f\"Evaluation results saved to {os.path.join(args.output_dir, f'evaluation_results_{args.model_name}.csv')}\")\n",
    "\n",
    "        metrics_df_eval = calculate_metrics_per_100g(results_df_eval) # Pass the correct df\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"MODEL PERFORMANCE METRICS - {args.model_name} (Per 100g on Val Set)\")\n",
    "        print(\"=\"*80)\n",
    "        print(metrics_df_eval.to_string(index=False, float_format='%.3f'))\n",
    "\n",
    "        if not args.no_plots:\n",
    "            print(\"\\nGenerating plots...\")\n",
    "            plot_training_history(history, args.model_name, args.output_dir, TARGET_COLUMNS)\n",
    "            plot_predictions_vs_actual(results_df_eval, args.model_name, args.output_dir, TARGET_COLUMNS)\n",
    "            show_sample_predictions_with_images(results_df_eval, IMAGERY_DIR, args.model_name, args.output_dir, TARGET_COLUMNS)\n",
    "        else:\n",
    "            print(\"\\nPlotting disabled.\")\n",
    "    else:\n",
    "        print(f\"No best model found at {MODEL_SAVE_PATH} to evaluate.\")\n",
    "\n",
    "    print(f\"\\nFinished run for model: {args.model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parse_args()\n",
    "    \n",
    "# Update global constants based on args for functions that might use them directly\n",
    "# BATCH_SIZE = args.batch_size # Already handled by passing args\n",
    "# LEARNING_RATE = args.lr\n",
    "# NUM_EPOCHS = args.epochs\n",
    "\n",
    "# Set matplotlib backend for non-interactive environments if plots are enabled\n",
    "if not args.no_plots:\n",
    "    # Try to set a non-interactive backend if no display is available\n",
    "    try:\n",
    "        plt.gcf() \n",
    "    except Exception: #plt.RuntimeError or other backend errors:\n",
    "        import matplotlib\n",
    "        matplotlib.use('Agg') # Use 'Agg' for non-interactive plotting to files\n",
    "        print(\"Matplotlib backend set to 'Agg' for non-interactive plotting.\")\n",
    "        # Re-import pyplot after setting backend\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "run_training_and_evaluation(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Verbose Training loop with model-specific saving\n",
    "best_val_loss = float('inf')\n",
    "history = {'train_loss': [], 'val_loss': [], 'percentage_errors': [], 'lr': []}\n",
    "\n",
    "# Print training configuration\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"Number of Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"Training Samples: {len(train_dataset)}\")\n",
    "print(f\"Validation Samples: {len(val_dataset)}\")\n",
    "print(f\"Target Columns: {TARGET_COLUMNS}\")\n",
    "print(f\"Model Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "# Model-specific save path\n",
    "MODEL_SAVE_PATH = f'best_nutrition_model_{MODEL_NAME}.pth'\n",
    "\n",
    "try:\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        # Get current learning rate\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        print(f\"\\nEPOCH {epoch+1}/{NUM_EPOCHS} | LR: {current_lr:.6f}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # Train\n",
    "        print(\"Training phase:\")\n",
    "        train_loss = train_epoch(model, train_loader, criterion, optimizer, DEVICE)\n",
    "        \n",
    "        # Validate\n",
    "        print(\"\\nValidation phase:\")\n",
    "        val_loss, percentage_errors, predictions, labels = validate(\n",
    "            model, val_loader, criterion, DEVICE\n",
    "        )\n",
    "        \n",
    "        # Calculate epoch time\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        \n",
    "        # Update scheduler\n",
    "        old_lr = optimizer.param_groups[0]['lr']\n",
    "        scheduler.step(val_loss)\n",
    "        new_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # Save history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['percentage_errors'].append(percentage_errors)\n",
    "        history['lr'].append(current_lr)\n",
    "        \n",
    "        # Print detailed results\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"EPOCH {epoch+1} RESULTS:\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}\")\n",
    "        print(f\"Epoch Time: {epoch_time:.2f} seconds\")\n",
    "        \n",
    "        # Calculate improvement\n",
    "        if epoch > 0:\n",
    "            train_improvement = (history['train_loss'][-2] - train_loss) / history['train_loss'][-2] * 100\n",
    "            val_improvement = (history['val_loss'][-2] - val_loss) / history['val_loss'][-2] * 100\n",
    "            print(f\"Train Loss Change: {train_improvement:+.2f}%\")\n",
    "            print(f\"Val Loss Change: {val_improvement:+.2f}%\")\n",
    "        \n",
    "        print(\"\\nPERCENTAGE ERRORS BY NUTRIENT (per 100g):\")\n",
    "        print(\"-\" * 40)\n",
    "        for nutrient, error in percentage_errors.items():\n",
    "            # Show trend if we have history\n",
    "            if len(history['percentage_errors']) > 1:\n",
    "                prev_error = history['percentage_errors'][-2][nutrient]\n",
    "                change = error - prev_error\n",
    "                print(f\"  {nutrient:20s}: {error:6.2f}% ({change:+.2f}%)\")\n",
    "            else:\n",
    "                print(f\"  {nutrient:20s}: {error:6.2f}%\")\n",
    "        \n",
    "        # Calculate average percentage error\n",
    "        avg_percentage_error = np.mean(list(percentage_errors.values()))\n",
    "        print(f\"  {'Average':20s}: {avg_percentage_error:6.2f}%\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            improvement_pct = (best_val_loss - val_loss) / best_val_loss * 100 if best_val_loss != float('inf') else 100\n",
    "            best_val_loss = val_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'best_val_loss': best_val_loss,\n",
    "                'model_name': MODEL_NAME,\n",
    "                'target_columns': TARGET_COLUMNS,\n",
    "                'history': history\n",
    "            }, MODEL_SAVE_PATH)\n",
    "            print(f\"\\n✓ NEW BEST MODEL SAVED to {MODEL_SAVE_PATH}! (Improvement: {improvement_pct:.2f}%)\")\n",
    "        else:\n",
    "            epochs_since_best = epoch - history['val_loss'].index(min(history['val_loss']))\n",
    "            print(f\"\\n  No improvement for {epochs_since_best} epoch(s)\")\n",
    "        \n",
    "        # Check if learning rate changed\n",
    "        if old_lr != new_lr:\n",
    "            print(f\"\\n⚡ Learning rate reduced: {old_lr:.6f} → {new_lr:.6f}\")\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n\\n⚠️  Training interrupted by user!\")\n",
    "    print(f\"Completed {epoch}/{NUM_EPOCHS} epochs\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n\\n❌ Error during training: {e}\")\n",
    "    print(f\"Failed at epoch: {epoch+1}\")\n",
    "    raise\n",
    "\n",
    "finally:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"TRAINING SUMMARY - {MODEL_NAME}\")\n",
    "    print(\"=\"*60)\n",
    "    if history['train_loss']:\n",
    "        print(f\"Final Train Loss: {history['train_loss'][-1]:.4f}\")\n",
    "        print(f\"Final Val Loss: {history['val_loss'][-1]:.4f}\")\n",
    "        print(f\"Best Val Loss: {best_val_loss:.4f}\")\n",
    "        print(f\"Total Epochs Completed: {len(history['train_loss'])}\")\n",
    "        print(f\"Model saved as: {MODEL_SAVE_PATH}\")\n",
    "        \n",
    "        # Final percentage errors\n",
    "        if history['percentage_errors']:\n",
    "            print(\"\\nFinal Percentage Errors (per 100g):\")\n",
    "            final_errors = history['percentage_errors'][-1]\n",
    "            for nutrient, error in final_errors.items():\n",
    "                print(f\"  {nutrient}: {error:.2f}%\")\n",
    "            \n",
    "            # Average error\n",
    "            avg_error = np.mean(list(final_errors.values()))\n",
    "            print(f\"\\nAverage Percentage Error: {avg_error:.2f}%\")\n",
    "    \n",
    "    # Save final history for this model\n",
    "    history_path = f'training_history_{MODEL_NAME}.pkl'\n",
    "    import pickle\n",
    "    with open(history_path, 'wb') as f:\n",
    "        pickle.dump(history, f)\n",
    "    print(f\"\\nTraining history saved to: {history_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10-continue\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss plot\n",
    "ax1.plot(history['train_loss'], label='Train Loss')\n",
    "ax1.plot(history['val_loss'], label='Val Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training and Validation Loss')\n",
    "ax1.legend()\n",
    "\n",
    "# Percentage error plot\n",
    "percentage_df = pd.DataFrame(history['percentage_errors'])\n",
    "for col in percentage_df.columns:\n",
    "    ax2.plot(percentage_df[col], label=col)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Percentage Error (%)')\n",
    "ax2.set_title('Percentage Errors by Nutrient')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "# Make sure BATCH_SIZE, val_ids, filtered_metadata, DEVICE, MODEL_NAME, val_loader, model are defined as in your environment.\n",
    "# For example:\n",
    "# BATCH_SIZE = 32 # Example value\n",
    "# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Example value\n",
    "# MODEL_NAME = \"your_model_name\" # Example value\n",
    "# val_ids = [...] # Should be defined\n",
    "# filtered_metadata = pd.DataFrame(...) # Should be defined\n",
    "# val_loader = ... # Should be defined\n",
    "# model = ... # Your model instance should be defined\n",
    "\n",
    "# Cell 11: Modified evaluation for per-100g predictions\n",
    "# Load best model and evaluate\n",
    "\n",
    "# Path to the saved checkpoint\n",
    "checkpoint_path = f'best_nutrition_model_{MODEL_NAME}.pth'\n",
    "\n",
    "# 1. Load the entire checkpoint dictionary.\n",
    "# Use weights_only=False because the checkpoint contains non-tensor data (like epoch, history, etc.),\n",
    "# and PyTorch 2.6+ defaults to weights_only=True, which would cause an UnpicklingError.\n",
    "# This is safe because you trust the source of the .pth file (it was saved by your Cell 10).\n",
    "print(f\"Loading checkpoint from {checkpoint_path}...\")\n",
    "checkpoint = torch.load(checkpoint_path, weights_only=False, map_location=DEVICE) # Added weights_only=False and map_location\n",
    "\n",
    "# 2. Load the model's state_dict from the checkpoint dictionary\n",
    "# Your saving code (Cell 10) stores the model's weights under the key 'model_state_dict'.\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval() # Set model to evaluation mode\n",
    "\n",
    "print(f\"Model {MODEL_NAME} loaded successfully and set to evaluation mode.\")\n",
    "\n",
    "# Get predictions on validation set\n",
    "print(f\"Evaluating {MODEL_NAME} on validation set...\")\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "all_dish_ids = []\n",
    "all_weights = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch_data in enumerate(tqdm(val_loader, desc='Predicting')):\n",
    "        # Assuming val_loader yields (images, labels) or a dict. Adjust if different.\n",
    "        if isinstance(batch_data, (list, tuple)):\n",
    "            images, labels_batch = batch_data\n",
    "        elif isinstance(batch_data, dict): # If your DataLoader returns a dictionary\n",
    "            images = batch_data['image'] # Adjust key if necessary\n",
    "            labels_batch = batch_data['labels'] # Adjust key if necessary\n",
    "        else:\n",
    "            raise TypeError(f\"Unexpected batch data type: {type(batch_data)}\")\n",
    "\n",
    "        images = images.to(DEVICE)\n",
    "        outputs = model(images)\n",
    "\n",
    "        all_predictions.append(outputs.cpu().numpy())\n",
    "        all_labels.append(labels_batch.cpu().numpy()) # Assuming labels_batch is a tensor\n",
    "\n",
    "        # Get dish IDs and weights for this batch\n",
    "        # Ensure val_ids is correctly populated and corresponds to val_loader's order\n",
    "        batch_start_index = i * val_loader.batch_size # Use val_loader.batch_size\n",
    "        batch_end_index = min(batch_start_index + val_loader.batch_size, len(val_ids))\n",
    "        current_batch_dish_ids = val_ids[batch_start_index:batch_end_index]\n",
    "        all_dish_ids.extend(current_batch_dish_ids)\n",
    "\n",
    "        # Get actual weights for conversion back to absolute values\n",
    "        for dish_id in current_batch_dish_ids:\n",
    "            # Ensure filtered_metadata is available and has 'dish_id' and 'weight' columns\n",
    "            weight_values = filtered_metadata[filtered_metadata['dish_id'] == dish_id]['weight'].values\n",
    "            if len(weight_values) > 0:\n",
    "                all_weights.append(weight_values[0])\n",
    "            else:\n",
    "                print(f\"Warning: No weight found for dish_id {dish_id}. Appending NaN or a default.\")\n",
    "                all_weights.append(np.nan) # Or handle as appropriate, e.g., skip or use a default\n",
    "\n",
    "# Ensure lengths match before concatenating, especially if some weights were not found\n",
    "min_len = min(len(all_predictions), len(all_labels), len(all_dish_ids), len(all_weights))\n",
    "if len(all_predictions) * val_loader.batch_size < len(val_ids) and val_loader.drop_last == False :\n",
    "     #This means your original code for batch_dish_ids may have an off-by-one if drop_last=False\n",
    "     #The val_ids slicing must exactly match the samples processed by the dataloader.\n",
    "     #The fix above using val_loader.batch_size and batch_start_index/batch_end_index should be more robust.\n",
    "     pass\n",
    "\n",
    "\n",
    "predictions_np = np.concatenate(all_predictions)\n",
    "labels_np = np.concatenate(all_labels)\n",
    "\n",
    "# Adjust slicing to the actual number of predictions made, which should match labels\n",
    "num_samples_processed = len(predictions_np)\n",
    "all_dish_ids = all_dish_ids[:num_samples_processed]\n",
    "all_weights = all_weights[:num_samples_processed]\n",
    "\n",
    "\n",
    "# Create DataFrame with per-100g values\n",
    "results_df = pd.DataFrame({\n",
    "    'dish_id': all_dish_ids,\n",
    "    'weight': all_weights,\n",
    "    'calories_per_100g_pred': predictions_np[:, 0],\n",
    "    'calories_per_100g_true': labels_np[:, 0],\n",
    "    'fat_per_100g_pred': predictions_np[:, 1],\n",
    "    'fat_per_100g_true': labels_np[:, 1],\n",
    "    'carbs_per_100g_pred': predictions_np[:, 2],\n",
    "    'carbs_per_100g_true': labels_np[:, 2],\n",
    "    'protein_per_100g_pred': predictions_np[:, 3],\n",
    "    'protein_per_100g_true': labels_np[:, 3]\n",
    "})\n",
    "\n",
    "# Calculate absolute values for comparison\n",
    "for nutrient in ['calories', 'fat', 'carbs', 'protein']:\n",
    "    results_df[f'{nutrient}_abs_pred'] = results_df[f'{nutrient}_per_100g_pred'] * results_df['weight'] / 100\n",
    "    results_df[f'{nutrient}_abs_true'] = results_df[f'{nutrient}_per_100g_true'] * results_df['weight'] / 100\n",
    "\n",
    "print(f\"\\nPredictions completed for {len(results_df)} samples\")\n",
    "if results_df['weight'].isnull().any():\n",
    "    print(\"Warning: Some weights were NaN. Absolute nutrient calculations might be affected for those rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Modified metrics calculation for per-100g\n",
    "def calculate_metrics_per_100g(results_df):\n",
    "    metrics_list = []\n",
    "    \n",
    "    # Calculate metrics for per-100g predictions\n",
    "    for nutrient in ['calories', 'fat', 'carbs', 'protein']:\n",
    "        per_100g_true = results_df[f'{nutrient}_per_100g_true'].values\n",
    "        per_100g_pred = results_df[f'{nutrient}_per_100g_pred'].values\n",
    "        \n",
    "        mae = mean_absolute_error(per_100g_true, per_100g_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(per_100g_true, per_100g_pred))\n",
    "        r2 = r2_score(per_100g_true, per_100g_pred)\n",
    "        \n",
    "        # Percentage error\n",
    "        mean_true = np.mean(per_100g_true)\n",
    "        percentage_error = (mae / mean_true) * 100 if mean_true != 0 else 0\n",
    "        \n",
    "        metrics_list.append({\n",
    "            'nutrient': f'{nutrient}_per_100g',\n",
    "            'MAE': mae,\n",
    "            'RMSE': rmse,\n",
    "            'R²': r2,\n",
    "            'Percentage Error': percentage_error,\n",
    "            'Mean True': mean_true,\n",
    "            'Mean Pred': np.mean(per_100g_pred)\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(metrics_list)\n",
    "\n",
    "# Calculate metrics\n",
    "metrics_df = calculate_metrics_per_100g(results_df)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"MODEL PERFORMANCE METRICS - {MODEL_NAME} (Per 100g)\")\n",
    "print(\"=\"*80)\n",
    "print(metrics_df.to_string(index=False, float_format='%.3f'))\n",
    "\n",
    "# Also show how this translates to absolute predictions\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EFFECTIVE ABSOLUTE PREDICTION ERRORS (using ground truth weight)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for nutrient in ['calories', 'fat', 'carbs', 'protein']:\n",
    "    abs_true = results_df[f'{nutrient}_abs_true'].values\n",
    "    abs_pred = results_df[f'{nutrient}_abs_pred'].values\n",
    "    \n",
    "    mae = mean_absolute_error(abs_true, abs_pred)\n",
    "    mean_true = np.mean(abs_true)\n",
    "    percentage_error = (mae / mean_true) * 100\n",
    "    \n",
    "    print(f\"{nutrient:10s}: {percentage_error:.2f}% error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd # Assuming results_df is a pandas DataFrame\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Ensure results_df is loaded or available from the previous cell (Cell 11)\n",
    "# For example:\n",
    "# results_df = pd.read_csv('path_to_your_results.csv') # Or however it's available\n",
    "\n",
    "# Cell 13: Visualize predictions vs actual values (per 100g)\n",
    "\n",
    "# Define the list of nutrient key prefixes to plot.\n",
    "# These MUST match the prefixes used in results_df for per-100g values\n",
    "# (e.g., 'calories_per_100g_true', 'calories_per_100g_pred').\n",
    "nutrients_to_plot_keys = [\n",
    "    'calories_per_100g',\n",
    "    'fat_per_100g',\n",
    "    'carbs_per_100g',    # Matches 'carbs_per_100g' used in results_df creation\n",
    "    'protein_per_100g'\n",
    "]\n",
    "\n",
    "num_nutrients_to_plot = len(nutrients_to_plot_keys)\n",
    "\n",
    "# Create subplots (2 rows, 3 columns allows up to 6 plots)\n",
    "# If you have exactly 4 nutrients, you might prefer a 2x2 layout.\n",
    "# For a dynamic layout:\n",
    "# ncols = 2\n",
    "# nrows = (num_nutrients_to_plot + ncols - 1) // ncols\n",
    "# fig, axes = plt.subplots(nrows, ncols, figsize=(7 * ncols, 6 * nrows))\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12)) # As per original code\n",
    "axes = axes.flatten() # Flatten to a 1D array for easy iteration\n",
    "\n",
    "for i, nutrient_key_prefix in enumerate(nutrients_to_plot_keys):\n",
    "    ax = axes[i]\n",
    "\n",
    "    true_col = f'{nutrient_key_prefix}_true'\n",
    "    pred_col = f'{nutrient_key_prefix}_pred'\n",
    "\n",
    "    # Defensive check: ensure columns exist in the DataFrame\n",
    "    if true_col not in results_df.columns or pred_col not in results_df.columns:\n",
    "        print(f\"Warning: Columns '{true_col}' or '{pred_col}' not found in results_df. Skipping plot for '{nutrient_key_prefix}'.\")\n",
    "        ax.set_title(f\"{nutrient_key_prefix.replace('_per_100g', '').capitalize()} - Data Missing\")\n",
    "        ax.axis('off') # Hide axis if data is missing\n",
    "        continue\n",
    "\n",
    "    # Get data\n",
    "    x_data = results_df[true_col].values\n",
    "    y_data = results_df[pred_col].values\n",
    "\n",
    "    # Remove NaN values to prevent errors in calculations and plotting\n",
    "    valid_indices = ~ (np.isnan(x_data) | np.isnan(y_data))\n",
    "    x_plot = x_data[valid_indices]\n",
    "    y_plot = y_data[valid_indices]\n",
    "\n",
    "    if len(x_plot) == 0: # Not enough data to plot\n",
    "        print(f\"Warning: No valid (non-NaN) data points for '{nutrient_key_prefix}'. Skipping plot.\")\n",
    "        ax.set_title(f\"{nutrient_key_prefix.replace('_per_100g', '').capitalize()} - No Valid Data\")\n",
    "        ax.axis('off')\n",
    "        continue\n",
    "\n",
    "    # Create scatter plot\n",
    "    ax.scatter(x_plot, y_plot, alpha=0.5, s=50, edgecolors='k', linewidth=0.5)\n",
    "\n",
    "    # Add perfect prediction line\n",
    "    # Ensure min/max are calculated on the actual plotted data (x_plot, y_plot)\n",
    "    min_val = min(x_plot.min(), y_plot.min()) if len(x_plot) > 0 else 0\n",
    "    max_val = max(x_plot.max(), y_plot.max()) if len(x_plot) > 0 else 1\n",
    "    if min_val == max_val: # Avoid issues if all points are identical\n",
    "        min_val -= 0.5\n",
    "        max_val += 0.5\n",
    "    ax.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect Prediction')\n",
    "\n",
    "    # Add trend line (polyfit requires at least 2 points for degree 1)\n",
    "    if len(x_plot) > 1:\n",
    "        z = np.polyfit(x_plot, y_plot, 1)\n",
    "        p = np.poly1d(z)\n",
    "        trend_line_x_points = np.array([x_plot.min(), x_plot.max()]) # Use min/max of actual data\n",
    "        ax.plot(trend_line_x_points, p(trend_line_x_points), \"b-\", alpha=0.8, label=f'Trend: y={z[0]:.2f}x+{z[1]:.2f}')\n",
    "        \n",
    "        # Calculate R² score\n",
    "        r2 = r2_score(x_plot, y_plot)\n",
    "        r2_text = f\"R² = {r2:.3f}\"\n",
    "    else:\n",
    "        # Not enough points for trend line or R²\n",
    "        ax.plot([], [], \"b-\", alpha=0.8, label='Trend: N/A (too few points)') # Placeholder for legend\n",
    "        r2_text = \"R² = N/A (too few points)\"\n",
    "\n",
    "\n",
    "    # Labels and title\n",
    "    # Create a more display-friendly nutrient name\n",
    "    display_nutrient_name = nutrient_key_prefix.replace('_per_100g', '').replace('_', ' ').capitalize()\n",
    "    \n",
    "    ax.set_xlabel(f'True {display_nutrient_name} (per 100g)')\n",
    "    ax.set_ylabel(f'Predicted {display_nutrient_name} (per 100g)')\n",
    "    ax.set_title(f'{display_nutrient_name} (per 100g)\\n{r2_text}')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Hide any unused subplots if num_nutrients_to_plot < total number of axes\n",
    "for j in range(num_nutrients_to_plot, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "# Adjust layout to prevent overlap and make space for suptitle\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.97]) # rect=[left, bottom, right, top]\n",
    "plt.suptitle('Predictions vs True Values (per 100g)', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Error distribution analysis\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, nutrient in enumerate(TARGET_COLUMNS):\n",
    "    ax = axes[i]\n",
    "    true_col = f'{nutrient}_true'\n",
    "    pred_col = f'{nutrient}_pred'\n",
    "    \n",
    "    # Calculate errors\n",
    "    errors = results_df[pred_col] - results_df[true_col]\n",
    "    relative_errors = (errors / results_df[true_col]) * 100\n",
    "    \n",
    "    # Remove outliers for better visualization\n",
    "    q1 = relative_errors.quantile(0.25)\n",
    "    q3 = relative_errors.quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "    filtered_errors = relative_errors[(relative_errors >= lower_bound) & (relative_errors <= upper_bound)]\n",
    "    \n",
    "    # Create histogram\n",
    "    ax.hist(filtered_errors, bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    ax.axvline(x=0, color='red', linestyle='--', linewidth=2, label='Zero Error')\n",
    "    ax.axvline(x=filtered_errors.mean(), color='green', linestyle='-', linewidth=2, \n",
    "               label=f'Mean: {filtered_errors.mean():.1f}%')\n",
    "    \n",
    "    # Labels\n",
    "    ax.set_xlabel('Relative Error (%)')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_title(f'{nutrient.capitalize()} Error Distribution')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: Show sample predictions with images\n",
    "def show_predictions_with_images(n_samples=6):\n",
    "    # Get random samples\n",
    "    sample_indices = np.random.choice(len(results_df), n_samples, replace=False)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, ax in enumerate(axes):\n",
    "        if idx >= n_samples:\n",
    "            ax.axis('off')\n",
    "            continue\n",
    "            \n",
    "        sample_idx = sample_indices[idx]\n",
    "        dish_id = results_df.iloc[sample_idx]['dish_id']\n",
    "        \n",
    "        # Load and display image\n",
    "        img_path = os.path.join(IMAGERY_DIR, dish_id, \"rgb.png\")\n",
    "        img = Image.open(img_path)\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "        \n",
    "        # Create prediction text\n",
    "        pred_text = \"Predicted:\\n\"\n",
    "        true_text = \"Actual:\\n\"\n",
    "        \n",
    "        for nutrient in TARGET_COLUMNS:\n",
    "            pred_val = results_df.iloc[sample_idx][f'{nutrient}_pred']\n",
    "            true_val = results_df.iloc[sample_idx][f'{nutrient}_true']\n",
    "            error = abs(pred_val - true_val) / true_val * 100\n",
    "            \n",
    "            if nutrient == 'calories':\n",
    "                pred_text += f\"Cal: {pred_val:.0f}\\n\"\n",
    "                true_text += f\"Cal: {true_val:.0f} ({error:.1f}%)\\n\"\n",
    "            elif nutrient == 'weight':\n",
    "                pred_text += f\"Weight: {pred_val:.0f}g\\n\"\n",
    "                true_text += f\"Weight: {true_val:.0f}g ({error:.1f}%)\\n\"\n",
    "            else:\n",
    "                pred_text += f\"{nutrient.capitalize()}: {pred_val:.1f}g\\n\"\n",
    "                true_text += f\"{nutrient.capitalize()}: {true_val:.1f}g ({error:.1f}%)\\n\"\n",
    "        \n",
    "        # Add text to image\n",
    "        ax.text(0.02, 0.98, pred_text, transform=ax.transAxes, \n",
    "                verticalalignment='top', bbox=dict(boxstyle='round', \n",
    "                facecolor='lightblue', alpha=0.8), fontsize=9)\n",
    "        ax.text(0.98, 0.98, true_text, transform=ax.transAxes, \n",
    "                verticalalignment='top', horizontalalignment='right',\n",
    "                bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8), fontsize=9)\n",
    "        \n",
    "        ax.set_title(f\"Dish: {dish_id}\", fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('Sample Predictions vs Ground Truth', fontsize=14, y=1.02)\n",
    "    plt.show()\n",
    "\n",
    "show_predictions_with_images(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
