{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# DEVICE = torch.device(\"mps\" if torch.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOCAL_BASE_DIR = \"/Users/georgiikuznetsov/Desktop/coding/CNN_nutrition/nutrition5k\"\n",
    "LOCAL_BASE_DIR = \"/users/eleves-b/2023/georgii.kuznetsov/CNN_nutrition/nutrition5k\"\n",
    "\n",
    "IMAGERY_DIR = os.path.join(LOCAL_BASE_DIR, \"imagery/realsense_overhead\")\n",
    "METADATA_FILE_CAFE1 = os.path.join(LOCAL_BASE_DIR, \"metadata/dish_metadata_cafe1.csv\")\n",
    "METADATA_FILE_CAFE2 = os.path.join(LOCAL_BASE_DIR, \"metadata/dish_metadata_cafe2.csv\")\n",
    "\n",
    "assert(os.path.exists(LOCAL_BASE_DIR))\n",
    "assert(os.path.exists(IMAGERY_DIR))\n",
    "assert(os.path.exists(METADATA_FILE_CAFE1))\n",
    "assert(os.path.exists(METADATA_FILE_CAFE2))\n",
    "\n",
    "RGB_IMAGE_FILENAME = \"rgb.png\" \n",
    "\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-3  # Higher LR for training from scratch\n",
    "NUM_EPOCHS = 100  # More epochs needed when training from scratch\n",
    "\n",
    "\n",
    "TARGET_COLUMNS = ['calories_per_100g', 'fat_per_100g', 'carbs_per_100g', 'protein_per_100g']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Convolutional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConvNet(nn.Module):\n",
    "    \"\"\"Simple CNN from scratch\"\"\"\n",
    "    def __init__(self, num_outputs=4):\n",
    "        super(SimpleConvNet, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        # Calculate size after convolutions\n",
    "        # 224 -> 112 -> 56 -> 28 -> 14 (after 4 pooling layers)\n",
    "        self.fc1 = nn.Linear(256 * 14 * 14, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, num_outputs)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.pool(F.relu(self.bn4(self.conv4(x))))\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep convolutional network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepConvNet(nn.Module):\n",
    "    \"\"\"Deeper CNN with residual connections\"\"\"\n",
    "    def __init__(self, num_outputs=4):\n",
    "        super(DeepConvNet, self).__init__()\n",
    "        \n",
    "        # Initial conv\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.pool1 = nn.MaxPool2d(3, stride=2, padding=1)\n",
    "        \n",
    "        # Residual blocks\n",
    "        self.res_block1 = self._make_residual_block(64, 128)\n",
    "        self.res_block2 = self._make_residual_block(128, 256)\n",
    "        self.res_block3 = self._make_residual_block(256, 512)\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, num_outputs)\n",
    "        \n",
    "    def _make_residual_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
    "        \n",
    "        x = self.res_block1(x)\n",
    "        x = self.res_block2(x)\n",
    "        x = self.res_block3(x)\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mobile Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MobileNetLike(nn.Module):\n",
    "    \"\"\"Lightweight model inspired by MobileNet (depthwise separable convolutions)\"\"\"\n",
    "    def __init__(self, num_outputs=4):\n",
    "        super(MobileNetLike, self).__init__()\n",
    "        \n",
    "        def depthwise_separable_conv(in_channels, out_channels, stride=1):\n",
    "            return nn.Sequential(\n",
    "                # Depthwise\n",
    "                nn.Conv2d(in_channels, in_channels, 3, stride=stride, \n",
    "                         padding=1, groups=in_channels),\n",
    "                nn.BatchNorm2d(in_channels),\n",
    "                nn.ReLU(inplace=True),\n",
    "                # Pointwise\n",
    "                nn.Conv2d(in_channels, out_channels, 1),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, stride=2, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.dw_conv2 = depthwise_separable_conv(32, 64, stride=2)\n",
    "        self.dw_conv3 = depthwise_separable_conv(64, 128, stride=2)\n",
    "        self.dw_conv4 = depthwise_separable_conv(128, 256, stride=2)\n",
    "        self.dw_conv5 = depthwise_separable_conv(256, 512, stride=2)\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, num_outputs)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.dw_conv2(x)\n",
    "        x = self.dw_conv3(x)\n",
    "        x = self.dw_conv4(x)\n",
    "        x = self.dw_conv5(x)\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetFromScratch(nn.Module):\n",
    "    \"\"\"ResNet-like architmecture without pre-training\"\"\"\n",
    "    def __init__(self, num_outputs=4, use_pretrained=False):\n",
    "        super(ResNetFromScratch, self).__init__()\n",
    "        # Use ResNet34 architecture but without pre-trained weights\n",
    "        self.backbone = models.resnet34(pretrained=use_pretrained)\n",
    "        num_features = self.backbone.fc.in_features\n",
    "        \n",
    "        # Replace the final layer\n",
    "        self.backbone.fc = nn.Sequential(\n",
    "            nn.Linear(num_features, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_outputs)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskLoss(nn.Module):\n",
    "    def __init__(self, task_weights=None):\n",
    "        super(MultiTaskLoss, self).__init__()\n",
    "        if task_weights is None:\n",
    "            self.task_weights = torch.ones(5)\n",
    "        else:\n",
    "            self.task_weights = torch.tensor(task_weights, dtype=torch.float32)\n",
    "    \n",
    "    def forward(self, predictions, targets):\n",
    "        # Calculate MAE for each task\n",
    "        losses = torch.abs(predictions - targets)\n",
    "        \n",
    "        # Ensure task weights are on the same device\n",
    "        if self.task_weights.device != predictions.device:\n",
    "            self.task_weights = self.task_weights.to(predictions.device)\n",
    "        \n",
    "        # Weight the losses\n",
    "        weighted_losses = losses * self.task_weights\n",
    "        \n",
    "        # Return mean loss\n",
    "        return weighted_losses.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Data preprocessing with per-100g normalization\n",
    "def parse_nutrition_csv(file_path):\n",
    "    dishes = []\n",
    "    ingredients_list = []\n",
    "    \n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(',')\n",
    "            if not parts[0].startswith('dish_'):\n",
    "                continue\n",
    "                \n",
    "            # Extract dish info (first 6 fields)\n",
    "            dish_id = parts[0]\n",
    "            dish_calories = float(parts[1])\n",
    "            dish_weight = float(parts[2])\n",
    "            dish_fat = float(parts[3])\n",
    "            dish_carbs = float(parts[4])\n",
    "            dish_protein = float(parts[5])\n",
    "            \n",
    "            # Skip dishes with zero weight\n",
    "            if dish_weight == 0:\n",
    "                continue\n",
    "            \n",
    "            # Calculate per 100g values\n",
    "            dishes.append({\n",
    "                'dish_id': dish_id,\n",
    "                'calories': dish_calories,\n",
    "                'weight': dish_weight,\n",
    "                'fat': dish_fat,\n",
    "                'carbs': dish_carbs,\n",
    "                'protein': dish_protein,\n",
    "                # Per 100g calculations\n",
    "                'calories_per_100g': (dish_calories / dish_weight) * 100,\n",
    "                'fat_per_100g': (dish_fat / dish_weight) * 100,\n",
    "                'carbs_per_100g': (dish_carbs / dish_weight) * 100,\n",
    "                'protein_per_100g': (dish_protein / dish_weight) * 100\n",
    "            })\n",
    "    \n",
    "    dish_df = pd.DataFrame(dishes)\n",
    "    ingredient_df = pd.DataFrame(ingredients_list) if ingredients_list else pd.DataFrame()\n",
    "    \n",
    "    return dish_df, ingredient_df\n",
    "\n",
    "# Load and combine metadata\n",
    "dish_df_cafe1, _ = parse_nutrition_csv(METADATA_FILE_CAFE1)\n",
    "dish_df_cafe2, _ = parse_nutrition_csv(METADATA_FILE_CAFE2)\n",
    "dish_metadata_df = pd.concat([dish_df_cafe1, dish_df_cafe2], ignore_index=True)\n",
    "\n",
    "# Filter for dishes with available images\n",
    "available_dishes = [d for d in os.listdir(IMAGERY_DIR) \n",
    "                   if os.path.isdir(os.path.join(IMAGERY_DIR, d)) and \n",
    "                   os.path.exists(os.path.join(IMAGERY_DIR, d, \"rgb.png\"))]\n",
    "filtered_metadata = dish_metadata_df[dish_metadata_df['dish_id'].isin(available_dishes)]\n",
    "\n",
    "# Remove any rows with NaN or infinite values\n",
    "filtered_metadata = filtered_metadata.replace([np.inf, -np.inf], np.nan)\n",
    "filtered_metadata = filtered_metadata.dropna(subset=TARGET_COLUMNS)\n",
    "\n",
    "print(f\"Found {len(filtered_metadata)} dishes with both metadata and images\")\n",
    "print(\"\\nPer-100g statistics:\")\n",
    "print(filtered_metadata[TARGET_COLUMNS].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load and combine metadata\n",
    "dish_df_cafe1, _ = parse_nutrition_csv(METADATA_FILE_CAFE1)\n",
    "dish_df_cafe2, _ = parse_nutrition_csv(METADATA_FILE_CAFE2)\n",
    "dish_metadata_df = pd.concat([dish_df_cafe1, dish_df_cafe2], ignore_index=True)\n",
    "\n",
    "# Filter for dishes with available images\n",
    "available_dishes = [d for d in os.listdir(IMAGERY_DIR) \n",
    "                   if os.path.exists(os.path.join(IMAGERY_DIR, d, \"rgb.png\"))]\n",
    "filtered_metadata = dish_metadata_df[dish_metadata_df['dish_id'].isin(available_dishes)]\n",
    "\n",
    "print(f\"Found {len(filtered_metadata)} dishes with both metadata and images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NutritionDataset(Dataset):\n",
    "    def __init__(self, dish_ids, labels, imagery_dir, transform=None):\n",
    "        self.dish_ids = dish_ids\n",
    "        self.labels = labels\n",
    "        self.imagery_dir = imagery_dir\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dish_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        dish_id = self.dish_ids[idx]\n",
    "        \n",
    "        # Load RGB image\n",
    "        img_path = os.path.join(self.imagery_dir, dish_id, \"rgb.png\")\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # Get labels\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "# Define transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare datasets and dataloaders - Fixed for macOS\n",
    "dish_ids = filtered_metadata['dish_id'].tolist()\n",
    "labels = filtered_metadata[TARGET_COLUMNS].values.astype(np.float32)\n",
    "\n",
    "# Split data\n",
    "train_ids, val_ids, train_labels, val_labels = train_test_split(\n",
    "    dish_ids, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = NutritionDataset(train_ids, train_labels, IMAGERY_DIR, train_transform)\n",
    "val_dataset = NutritionDataset(val_ids, val_labels, IMAGERY_DIR, val_transform)\n",
    "\n",
    "# Create dataloaders - Set num_workers=0 to avoid multiprocessing issues\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Initialize multiple models for comparison\n",
    "# Dictionary to store different model configurations\n",
    "model_configs = {\n",
    "    'SimpleConvNet': SimpleConvNet(num_outputs=4),\n",
    "    'DeepConvNet': DeepConvNet(num_outputs=4),\n",
    "    'MobileNetLike': MobileNetLike(num_outputs=4),\n",
    "    'ResNetFromScratch': ResNetFromScratch(num_outputs=4, use_pretrained=False),\n",
    "    'ResNetPretrained': ResNetFromScratch(num_outputs=4, use_pretrained=True)  # For comparison\n",
    "}\n",
    "\n",
    "# Select which model to train\n",
    "MODEL_NAME = 'DeepConvNet'  # Change this to try different models\n",
    "model = model_configs[MODEL_NAME].to(DEVICE)\n",
    "\n",
    "print(f\"Selected model: {MODEL_NAME}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.L1Loss()  # MAE loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Enhanced training and validation functions with progress tracking\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    batch_losses = []\n",
    "    \n",
    "    # Create progress bar\n",
    "    pbar = tqdm(loader, desc='Training', leave=False)\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(pbar):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_loss = loss.item()\n",
    "        total_loss += batch_loss\n",
    "        batch_losses.append(batch_loss)\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{batch_loss:.4f}',\n",
    "            'avg_loss': f'{np.mean(batch_losses):.4f}'\n",
    "        })\n",
    "    \n",
    "    pbar.close()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    # Create progress bar\n",
    "    pbar = tqdm(loader, desc='Validating', leave=False)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in pbar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            batch_loss = loss.item()\n",
    "            total_loss += batch_loss\n",
    "            \n",
    "            all_predictions.append(outputs.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({'loss': f'{batch_loss:.4f}'})\n",
    "    \n",
    "    pbar.close()\n",
    "    \n",
    "    predictions = np.concatenate(all_predictions)\n",
    "    labels = np.concatenate(all_labels)\n",
    "    \n",
    "    # Calculate percentage errors\n",
    "    percentage_errors = {}\n",
    "    for i, col in enumerate(TARGET_COLUMNS):\n",
    "        mae = mean_absolute_error(labels[:, i], predictions[:, i])\n",
    "        mean_val = labels[:, i].mean()\n",
    "        percentage_error = (mae / mean_val) * 100 if mean_val != 0 else 0\n",
    "        percentage_errors[col] = percentage_error\n",
    "    \n",
    "    return total_loss / len(loader), percentage_errors, predictions, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Verbose Training loop with model-specific saving\n",
    "best_val_loss = float('inf')\n",
    "history = {'train_loss': [], 'val_loss': [], 'percentage_errors': [], 'lr': []}\n",
    "\n",
    "# Print training configuration\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"Number of Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"Training Samples: {len(train_dataset)}\")\n",
    "print(f\"Validation Samples: {len(val_dataset)}\")\n",
    "print(f\"Target Columns: {TARGET_COLUMNS}\")\n",
    "print(f\"Model Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "# Model-specific save path\n",
    "MODEL_SAVE_PATH = f'best_nutrition_model_{MODEL_NAME}.pth'\n",
    "\n",
    "try:\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        # Get current learning rate\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        print(f\"\\nEPOCH {epoch+1}/{NUM_EPOCHS} | LR: {current_lr:.6f}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # Train\n",
    "        print(\"Training phase:\")\n",
    "        train_loss = train_epoch(model, train_loader, criterion, optimizer, DEVICE)\n",
    "        \n",
    "        # Validate\n",
    "        print(\"\\nValidation phase:\")\n",
    "        val_loss, percentage_errors, predictions, labels = validate(\n",
    "            model, val_loader, criterion, DEVICE\n",
    "        )\n",
    "        \n",
    "        # Calculate epoch time\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        \n",
    "        # Update scheduler\n",
    "        old_lr = optimizer.param_groups[0]['lr']\n",
    "        scheduler.step(val_loss)\n",
    "        new_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # Save history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['percentage_errors'].append(percentage_errors)\n",
    "        history['lr'].append(current_lr)\n",
    "        \n",
    "        # Print detailed results\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"EPOCH {epoch+1} RESULTS:\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}\")\n",
    "        print(f\"Epoch Time: {epoch_time:.2f} seconds\")\n",
    "        \n",
    "        # Calculate improvement\n",
    "        if epoch > 0:\n",
    "            train_improvement = (history['train_loss'][-2] - train_loss) / history['train_loss'][-2] * 100\n",
    "            val_improvement = (history['val_loss'][-2] - val_loss) / history['val_loss'][-2] * 100\n",
    "            print(f\"Train Loss Change: {train_improvement:+.2f}%\")\n",
    "            print(f\"Val Loss Change: {val_improvement:+.2f}%\")\n",
    "        \n",
    "        print(\"\\nPERCENTAGE ERRORS BY NUTRIENT (per 100g):\")\n",
    "        print(\"-\" * 40)\n",
    "        for nutrient, error in percentage_errors.items():\n",
    "            # Show trend if we have history\n",
    "            if len(history['percentage_errors']) > 1:\n",
    "                prev_error = history['percentage_errors'][-2][nutrient]\n",
    "                change = error - prev_error\n",
    "                print(f\"  {nutrient:20s}: {error:6.2f}% ({change:+.2f}%)\")\n",
    "            else:\n",
    "                print(f\"  {nutrient:20s}: {error:6.2f}%\")\n",
    "        \n",
    "        # Calculate average percentage error\n",
    "        avg_percentage_error = np.mean(list(percentage_errors.values()))\n",
    "        print(f\"  {'Average':20s}: {avg_percentage_error:6.2f}%\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            improvement_pct = (best_val_loss - val_loss) / best_val_loss * 100 if best_val_loss != float('inf') else 100\n",
    "            best_val_loss = val_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'best_val_loss': best_val_loss,\n",
    "                'model_name': MODEL_NAME,\n",
    "                'target_columns': TARGET_COLUMNS,\n",
    "                'history': history\n",
    "            }, MODEL_SAVE_PATH)\n",
    "            print(f\"\\n✓ NEW BEST MODEL SAVED to {MODEL_SAVE_PATH}! (Improvement: {improvement_pct:.2f}%)\")\n",
    "        else:\n",
    "            epochs_since_best = epoch - history['val_loss'].index(min(history['val_loss']))\n",
    "            print(f\"\\n  No improvement for {epochs_since_best} epoch(s)\")\n",
    "        \n",
    "        # Check if learning rate changed\n",
    "        if old_lr != new_lr:\n",
    "            print(f\"\\n⚡ Learning rate reduced: {old_lr:.6f} → {new_lr:.6f}\")\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n\\n⚠️  Training interrupted by user!\")\n",
    "    print(f\"Completed {epoch}/{NUM_EPOCHS} epochs\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n\\n❌ Error during training: {e}\")\n",
    "    print(f\"Failed at epoch: {epoch+1}\")\n",
    "    raise\n",
    "\n",
    "finally:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"TRAINING SUMMARY - {MODEL_NAME}\")\n",
    "    print(\"=\"*60)\n",
    "    if history['train_loss']:\n",
    "        print(f\"Final Train Loss: {history['train_loss'][-1]:.4f}\")\n",
    "        print(f\"Final Val Loss: {history['val_loss'][-1]:.4f}\")\n",
    "        print(f\"Best Val Loss: {best_val_loss:.4f}\")\n",
    "        print(f\"Total Epochs Completed: {len(history['train_loss'])}\")\n",
    "        print(f\"Model saved as: {MODEL_SAVE_PATH}\")\n",
    "        \n",
    "        # Final percentage errors\n",
    "        if history['percentage_errors']:\n",
    "            print(\"\\nFinal Percentage Errors (per 100g):\")\n",
    "            final_errors = history['percentage_errors'][-1]\n",
    "            for nutrient, error in final_errors.items():\n",
    "                print(f\"  {nutrient}: {error:.2f}%\")\n",
    "            \n",
    "            # Average error\n",
    "            avg_error = np.mean(list(final_errors.values()))\n",
    "            print(f\"\\nAverage Percentage Error: {avg_error:.2f}%\")\n",
    "    \n",
    "    # Save final history for this model\n",
    "    history_path = f'training_history_{MODEL_NAME}.pkl'\n",
    "    import pickle\n",
    "    with open(history_path, 'wb') as f:\n",
    "        pickle.dump(history, f)\n",
    "    print(f\"\\nTraining history saved to: {history_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10-continue\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss plot\n",
    "ax1.plot(history['train_loss'], label='Train Loss')\n",
    "ax1.plot(history['val_loss'], label='Val Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training and Validation Loss')\n",
    "ax1.legend()\n",
    "\n",
    "# Percentage error plot\n",
    "percentage_df = pd.DataFrame(history['percentage_errors'])\n",
    "for col in percentage_df.columns:\n",
    "    ax2.plot(percentage_df[col], label=col)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Percentage Error (%)')\n",
    "ax2.set_title('Percentage Errors by Nutrient')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "# Make sure BATCH_SIZE, val_ids, filtered_metadata, DEVICE, MODEL_NAME, val_loader, model are defined as in your environment.\n",
    "# For example:\n",
    "# BATCH_SIZE = 32 # Example value\n",
    "# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Example value\n",
    "# MODEL_NAME = \"your_model_name\" # Example value\n",
    "# val_ids = [...] # Should be defined\n",
    "# filtered_metadata = pd.DataFrame(...) # Should be defined\n",
    "# val_loader = ... # Should be defined\n",
    "# model = ... # Your model instance should be defined\n",
    "\n",
    "# Cell 11: Modified evaluation for per-100g predictions\n",
    "# Load best model and evaluate\n",
    "\n",
    "# Path to the saved checkpoint\n",
    "checkpoint_path = f'best_nutrition_model_{MODEL_NAME}.pth'\n",
    "\n",
    "# 1. Load the entire checkpoint dictionary.\n",
    "# Use weights_only=False because the checkpoint contains non-tensor data (like epoch, history, etc.),\n",
    "# and PyTorch 2.6+ defaults to weights_only=True, which would cause an UnpicklingError.\n",
    "# This is safe because you trust the source of the .pth file (it was saved by your Cell 10).\n",
    "print(f\"Loading checkpoint from {checkpoint_path}...\")\n",
    "checkpoint = torch.load(checkpoint_path, weights_only=False, map_location=DEVICE) # Added weights_only=False and map_location\n",
    "\n",
    "# 2. Load the model's state_dict from the checkpoint dictionary\n",
    "# Your saving code (Cell 10) stores the model's weights under the key 'model_state_dict'.\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval() # Set model to evaluation mode\n",
    "\n",
    "print(f\"Model {MODEL_NAME} loaded successfully and set to evaluation mode.\")\n",
    "\n",
    "# Get predictions on validation set\n",
    "print(f\"Evaluating {MODEL_NAME} on validation set...\")\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "all_dish_ids = []\n",
    "all_weights = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch_data in enumerate(tqdm(val_loader, desc='Predicting')):\n",
    "        # Assuming val_loader yields (images, labels) or a dict. Adjust if different.\n",
    "        if isinstance(batch_data, (list, tuple)):\n",
    "            images, labels_batch = batch_data\n",
    "        elif isinstance(batch_data, dict): # If your DataLoader returns a dictionary\n",
    "            images = batch_data['image'] # Adjust key if necessary\n",
    "            labels_batch = batch_data['labels'] # Adjust key if necessary\n",
    "        else:\n",
    "            raise TypeError(f\"Unexpected batch data type: {type(batch_data)}\")\n",
    "\n",
    "        images = images.to(DEVICE)\n",
    "        outputs = model(images)\n",
    "\n",
    "        all_predictions.append(outputs.cpu().numpy())\n",
    "        all_labels.append(labels_batch.cpu().numpy()) # Assuming labels_batch is a tensor\n",
    "\n",
    "        # Get dish IDs and weights for this batch\n",
    "        # Ensure val_ids is correctly populated and corresponds to val_loader's order\n",
    "        batch_start_index = i * val_loader.batch_size # Use val_loader.batch_size\n",
    "        batch_end_index = min(batch_start_index + val_loader.batch_size, len(val_ids))\n",
    "        current_batch_dish_ids = val_ids[batch_start_index:batch_end_index]\n",
    "        all_dish_ids.extend(current_batch_dish_ids)\n",
    "\n",
    "        # Get actual weights for conversion back to absolute values\n",
    "        for dish_id in current_batch_dish_ids:\n",
    "            # Ensure filtered_metadata is available and has 'dish_id' and 'weight' columns\n",
    "            weight_values = filtered_metadata[filtered_metadata['dish_id'] == dish_id]['weight'].values\n",
    "            if len(weight_values) > 0:\n",
    "                all_weights.append(weight_values[0])\n",
    "            else:\n",
    "                print(f\"Warning: No weight found for dish_id {dish_id}. Appending NaN or a default.\")\n",
    "                all_weights.append(np.nan) # Or handle as appropriate, e.g., skip or use a default\n",
    "\n",
    "# Ensure lengths match before concatenating, especially if some weights were not found\n",
    "min_len = min(len(all_predictions), len(all_labels), len(all_dish_ids), len(all_weights))\n",
    "if len(all_predictions) * val_loader.batch_size < len(val_ids) and val_loader.drop_last == False :\n",
    "     #This means your original code for batch_dish_ids may have an off-by-one if drop_last=False\n",
    "     #The val_ids slicing must exactly match the samples processed by the dataloader.\n",
    "     #The fix above using val_loader.batch_size and batch_start_index/batch_end_index should be more robust.\n",
    "     pass\n",
    "\n",
    "\n",
    "predictions_np = np.concatenate(all_predictions)\n",
    "labels_np = np.concatenate(all_labels)\n",
    "\n",
    "# Adjust slicing to the actual number of predictions made, which should match labels\n",
    "num_samples_processed = len(predictions_np)\n",
    "all_dish_ids = all_dish_ids[:num_samples_processed]\n",
    "all_weights = all_weights[:num_samples_processed]\n",
    "\n",
    "\n",
    "# Create DataFrame with per-100g values\n",
    "results_df = pd.DataFrame({\n",
    "    'dish_id': all_dish_ids,\n",
    "    'weight': all_weights,\n",
    "    'calories_per_100g_pred': predictions_np[:, 0],\n",
    "    'calories_per_100g_true': labels_np[:, 0],\n",
    "    'fat_per_100g_pred': predictions_np[:, 1],\n",
    "    'fat_per_100g_true': labels_np[:, 1],\n",
    "    'carbs_per_100g_pred': predictions_np[:, 2],\n",
    "    'carbs_per_100g_true': labels_np[:, 2],\n",
    "    'protein_per_100g_pred': predictions_np[:, 3],\n",
    "    'protein_per_100g_true': labels_np[:, 3]\n",
    "})\n",
    "\n",
    "# Calculate absolute values for comparison\n",
    "for nutrient in ['calories', 'fat', 'carbs', 'protein']:\n",
    "    results_df[f'{nutrient}_abs_pred'] = results_df[f'{nutrient}_per_100g_pred'] * results_df['weight'] / 100\n",
    "    results_df[f'{nutrient}_abs_true'] = results_df[f'{nutrient}_per_100g_true'] * results_df['weight'] / 100\n",
    "\n",
    "print(f\"\\nPredictions completed for {len(results_df)} samples\")\n",
    "if results_df['weight'].isnull().any():\n",
    "    print(\"Warning: Some weights were NaN. Absolute nutrient calculations might be affected for those rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Modified metrics calculation for per-100g\n",
    "def calculate_metrics_per_100g(results_df):\n",
    "    metrics_list = []\n",
    "    \n",
    "    # Calculate metrics for per-100g predictions\n",
    "    for nutrient in ['calories', 'fat', 'carbs', 'protein']:\n",
    "        per_100g_true = results_df[f'{nutrient}_per_100g_true'].values\n",
    "        per_100g_pred = results_df[f'{nutrient}_per_100g_pred'].values\n",
    "        \n",
    "        mae = mean_absolute_error(per_100g_true, per_100g_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(per_100g_true, per_100g_pred))\n",
    "        r2 = r2_score(per_100g_true, per_100g_pred)\n",
    "        \n",
    "        # Percentage error\n",
    "        mean_true = np.mean(per_100g_true)\n",
    "        percentage_error = (mae / mean_true) * 100 if mean_true != 0 else 0\n",
    "        \n",
    "        metrics_list.append({\n",
    "            'nutrient': f'{nutrient}_per_100g',\n",
    "            'MAE': mae,\n",
    "            'RMSE': rmse,\n",
    "            'R²': r2,\n",
    "            'Percentage Error': percentage_error,\n",
    "            'Mean True': mean_true,\n",
    "            'Mean Pred': np.mean(per_100g_pred)\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(metrics_list)\n",
    "\n",
    "# Calculate metrics\n",
    "metrics_df = calculate_metrics_per_100g(results_df)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"MODEL PERFORMANCE METRICS - {MODEL_NAME} (Per 100g)\")\n",
    "print(\"=\"*80)\n",
    "print(metrics_df.to_string(index=False, float_format='%.3f'))\n",
    "\n",
    "# Also show how this translates to absolute predictions\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EFFECTIVE ABSOLUTE PREDICTION ERRORS (using ground truth weight)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for nutrient in ['calories', 'fat', 'carbs', 'protein']:\n",
    "    abs_true = results_df[f'{nutrient}_abs_true'].values\n",
    "    abs_pred = results_df[f'{nutrient}_abs_pred'].values\n",
    "    \n",
    "    mae = mean_absolute_error(abs_true, abs_pred)\n",
    "    mean_true = np.mean(abs_true)\n",
    "    percentage_error = (mae / mean_true) * 100\n",
    "    \n",
    "    print(f\"{nutrient:10s}: {percentage_error:.2f}% error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd # Assuming results_df is a pandas DataFrame\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Ensure results_df is loaded or available from the previous cell (Cell 11)\n",
    "# For example:\n",
    "# results_df = pd.read_csv('path_to_your_results.csv') # Or however it's available\n",
    "\n",
    "# Cell 13: Visualize predictions vs actual values (per 100g)\n",
    "\n",
    "# Define the list of nutrient key prefixes to plot.\n",
    "# These MUST match the prefixes used in results_df for per-100g values\n",
    "# (e.g., 'calories_per_100g_true', 'calories_per_100g_pred').\n",
    "nutrients_to_plot_keys = [\n",
    "    'calories_per_100g',\n",
    "    'fat_per_100g',\n",
    "    'carbs_per_100g',    # Matches 'carbs_per_100g' used in results_df creation\n",
    "    'protein_per_100g'\n",
    "]\n",
    "\n",
    "num_nutrients_to_plot = len(nutrients_to_plot_keys)\n",
    "\n",
    "# Create subplots (2 rows, 3 columns allows up to 6 plots)\n",
    "# If you have exactly 4 nutrients, you might prefer a 2x2 layout.\n",
    "# For a dynamic layout:\n",
    "# ncols = 2\n",
    "# nrows = (num_nutrients_to_plot + ncols - 1) // ncols\n",
    "# fig, axes = plt.subplots(nrows, ncols, figsize=(7 * ncols, 6 * nrows))\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12)) # As per original code\n",
    "axes = axes.flatten() # Flatten to a 1D array for easy iteration\n",
    "\n",
    "for i, nutrient_key_prefix in enumerate(nutrients_to_plot_keys):\n",
    "    ax = axes[i]\n",
    "\n",
    "    true_col = f'{nutrient_key_prefix}_true'\n",
    "    pred_col = f'{nutrient_key_prefix}_pred'\n",
    "\n",
    "    # Defensive check: ensure columns exist in the DataFrame\n",
    "    if true_col not in results_df.columns or pred_col not in results_df.columns:\n",
    "        print(f\"Warning: Columns '{true_col}' or '{pred_col}' not found in results_df. Skipping plot for '{nutrient_key_prefix}'.\")\n",
    "        ax.set_title(f\"{nutrient_key_prefix.replace('_per_100g', '').capitalize()} - Data Missing\")\n",
    "        ax.axis('off') # Hide axis if data is missing\n",
    "        continue\n",
    "\n",
    "    # Get data\n",
    "    x_data = results_df[true_col].values\n",
    "    y_data = results_df[pred_col].values\n",
    "\n",
    "    # Remove NaN values to prevent errors in calculations and plotting\n",
    "    valid_indices = ~ (np.isnan(x_data) | np.isnan(y_data))\n",
    "    x_plot = x_data[valid_indices]\n",
    "    y_plot = y_data[valid_indices]\n",
    "\n",
    "    if len(x_plot) == 0: # Not enough data to plot\n",
    "        print(f\"Warning: No valid (non-NaN) data points for '{nutrient_key_prefix}'. Skipping plot.\")\n",
    "        ax.set_title(f\"{nutrient_key_prefix.replace('_per_100g', '').capitalize()} - No Valid Data\")\n",
    "        ax.axis('off')\n",
    "        continue\n",
    "\n",
    "    # Create scatter plot\n",
    "    ax.scatter(x_plot, y_plot, alpha=0.5, s=50, edgecolors='k', linewidth=0.5)\n",
    "\n",
    "    # Add perfect prediction line\n",
    "    # Ensure min/max are calculated on the actual plotted data (x_plot, y_plot)\n",
    "    min_val = min(x_plot.min(), y_plot.min()) if len(x_plot) > 0 else 0\n",
    "    max_val = max(x_plot.max(), y_plot.max()) if len(x_plot) > 0 else 1\n",
    "    if min_val == max_val: # Avoid issues if all points are identical\n",
    "        min_val -= 0.5\n",
    "        max_val += 0.5\n",
    "    ax.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect Prediction')\n",
    "\n",
    "    # Add trend line (polyfit requires at least 2 points for degree 1)\n",
    "    if len(x_plot) > 1:\n",
    "        z = np.polyfit(x_plot, y_plot, 1)\n",
    "        p = np.poly1d(z)\n",
    "        trend_line_x_points = np.array([x_plot.min(), x_plot.max()]) # Use min/max of actual data\n",
    "        ax.plot(trend_line_x_points, p(trend_line_x_points), \"b-\", alpha=0.8, label=f'Trend: y={z[0]:.2f}x+{z[1]:.2f}')\n",
    "        \n",
    "        # Calculate R² score\n",
    "        r2 = r2_score(x_plot, y_plot)\n",
    "        r2_text = f\"R² = {r2:.3f}\"\n",
    "    else:\n",
    "        # Not enough points for trend line or R²\n",
    "        ax.plot([], [], \"b-\", alpha=0.8, label='Trend: N/A (too few points)') # Placeholder for legend\n",
    "        r2_text = \"R² = N/A (too few points)\"\n",
    "\n",
    "\n",
    "    # Labels and title\n",
    "    # Create a more display-friendly nutrient name\n",
    "    display_nutrient_name = nutrient_key_prefix.replace('_per_100g', '').replace('_', ' ').capitalize()\n",
    "    \n",
    "    ax.set_xlabel(f'True {display_nutrient_name} (per 100g)')\n",
    "    ax.set_ylabel(f'Predicted {display_nutrient_name} (per 100g)')\n",
    "    ax.set_title(f'{display_nutrient_name} (per 100g)\\n{r2_text}')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Hide any unused subplots if num_nutrients_to_plot < total number of axes\n",
    "for j in range(num_nutrients_to_plot, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "# Adjust layout to prevent overlap and make space for suptitle\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.97]) # rect=[left, bottom, right, top]\n",
    "plt.suptitle('Predictions vs True Values (per 100g)', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Error distribution analysis\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, nutrient in enumerate(TARGET_COLUMNS):\n",
    "    ax = axes[i]\n",
    "    true_col = f'{nutrient}_true'\n",
    "    pred_col = f'{nutrient}_pred'\n",
    "    \n",
    "    # Calculate errors\n",
    "    errors = results_df[pred_col] - results_df[true_col]\n",
    "    relative_errors = (errors / results_df[true_col]) * 100\n",
    "    \n",
    "    # Remove outliers for better visualization\n",
    "    q1 = relative_errors.quantile(0.25)\n",
    "    q3 = relative_errors.quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "    filtered_errors = relative_errors[(relative_errors >= lower_bound) & (relative_errors <= upper_bound)]\n",
    "    \n",
    "    # Create histogram\n",
    "    ax.hist(filtered_errors, bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    ax.axvline(x=0, color='red', linestyle='--', linewidth=2, label='Zero Error')\n",
    "    ax.axvline(x=filtered_errors.mean(), color='green', linestyle='-', linewidth=2, \n",
    "               label=f'Mean: {filtered_errors.mean():.1f}%')\n",
    "    \n",
    "    # Labels\n",
    "    ax.set_xlabel('Relative Error (%)')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_title(f'{nutrient.capitalize()} Error Distribution')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: Show sample predictions with images\n",
    "def show_predictions_with_images(n_samples=6):\n",
    "    # Get random samples\n",
    "    sample_indices = np.random.choice(len(results_df), n_samples, replace=False)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, ax in enumerate(axes):\n",
    "        if idx >= n_samples:\n",
    "            ax.axis('off')\n",
    "            continue\n",
    "            \n",
    "        sample_idx = sample_indices[idx]\n",
    "        dish_id = results_df.iloc[sample_idx]['dish_id']\n",
    "        \n",
    "        # Load and display image\n",
    "        img_path = os.path.join(IMAGERY_DIR, dish_id, \"rgb.png\")\n",
    "        img = Image.open(img_path)\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "        \n",
    "        # Create prediction text\n",
    "        pred_text = \"Predicted:\\n\"\n",
    "        true_text = \"Actual:\\n\"\n",
    "        \n",
    "        for nutrient in TARGET_COLUMNS:\n",
    "            pred_val = results_df.iloc[sample_idx][f'{nutrient}_pred']\n",
    "            true_val = results_df.iloc[sample_idx][f'{nutrient}_true']\n",
    "            error = abs(pred_val - true_val) / true_val * 100\n",
    "            \n",
    "            if nutrient == 'calories':\n",
    "                pred_text += f\"Cal: {pred_val:.0f}\\n\"\n",
    "                true_text += f\"Cal: {true_val:.0f} ({error:.1f}%)\\n\"\n",
    "            elif nutrient == 'weight':\n",
    "                pred_text += f\"Weight: {pred_val:.0f}g\\n\"\n",
    "                true_text += f\"Weight: {true_val:.0f}g ({error:.1f}%)\\n\"\n",
    "            else:\n",
    "                pred_text += f\"{nutrient.capitalize()}: {pred_val:.1f}g\\n\"\n",
    "                true_text += f\"{nutrient.capitalize()}: {true_val:.1f}g ({error:.1f}%)\\n\"\n",
    "        \n",
    "        # Add text to image\n",
    "        ax.text(0.02, 0.98, pred_text, transform=ax.transAxes, \n",
    "                verticalalignment='top', bbox=dict(boxstyle='round', \n",
    "                facecolor='lightblue', alpha=0.8), fontsize=9)\n",
    "        ax.text(0.98, 0.98, true_text, transform=ax.transAxes, \n",
    "                verticalalignment='top', horizontalalignment='right',\n",
    "                bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8), fontsize=9)\n",
    "        \n",
    "        ax.set_title(f\"Dish: {dish_id}\", fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('Sample Predictions vs Ground Truth', fontsize=14, y=1.02)\n",
    "    plt.show()\n",
    "\n",
    "show_predictions_with_images(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
