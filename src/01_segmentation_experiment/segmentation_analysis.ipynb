{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Calorie Prediction on Nutrition5k with segmentation\n",
    "\n",
    " This notebook implements a pipeline for calorie/macronutrient prediction from Nutrition5k dish images.\n",
    "\n",
    " It uses U-2-Net for food segmentation before training a\n",
    "\n",
    " ResNet-based CNN for calorie/macronutrient regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import multiprocessing\n",
    "import cv2\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Tuple, Dict, Optional, Any\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "# Attempt to import U-2-Net model structure\n",
    "# You need to ensure 'u2net_model.py' is in your PYTHONPATH or the same directory.\n",
    "# This is a placeholder import. You'll need the actual U2NET model definition.\n",
    "try:\n",
    "    from u2net_model import U2NET # Or U2NETP, depending on the model used\n",
    "    logger.info(\"Successfully imported U2NET from u2net_model.py\")\n",
    "except ImportError:\n",
    "    logger.warning(\"Could not import U2NET from u2net_model.py. Segmentation will not work.\")\n",
    "    logger.warning(\"Please ensure u2net_model.py (containing the U2NET class definition) is in your project directory or Python path.\")\n",
    "    U2NET = None # Placeholder if import fails\n",
    "\n",
    "\n",
    "\n",
    "# Ensure PyTorch multiprocessing works correctly\n",
    "if multiprocessing.get_start_method(allow_none=True) != 'spawn':\n",
    "    try:\n",
    "        multiprocessing.set_start_method('spawn', force=True)\n",
    "        logger.info(\"Set multiprocessing start method to 'spawn'.\")\n",
    "    except RuntimeError as e:\n",
    "        logger.warning(f\"Could not set multiprocessing start method to 'spawn': {e}\")\n",
    "\n",
    "\n",
    "def parse_nutrition_csv(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Parses the Nutrition5k dish metadata CSV file which has an irregular structure.\n",
    "    Extracts dish_id, calories, weight, fat, carbs, and protein.\n",
    "    \"\"\"\n",
    "    dishes_data = []\n",
    "    try:\n",
    "        with open(file_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split(\",\")\n",
    "                if not parts or not parts[0].startswith(\"dish_\"): # Ensure parts is not empty and starts with \"dish_\"\n",
    "                    continue\n",
    "                try:\n",
    "                    dish_id = parts[0]\n",
    "                    # Ensure there are enough parts for all expected values\n",
    "                    if len(parts) > 5:\n",
    "                        dishes_data.append({\n",
    "                            \"dish_id\": dish_id,\n",
    "                            \"calories\": float(parts[1]),\n",
    "                            \"weight\": float(parts[2]), # Mass\n",
    "                            \"fat\": float(parts[3]),\n",
    "                            \"carbs\": float(parts[4]),\n",
    "                            \"protein\": float(parts[5]),\n",
    "                        })\n",
    "                    else:\n",
    "                        logger.warning(f\"Skipping malformed 'dish_' line (not enough parts) in {file_path}: {line.strip()}\")\n",
    "                except ValueError as e:\n",
    "                    logger.warning(f\"ValueError for 'dish_' line in {file_path}: {line.strip()} - {e}\")\n",
    "                except IndexError as e:\n",
    "                    logger.warning(f\"IndexError for 'dish_' line in {file_path}: {line.strip()} - {e}\")\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"Metadata file not found: {file_path}\")\n",
    "        return pd.DataFrame() # Return empty DataFrame if file not found\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An unexpected error occurred while parsing {file_path}: {e}\", exc_info=True)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    return pd.DataFrame(dishes_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Configuration class for the training pipeline.\"\"\"\n",
    "    # Paths\n",
    "    BASE_DIR: str = \"./datasets/nutrition5k\" # UPDATE THIS PATH\n",
    "    IMAGERY_DIR: str = field(init=False)\n",
    "    METADATA_FILE_CAFE1: str = field(init=False)\n",
    "    METADATA_FILE_CAFE2: str = field(init=False)\n",
    "    RGB_IMAGE_FILENAME: str = \"rgb.png\"\n",
    "\n",
    "    # Segmentation Model (U-2-Net)\n",
    "    # IMPORTANT: Ensure u2net_model.py is present and U2NET_WEIGHTS_PATH points to the .pth file\n",
    "    U2NET_MODEL_DIR: str = \"./models/\" # Directory expected to contain u2net_model.py\n",
    "    U2NET_WEIGHTS_PATH: str = \"./models/u2net.pth\" # UPDATE if using a different name or path for U-2-Net weights\n",
    "    SEGMENTATION_INPUT_SIZE: Tuple[int, int] = (320, 320) # U-2-Net typical input size\n",
    "\n",
    "    # Processed Data Paths\n",
    "    PROCESSED_SEGMENTED_DIR: str = field(init=False) # For saving/loading segmented images\n",
    "    PRECOMPUTE_SEGMENTATION: bool = True # Master switch to use and save segmentation\n",
    "    OVERWRITE_EXISTING_SEGMENTATION: bool = False # If True, re-segments even if file exists\n",
    "\n",
    "    # CNN Model & Training\n",
    "    DEVICE: torch.device = field(default_factory=lambda: torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "    NUM_WORKERS: int = min(4, os.cpu_count() // 2 if os.cpu_count() else 1)\n",
    "    PIN_MEMORY: bool = field(default_factory=lambda: torch.cuda.is_available())\n",
    "    BATCH_SIZE: int = 16\n",
    "    LEARNING_RATE: float = 1e-3\n",
    "    NUM_EPOCHS: int = 100\n",
    "    PATIENCE_EARLY_STOPPING: int = 10\n",
    "    TARGET_COLUMN: str = \"calories\"\n",
    "    RANDOM_STATE: int = 42\n",
    "    CNN_INPUT_IMG_SIZE: Tuple[int, int] = (224, 224)\n",
    "    \n",
    "    # GPU Caching\n",
    "    USE_GPU_CACHING: bool = True # <--- ADD THIS\n",
    "    GPU_CACHE_SAFETY_MARGIN: float = 0.8 # Use 80% of free VRAM for caching estimate\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.IMAGERY_DIR = os.path.join(self.BASE_DIR, \"imagery/realsense_overhead\")\n",
    "        self.PROCESSED_SEGMENTED_DIR = os.path.join(self.BASE_DIR, \"imagery/u2net_segmented_food\")\n",
    "        self.METADATA_FILE_CAFE1 = os.path.join(self.BASE_DIR, \"metadata/dish_metadata_cafe1.csv\")\n",
    "        self.METADATA_FILE_CAFE2 = os.path.join(self.BASE_DIR, \"metadata/dish_metadata_cafe2.csv\")\n",
    "\n",
    "        if self.DEVICE.type == \"cuda\":\n",
    "            torch.backends.cudnn.benchmark = True\n",
    "        logger.info(f\"Configuration initialized. Device set to: {self.DEVICE}\")\n",
    "        if self.PRECOMPUTE_SEGMENTATION:\n",
    "            logger.info(f\"U-2-Net segmentation enabled. Processed images stored in: {self.PROCESSED_SEGMENTED_DIR}\")\n",
    "            os.makedirs(self.PROCESSED_SEGMENTED_DIR, exist_ok=True)\n",
    "        if self.USE_GPU_CACHING and self.DEVICE.type == 'cuda':\n",
    "            logger.info(f\"GPU Caching for datasets is ENABLED.\")\n",
    "        elif self.USE_GPU_CACHING and self.DEVICE.type == 'cpu':\n",
    "            logger.warning(\"GPU Caching requested but device is CPU. Caching will not occur.\")\n",
    "            self.USE_GPU_CACHING = False\n",
    "\n",
    "config = Config()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 3. U-2-Net Segmentation Module\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_u2net_model(model_path: str, device: torch.device) -> Optional[nn.Module]:\n",
    "    \"\"\"Loads the U-2-Net model and its pretrained weights.\"\"\"\n",
    "    if U2NET is None:\n",
    "        logger.error(\"U2NET class is not available (failed import). Cannot load model.\")\n",
    "        return None\n",
    "    try:\n",
    "        # Assuming U2NET class takes in_ch=3 (RGB) and out_ch=1 (grayscale mask)\n",
    "        model = U2NET(in_ch=3, out_ch=1)\n",
    "        if torch.cuda.is_available():\n",
    "            model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "        else:\n",
    "            model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        logger.info(f\"U-2-Net model loaded successfully from {model_path} to {device}.\")\n",
    "        return model\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"U-2-Net weights file not found at {model_path}.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading U-2-Net model: {e}\", exc_info=True)\n",
    "        return None\n",
    "\n",
    "def get_segmentation_mask_u2net(image_pil: Image.Image,\n",
    "                                u2net_model: nn.Module,\n",
    "                                input_size: Tuple[int, int] = (320, 320),\n",
    "                                device: torch.device = torch.device(\"cpu\")) -> Optional[Image.Image]:\n",
    "    \"\"\"\n",
    "    Generates a food segmentation mask using a preloaded U-2-Net model.\n",
    "    Returns a PIL Image (grayscale mask).\n",
    "    \"\"\"\n",
    "    if u2net_model is None:\n",
    "        return None\n",
    "\n",
    "    original_size = image_pil.size # (width, height)\n",
    "\n",
    "    # Prepare image for U-2-Net\n",
    "    img_tensor = transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]), # Typical ImageNet normalization\n",
    "    ])(image_pil).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = u2net_model(img_tensor)\n",
    "    \n",
    "    # U-2-Net typically returns multiple outputs (d0, d1, ... d6)\n",
    "    # d0 is usually the finest, full-resolution mask\n",
    "    pred_mask_tensor = outputs[0].squeeze().cpu().detach() # Get d0, remove batch, to CPU\n",
    "\n",
    "    # Normalize mask to 0-1 range (it's usually already close after sigmoid in model)\n",
    "    pred_mask_tensor = (pred_mask_tensor - torch.min(pred_mask_tensor)) / \\\n",
    "                       (torch.max(pred_mask_tensor) - torch.min(pred_mask_tensor) + 1e-8)\n",
    "\n",
    "    mask_pil = transforms.ToPILImage()(pred_mask_tensor.unsqueeze(0)) # Add channel dim back for ToPILImage\n",
    "    \n",
    "    # Resize mask to original image size\n",
    "    mask_pil = mask_pil.resize(original_size, Image.BILINEAR)\n",
    "    return mask_pil.convert(\"L\") # Convert to grayscale\n",
    "\n",
    "def apply_mask_and_crop(original_image_pil: Image.Image, mask_pil: Image.Image, threshold: float = 0.5) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Applies a binary mask to an image and crops to the bounding box of the masked area.\n",
    "    \"\"\"\n",
    "    binary_mask_np = (np.array(mask_pil) / 255.0 > threshold).astype(np.uint8)\n",
    "    \n",
    "    # Find contours to get bounding box\n",
    "    contours, _ = cv2.findContours(binary_mask_np, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    if not contours:\n",
    "        logger.warning(\"No contours found in mask. Returning original image (possibly uncropped).\")\n",
    "        # Fallback: create a black image or return a centrally cropped original\n",
    "        # For simplicity here, we'll make it so the whole image is kept if no contour\n",
    "        # which means it will just be resized later by the CNN's transforms.\n",
    "        # A better fallback might be a central crop of the original.\n",
    "        return original_image_pil\n",
    "\n",
    "    # Get bounding box of the largest contour (assuming it's the main food item)\n",
    "    largest_contour = max(contours, key=cv2.contourArea)\n",
    "    x, y, w, h = cv2.boundingRect(largest_contour)\n",
    "\n",
    "    if w == 0 or h == 0: # Should not happen if contours were found\n",
    "        return original_image_pil\n",
    "\n",
    "\n",
    "    # Create a 3-channel version of the binary mask for applying to RGB image\n",
    "    mask_rgb_np = np.stack([binary_mask_np*255]*3, axis=-1) # PIL expects 0-255 for mask\n",
    "    mask_for_pil = Image.fromarray(mask_rgb_np.astype(np.uint8)).convert(\"1\") # Convert to binary PIL mask\n",
    "\n",
    "\n",
    "    # Apply mask (make background black)\n",
    "    # Create a black background image\n",
    "    black_background = Image.new(\"RGB\", original_image_pil.size, (0,0,0))\n",
    "    # Composite original image onto black background using the mask\n",
    "    masked_image_pil = Image.composite(original_image_pil, black_background, mask_for_pil)\n",
    "\n",
    "\n",
    "    # Crop to bounding box\n",
    "    cropped_image_pil = masked_image_pil.crop((x, y, x + w, y + h))\n",
    "    \n",
    "    if cropped_image_pil.width == 0 or cropped_image_pil.height == 0:\n",
    "        logger.warning(\"Cropped image has zero dimension. Returning original masked (uncropped) image.\")\n",
    "        return masked_image_pil # Fallback to uncropped masked image\n",
    "\n",
    "    return cropped_image_pil\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## 4. Nutrition5k Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Nutrition5kDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 metadata_df: pd.DataFrame,\n",
    "                 imagery_dir: str,\n",
    "                 processed_dir: str,\n",
    "                 rgb_filename: str,\n",
    "                 transform: transforms.Compose,\n",
    "                 cnn_input_size: Tuple[int, int],\n",
    "                 segmentation_model: Optional[nn.Module] = None,\n",
    "                 segmentation_input_size: Optional[Tuple[int, int]] = None,\n",
    "                 precompute_segmentation: bool = True,\n",
    "                 overwrite_segmentation: bool = False,\n",
    "                 target_column: str = \"calories\",\n",
    "                 device: torch.device = torch.device(\"cpu\")):\n",
    "        self.metadata_df = metadata_df\n",
    "        self.imagery_dir = imagery_dir\n",
    "        self.processed_dir = processed_dir # Where segmented images are saved/loaded\n",
    "        self.rgb_filename = rgb_filename\n",
    "        self.transform = transform # For CNN input\n",
    "        self.cnn_input_size = cnn_input_size\n",
    "\n",
    "        self.segmentation_model = segmentation_model\n",
    "        self.segmentation_input_size = segmentation_input_size\n",
    "        self.precompute_segmentation = precompute_segmentation\n",
    "        self.overwrite_segmentation = overwrite_segmentation\n",
    "        self.target_column = target_column\n",
    "        self.device = device # Device for segmentation model\n",
    "\n",
    "        if self.precompute_segmentation and not os.path.exists(self.processed_dir):\n",
    "            os.makedirs(self.processed_dir, exist_ok=True)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.metadata_df)\n",
    "\n",
    "    def _get_segmented_image_path(self, dish_id: str) -> str:\n",
    "        dish_folder = os.path.join(self.processed_dir, dish_id)\n",
    "        return os.path.join(dish_folder, self.rgb_filename) # Using same filename for segmented\n",
    "\n",
    "    def _load_and_segment_image(self, dish_id: str) -> Optional[Image.Image]:\n",
    "        \"\"\"Loads original, segments, saves if needed, and returns segmented PIL image.\"\"\"\n",
    "        original_img_path = os.path.join(self.imagery_dir, dish_id, self.rgb_filename)\n",
    "        if not os.path.exists(original_img_path):\n",
    "            logger.warning(f\"Original image not found for {dish_id} at {original_img_path}\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            original_pil = Image.open(original_img_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading original image {original_img_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "        # if not self.segmentation_model or not self.segmentation_input_size:\n",
    "        #     logger.debug(f\"No segmentation model provided or configured for {dish_id}. Using original.\")\n",
    "        #     # Fallback: central square crop if no segmentation\n",
    "        #     w, h = original_pil.size\n",
    "        #     side = min(w,h)\n",
    "        #     return original_pil.crop(((w-side)//2, (h-side)//2, (w+side)//2, (h+side)//2))\n",
    "        \n",
    "        # mask_pil = get_segmentation_mask_u2net(original_pil, self.segmentation_model,\n",
    "        #                                        self.segmentation_input_size, self.device)\n",
    "        # if mask_pil is None:\n",
    "        #     logger.warning(f\"Segmentation failed for {dish_id}. Using original (center-cropped).\")\n",
    "        #     w, h = original_pil.size; side = min(w,h)\n",
    "        #     return original_pil.crop(((w-side)//2, (h-side)//2, (w+side)//2, (h+side)//2))\n",
    "\n",
    "        if not self.segmentation_model or not self.segmentation_input_size:\n",
    "            logger.debug(f\"No segmentation model provided or configured for {dish_id}. Using original (resized).\")\n",
    "            # Fallback: resize the original image instead of cropping\n",
    "            return original_pil.resize(self.cnn_input_size, Image.BILINEAR)\n",
    "\n",
    "        mask_pil = get_segmentation_mask_u2net(original_pil, self.segmentation_model,\n",
    "                               self.segmentation_input_size, self.device)\n",
    "        if mask_pil is None:\n",
    "            logger.warning(f\"Segmentation failed for {dish_id}. Using original (resized).\")\n",
    "            return original_pil.resize(self.cnn_input_size, Image.BILINEAR)\n",
    "\n",
    "\n",
    "        final_segmented_pil = apply_mask_and_crop(original_pil, mask_pil)\n",
    "\n",
    "        if self.precompute_segmentation: # Save the final segmented & cropped image\n",
    "            segmented_path = self._get_segmented_image_path(dish_id)\n",
    "            os.makedirs(os.path.dirname(segmented_path), exist_ok=True)\n",
    "            try:\n",
    "                final_segmented_pil.save(segmented_path)\n",
    "                logger.debug(f\"Saved segmented image for {dish_id} to {segmented_path}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error saving segmented image {segmented_path}: {e}\")\n",
    "        \n",
    "        return final_segmented_pil\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        dish_info = self.metadata_df.iloc[idx]\n",
    "        dish_id = dish_info[\"dish_id\"]\n",
    "        label = torch.tensor([dish_info[self.target_column]], dtype=torch.float32)\n",
    "\n",
    "        processed_image_pil: Optional[Image.Image] = None\n",
    "\n",
    "        if self.precompute_segmentation and not self.overwrite_segmentation:\n",
    "            segmented_path = self._get_segmented_image_path(dish_id)\n",
    "            if os.path.exists(segmented_path):\n",
    "                try:\n",
    "                    processed_image_pil = Image.open(segmented_path).convert(\"RGB\")\n",
    "                    logger.debug(f\"Loaded pre-segmented image for {dish_id} from {segmented_path}\")\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Error loading pre-segmented image {segmented_path}, re-segmenting: {e}\")\n",
    "                    processed_image_pil = None # Force re-segmentation\n",
    "\n",
    "        if processed_image_pil is None: # Not found, or overwrite, or precompute is off but model exists\n",
    "            processed_image_pil = self._load_and_segment_image(dish_id)\n",
    "\n",
    "        if processed_image_pil is None: # If all attempts failed\n",
    "            logger.error(f\"Failed to load or segment image for {dish_id}. Returning zeros.\")\n",
    "            # Return a dummy tensor that matches expected output dimensions\n",
    "            dummy_image = torch.zeros(3, *self.cnn_input_size)\n",
    "            return dummy_image, label # Still return label to avoid dataloader issues\n",
    "\n",
    "        # Apply CNN transforms\n",
    "        image_tensor = self.transform(processed_image_pil)\n",
    "        \n",
    "        return image_tensor, label\n",
    "\n",
    "def check_gpu_memory_for_caching(num_samples: int, sample_image_tensor_shape: Tuple, \n",
    "                                 sample_label_tensor_shape: Tuple, \n",
    "                                 device: torch.device,\n",
    "                                 safety_margin: float = 0.8) -> bool:\n",
    "    \"\"\"Estimates if there's enough GPU VRAM for caching.\"\"\"\n",
    "    if device.type != 'cuda' or not torch.cuda.is_available():\n",
    "        logger.info(\"Not on CUDA device or CUDA not available. GPU caching check skipped.\")\n",
    "        return False\n",
    "    if num_samples == 0:\n",
    "        return True # Nothing to cache\n",
    "\n",
    "    try:\n",
    "        # Estimate memory per sample\n",
    "        bytes_per_image = np.prod(sample_image_tensor_shape) * 4 # Assuming float32 (4 bytes)\n",
    "        bytes_per_label = np.prod(sample_label_tensor_shape) * 4 # Assuming float32\n",
    "        total_bytes_per_sample = bytes_per_image + bytes_per_label\n",
    "        estimated_vram_needed_mb = (num_samples * total_bytes_per_sample) / (1024**2)\n",
    "\n",
    "        # Get free GPU memory\n",
    "        torch.cuda.empty_cache() # Try to free up cached memory\n",
    "        free_vram_bytes, _ = torch.cuda.mem_get_info(device)\n",
    "        free_vram_mb = free_vram_bytes / (1024**2)\n",
    "        usable_vram_mb = free_vram_mb * safety_margin\n",
    "\n",
    "        logger.info(f\"Estimated VRAM for caching: {estimated_vram_needed_mb:.2f} MB\")\n",
    "        logger.info(f\"Available (usable with {safety_margin*100}%) VRAM: {usable_vram_mb:.2f} MB (Total Free: {free_vram_mb:.2f} MB)\")\n",
    "\n",
    "        return usable_vram_mb > estimated_vram_needed_mb\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error checking GPU memory: {e}\", exc_info=True)\n",
    "        return False\n",
    "\n",
    "class CachedDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A Dataset wrapper that caches all data from a base_dataset to a specified device.\n",
    "    The base_dataset is expected to return fully processed tensors.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_dataset: Dataset, device: torch.device, dataset_name: str = \"Dataset\"):\n",
    "        self.base_dataset = base_dataset\n",
    "        self.device = device\n",
    "        self.images: List[torch.Tensor] = []\n",
    "        self.labels: List[torch.Tensor] = []\n",
    "        \n",
    "        if len(self.base_dataset) == 0:\n",
    "            logger.warning(f\"Base {dataset_name} for caching is empty. CachedDataset will also be empty.\")\n",
    "            return\n",
    "\n",
    "        logger.info(f\"Caching {len(self.base_dataset)} samples from {dataset_name} to {self.device}...\")\n",
    "        for i in tqdm(range(len(self.base_dataset)), desc=f\"Caching {dataset_name} to {self.device}\", leave=False):\n",
    "            try:\n",
    "                img_tensor, label_tensor = self.base_dataset[i] # __getitem__ from Nutrition5kDataset\n",
    "                self.images.append(img_tensor.to(self.device))\n",
    "                self.labels.append(label_tensor.to(self.device))\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error caching sample {i} from {dataset_name}: {e}. Skipping this sample in cache.\")\n",
    "        \n",
    "        if not self.images:\n",
    "             logger.warning(f\"No samples were successfully cached for {dataset_name}. CachedDataset is effectively empty.\")\n",
    "        else:\n",
    "            logger.info(f\"Caching for {dataset_name} complete. {len(self.images)} samples cached to {self.device}.\")\n",
    "            \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.images)\n",
    "        \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        return self.images[idx], self.labels[idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## 5. Model Architecture (ResNet-based Regressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaloriePredictorCNN(nn.Module):\n",
    "    def __init__(self, num_outputs: int = 1, pretrained: bool = True):\n",
    "        super(CaloriePredictorCNN, self).__init__()\n",
    "        self.name = \"ResNet50CaloriePredictor\"\n",
    "        if pretrained:\n",
    "            weights = ResNet50_Weights.IMAGENET1K_V2 # Or V1\n",
    "            self.base_model = resnet50(weights=weights)\n",
    "            logger.info(\"Loaded ResNet50 with ImageNet pretrained weights.\")\n",
    "        else:\n",
    "            self.base_model = resnet50(weights=None)\n",
    "            logger.info(\"Loaded ResNet50 without pretrained weights.\")\n",
    "\n",
    "        num_ftrs = self.base_model.fc.in_features\n",
    "        # Replace the final fully connected layer for regression\n",
    "        self.base_model.fc = nn.Sequential(\n",
    "            nn.Linear(num_ftrs, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_outputs) # Predicting calories (1 output)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.base_model(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## 6. Training and Evaluation Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self, train_loader: DataLoader, val_loader: Optional[DataLoader],\n",
    "                 config_params: Config, model_name: str = \"model\"):\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.config = config_params\n",
    "        self.device = config_params.DEVICE\n",
    "        self.model_name = model_name\n",
    "        self.results: Dict[str, Any] = {}\n",
    "        self.use_non_blocking = self.train_loader.pin_memory and self.device.type == 'cuda' if self.train_loader else False\n",
    "\n",
    "\n",
    "    def _compute_metrics(self, predictions: np.ndarray, ground_truth: np.ndarray) -> Dict[str, float]:\n",
    "        # Ensure inputs are 1D for regression metrics\n",
    "        predictions = predictions.flatten()\n",
    "        ground_truth = ground_truth.flatten()\n",
    "        \n",
    "        mae = mean_absolute_error(ground_truth, predictions)\n",
    "        rmse = np.sqrt(mean_squared_error(ground_truth, predictions))\n",
    "        r2 = r2_score(ground_truth, predictions)\n",
    "        return {'mae': mae, 'rmse': rmse, 'r2': r2}\n",
    "\n",
    "    def _run_epoch(self, model: nn.Module, data_loader: DataLoader, criterion: nn.Module,\n",
    "                   optimizer: Optional[optim.Optimizer] = None, epoch_num: int = 0, phase: str = \"Train\") -> float:\n",
    "        is_train = phase == \"Train\"\n",
    "        if is_train:\n",
    "            model.train()\n",
    "            if optimizer is None:\n",
    "                raise ValueError(\"Optimizer must be provided for training phase.\")\n",
    "        else:\n",
    "            model.eval()\n",
    "\n",
    "        total_loss = 0.0\n",
    "        num_samples = 0\n",
    "        \n",
    "        pbar_desc = f\"Epoch {epoch_num+1}/{self.config.NUM_EPOCHS} [{phase}]\"\n",
    "        pbar = tqdm(data_loader, desc=pbar_desc, leave=False, dynamic_ncols=True)\n",
    "\n",
    "        for inputs, labels in pbar:\n",
    "            inputs = inputs.to(self.device, non_blocking=self.use_non_blocking if is_train else False)\n",
    "            labels = labels.to(self.device, non_blocking=self.use_non_blocking if is_train else False)\n",
    "\n",
    "            if is_train:\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "            \n",
    "            with torch.set_grad_enabled(is_train):\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                if is_train:\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            num_samples += inputs.size(0)\n",
    "            pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "        \n",
    "        if num_samples == 0: return 0.0 # Avoid division by zero if dataloader is empty\n",
    "        return total_loss / num_samples\n",
    "\n",
    "\n",
    "    def train_and_evaluate_model(self, model: nn.Module) -> Dict[str, Any]:\n",
    "        logger.info(f\"Training {self.model_name} on {self.device}...\")\n",
    "        model.to(self.device)\n",
    "        \n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=self.config.LEARNING_RATE, weight_decay=1e-4)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.2, \n",
    "                                                         patience=self.config.PATIENCE_EARLY_STOPPING // 2, verbose=True)\n",
    "\n",
    "        best_val_loss = float(\"inf\")\n",
    "        epochs_no_improve = 0\n",
    "        train_losses, val_losses = [], []\n",
    "        model_save_path = f\"{self.model_name}_best_state.pth\"\n",
    "\n",
    "        for epoch in range(self.config.NUM_EPOCHS):\n",
    "            train_loss = self._run_epoch(model, self.train_loader, criterion, optimizer, epoch, phase=\"Train\")\n",
    "            train_losses.append(train_loss)\n",
    "            \n",
    "            val_loss = float(\"inf\")\n",
    "            if self.val_loader and len(self.val_loader.dataset) > 0:\n",
    "                val_loss = self._run_epoch(model, self.val_loader, criterion, epoch_num=epoch, phase=\"Valid\")\n",
    "                val_losses.append(val_loss)\n",
    "            \n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            log_msg = f\"Epoch {epoch+1}/{self.config.NUM_EPOCHS} | Train Loss: {train_loss:.4f} | LR: {current_lr:.1e}\"\n",
    "            if self.val_loader and len(self.val_loader.dataset) > 0:\n",
    "                 log_msg += f\" | Val Loss: {val_loss:.4f}\"\n",
    "            logger.info(log_msg)\n",
    "\n",
    "            # Early stopping and model saving logic\n",
    "            current_metric_for_stopping = val_loss if (self.val_loader and len(self.val_loader.dataset) > 0) else train_loss\n",
    "\n",
    "            if current_metric_for_stopping < best_val_loss:\n",
    "                best_val_loss = current_metric_for_stopping\n",
    "                epochs_no_improve = 0\n",
    "                torch.save(model.state_dict(), model_save_path)\n",
    "                logger.info(f\"  New best {'validation' if self.val_loader else 'training'} loss: {best_val_loss:.4f}. Model state saved to {model_save_path}\")\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "                logger.info(f\"  {'Validation' if self.val_loader else 'Training'} loss did not improve for {epochs_no_improve} epoch(s).\")\n",
    "\n",
    "            if epochs_no_improve >= self.config.PATIENCE_EARLY_STOPPING:\n",
    "                logger.info(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
    "                break\n",
    "            \n",
    "            if self.val_loader and len(self.val_loader.dataset) > 0:\n",
    "                 scheduler.step(val_loss)\n",
    "            else:\n",
    "                 scheduler.step(train_loss)\n",
    "        \n",
    "        logger.info(f\"Loading best model state from {model_save_path} (Loss: {best_val_loss:.4f}).\")\n",
    "        try:\n",
    "            model.load_state_dict(torch.load(model_save_path, map_location=self.device))\n",
    "        except FileNotFoundError:\n",
    "            logger.warning(f\"Best model state file {model_save_path} not found. Using current model state.\")\n",
    "\n",
    "        # Final evaluation\n",
    "        test_preds_list, test_gt_list = [], []\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in tqdm(self.val_loader if self.val_loader and len(self.val_loader.dataset)>0 else self.train_loader, desc=\"Final Eval\"): # Use val or train if val empty\n",
    "                inputs = inputs.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "                outputs = model(inputs)\n",
    "                test_preds_list.append(outputs.cpu().numpy())\n",
    "                test_gt_list.append(labels.cpu().numpy())\n",
    "        \n",
    "        if not test_preds_list: # If no data was evaluated\n",
    "            logger.warning(\"No data evaluated in the final step.\")\n",
    "            self.results = {\"model\": model, \"train_losses\": train_losses, \"val_losses\": val_losses}\n",
    "            return self.results\n",
    "\n",
    "        test_predictions = np.vstack(test_preds_list)\n",
    "        test_ground_truth = np.vstack(test_gt_list)\n",
    "        final_metrics = self._compute_metrics(test_predictions, test_ground_truth)\n",
    "\n",
    "        self.results = {\n",
    "            \"model\": model, \"train_losses\": train_losses, \"val_losses\": val_losses,\n",
    "            \"final_metrics\": final_metrics,\n",
    "            \"final_predictions\": test_predictions, \"final_ground_truth\": test_ground_truth,\n",
    "        }\n",
    "        return self.results\n",
    "\n",
    "    def plot_training_history(self):\n",
    "        if not self.results or not self.results.get(\"train_losses\"): return\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(self.results[\"train_losses\"], label=\"Training Loss\")\n",
    "        if self.results.get(\"val_losses\") and self.results[\"val_losses\"]:\n",
    "            plt.plot(self.results[\"val_losses\"], label=\"Validation Loss\")\n",
    "        plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss (MSE)\")\n",
    "        plt.yscale(\"log\")\n",
    "        plt.title(f\"Training and Validation Loss for {self.model_name}\")\n",
    "        plt.legend(); plt.grid(True)\n",
    "        plt.savefig(f\"{self.model_name}_loss_curve.png\")\n",
    "        plt.show(); plt.close()\n",
    "\n",
    "    def display_final_metrics(self):\n",
    "        if not self.results or not self.results.get(\"final_metrics\"): return\n",
    "        metrics = self.results[\"final_metrics\"]\n",
    "        logger.info(f\"\\n--- Final Evaluation Metrics for {self.model_name} (on validation/train set) ---\")\n",
    "        logger.info(f\"  MAE: {metrics['mae']:.2f}\")\n",
    "        logger.info(f\"  RMSE: {metrics['rmse']:.2f}\")\n",
    "        logger.info(f\"  R²: {metrics['r2']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## 7. Main Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_on_test_set(model: nn.Module, \n",
    "                               test_loader: DataLoader, \n",
    "                               device: torch.device,\n",
    "                               criterion: Optional[nn.Module] = None) -> Dict[str, Any]:\n",
    "    \"\"\"Evaluates the model on the test set and returns predictions, ground truth, and metrics.\"\"\"\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_ground_truth = []\n",
    "    total_loss = 0.0\n",
    "    num_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(test_loader, desc=\"Evaluating on Test Set\"):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            if criterion:\n",
    "                loss = criterion(outputs, labels)\n",
    "                total_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "            all_predictions.append(outputs.cpu().numpy())\n",
    "            all_ground_truth.append(labels.cpu().numpy())\n",
    "            num_samples += inputs.size(0)\n",
    "\n",
    "    predictions_np = np.vstack(all_predictions).flatten()\n",
    "    ground_truth_np = np.vstack(all_ground_truth).flatten()\n",
    "\n",
    "    metrics = {\n",
    "        'mae': mean_absolute_error(ground_truth_np, predictions_np),\n",
    "        'rmse': np.sqrt(mean_squared_error(ground_truth_np, predictions_np)),\n",
    "        'r2': r2_score(ground_truth_np, predictions_np)\n",
    "    }\n",
    "    if criterion and num_samples > 0:\n",
    "        metrics['loss'] = total_loss / num_samples\n",
    "    \n",
    "    return {\n",
    "        \"predictions\": predictions_np,\n",
    "        \"ground_truth\": ground_truth_np,\n",
    "        \"metrics\": metrics\n",
    "    }\n",
    "\n",
    "def visualize_test_predictions(model: nn.Module,\n",
    "                               test_dataset: Dataset, # Pass the dataset directly to get specific items\n",
    "                               device: torch.device,\n",
    "                               num_samples: int = 5,\n",
    "                               target_column_name: str = \"Calories\",\n",
    "                               config_params: Config = None): # Pass config for CNN_INPUT_IMG_SIZE\n",
    "    \"\"\"Visualizes predictions for a few samples from the test set in a single figure.\"\"\"\n",
    "    if len(test_dataset) == 0:\n",
    "        logger.info(\"Test dataset is empty. Skipping visualization.\")\n",
    "        return\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    # Define denormalization transform (assuming ImageNet stats were used)\n",
    "    denormalize_transform = transforms.Compose([\n",
    "        transforms.Normalize(mean=[0., 0., 0.], std=[1/0.229, 1/0.224, 1/0.225]),\n",
    "        transforms.Normalize(mean=[-0.485, -0.456, -0.406], std=[1., 1., 1.]),\n",
    "        transforms.ToPILImage() # Convert tensor to PIL Image\n",
    "    ])\n",
    "\n",
    "    # Select random samples\n",
    "    indices = np.random.choice(len(test_dataset), min(num_samples, len(test_dataset)), replace=False)\n",
    "    \n",
    "    # Create a grid of subplots\n",
    "    rows = int(np.ceil(num_samples / 3))\n",
    "    cols = min(3, num_samples)\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(15, 5 * rows))\n",
    "    \n",
    "    # Handle case where we only have one sample\n",
    "    if num_samples == 1:\n",
    "        axes = np.array([axes])\n",
    "    \n",
    "    # Flatten axes for easy indexing if we have multiple rows\n",
    "    axes = axes.flatten() if num_samples > 1 else [axes]\n",
    "\n",
    "    for i, idx in enumerate(indices):\n",
    "        image_tensor, label_tensor = test_dataset[idx] # Get a single sample\n",
    "        image_tensor_unsqueezed = image_tensor.unsqueeze(0).to(device) # Add batch dim and move to device\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            prediction = model(image_tensor_unsqueezed).cpu().item() # Get single prediction value\n",
    "\n",
    "        true_value = label_tensor.item()\n",
    "        \n",
    "        # Denormalize the image tensor for display\n",
    "        img_for_plot_pil = denormalize_transform(image_tensor.cpu())\n",
    "\n",
    "        # Plot in the corresponding subplot\n",
    "        axes[i].imshow(img_for_plot_pil)\n",
    "        title_text = (f\"Sample {i+1}\\n\"\n",
    "                      f\"True {target_column_name}: {true_value:.2f}\\n\"\n",
    "                      f\"Predicted {target_column_name}: {prediction:.2f}\\n\"\n",
    "                      f\"Abs Error: {abs(true_value - prediction):.2f}\")\n",
    "        axes[i].set_title(title_text)\n",
    "        axes[i].axis(\"off\")\n",
    "    \n",
    "    # Hide any unused subplots\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "        axes[j].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"test_predictions_{target_column_name.lower()}.png\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def plot_predictions_vs_truth_scatter(predictions: np.ndarray, \n",
    "                                      ground_truth: np.ndarray, \n",
    "                                      target_column_name: str = \"Calories\"):\n",
    "    \"\"\"Displays a scatter plot of predicted values vs. true values.\"\"\"\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.scatter(ground_truth, predictions, alpha=0.5, label=\"Predictions\")\n",
    "    \n",
    "    # Plot a diagonal line (perfect prediction)\n",
    "    min_val = min(ground_truth.min(), predictions.min())\n",
    "    max_val = max(ground_truth.max(), predictions.max())\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label=\"Perfect Prediction\")\n",
    "    \n",
    "    plt.xlabel(f\"True {target_column_name}\")\n",
    "    plt.ylabel(f\"Predicted {target_column_name}\")\n",
    "    plt.title(f\"True vs. Predicted {target_column_name} (Test Set)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_pipeline():\n",
    "    global config # Use the global config instance\n",
    "    global U2NET # Make sure U2NET class is accessible if loaded dynamically\n",
    "\n",
    "    torch.manual_seed(config.RANDOM_STATE)\n",
    "    np.random.seed(config.RANDOM_STATE)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(config.RANDOM_STATE)\n",
    "\n",
    "    # --- Load Segmentation Model (U-2-Net) ---\n",
    "    u2net_segmentation_model = None\n",
    "    # ... (your existing U-2-Net loading logic) ...\n",
    "    if config.PRECOMPUTE_SEGMENTATION:\n",
    "        if not os.path.exists(config.U2NET_WEIGHTS_PATH):\n",
    "            logger.error(f\"U-2-Net weights not found at {config.U2NET_WEIGHTS_PATH}. Segmentation will be disabled.\")\n",
    "            config.PRECOMPUTE_SEGMENTATION = False # Disable if weights are missing\n",
    "        else:\n",
    "            import sys\n",
    "            if config.U2NET_MODEL_DIR not in sys.path:\n",
    "                 sys.path.append(config.U2NET_MODEL_DIR)\n",
    "            try:\n",
    "                if U2NET is None : # Attempt to import if not already available\n",
    "                    from u2net_model import U2NET as U2NET_class\n",
    "                    U2NET = U2NET_class # Assign to global U2NET\n",
    "\n",
    "                if U2NET is not None:\n",
    "                    u2net_segmentation_model = load_u2net_model(config.U2NET_WEIGHTS_PATH, config.DEVICE)\n",
    "                    if u2net_segmentation_model is None:\n",
    "                        config.PRECOMPUTE_SEGMENTATION = False\n",
    "                else:\n",
    "                    logger.error(\"U2NET class definition not found. Segmentation disabled.\")\n",
    "                    config.PRECOMPUTE_SEGMENTATION = False\n",
    "            except ImportError:\n",
    "                 logger.error(\"Failed to import U2NET from u2net_model.py. Segmentation disabled.\")\n",
    "                 config.PRECOMPUTE_SEGMENTATION = False\n",
    "            except Exception as e:\n",
    "                 logger.error(f\"An error occurred during U2NET setup: {e}. Segmentation disabled.\", exc_info=True)\n",
    "                 config.PRECOMPUTE_SEGMENTATION = False\n",
    "\n",
    "    # --- Load Metadata using custom parser ---\n",
    "    logger.info(f\"Parsing metadata file: {config.METADATA_FILE_CAFE1}\")\n",
    "    meta_cafe1 = parse_nutrition_csv(config.METADATA_FILE_CAFE1)\n",
    "    logger.info(f\"Parsing metadata file: {config.METADATA_FILE_CAFE2}\")\n",
    "    meta_cafe2 = parse_nutrition_csv(config.METADATA_FILE_CAFE2)\n",
    "\n",
    "    if meta_cafe1.empty and meta_cafe2.empty:\n",
    "        logger.error(\"Both metadata files are empty or failed to parse. Exiting.\")\n",
    "        return\n",
    "        \n",
    "    dish_metadata_df = pd.concat([meta_cafe1, meta_cafe2], ignore_index=True)\n",
    "    \n",
    "    if config.TARGET_COLUMN not in dish_metadata_df.columns or \"dish_id\" not in dish_metadata_df.columns:\n",
    "        logger.error(f\"Required columns ('dish_id', '{config.TARGET_COLUMN}') not found in parsed metadata. Check parse_nutrition_csv.\")\n",
    "        return\n",
    "\n",
    "    dish_metadata_df = dish_metadata_df[[\"dish_id\", config.TARGET_COLUMN]].copy()\n",
    "    dish_metadata_df.dropna(subset=[config.TARGET_COLUMN], inplace=True)\n",
    "    dish_metadata_df[config.TARGET_COLUMN] = pd.to_numeric(dish_metadata_df[config.TARGET_COLUMN], errors='coerce')\n",
    "    dish_metadata_df.dropna(subset=[config.TARGET_COLUMN], inplace=True)\n",
    "    logger.info(f\"Loaded {len(dish_metadata_df)} total metadata entries after custom parsing.\")\n",
    "\n",
    "    valid_dish_ids = []\n",
    "    for dish_id in tqdm(dish_metadata_df['dish_id'].unique(), desc=\"Checking image existence\"):\n",
    "        if os.path.exists(os.path.join(config.IMAGERY_DIR, dish_id, config.RGB_IMAGE_FILENAME)):\n",
    "            valid_dish_ids.append(dish_id)\n",
    "    dish_metadata_df = dish_metadata_df[dish_metadata_df['dish_id'].isin(valid_dish_ids)]\n",
    "    logger.info(f\"Filtered to {len(dish_metadata_df)} entries with existing images.\")\n",
    "\n",
    "    if len(dish_metadata_df) < 20: # Increased minimum for robust splitting\n",
    "        logger.error(f\"Not enough data ({len(dish_metadata_df)} samples) for splitting and training. Need at least 20. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # --- Split Data ---\n",
    "    # Splitting into train, validation, and test sets\n",
    "    train_val_df, test_df = train_test_split(dish_metadata_df, test_size=0.2, random_state=config.RANDOM_STATE)\n",
    "    train_df, val_df = train_test_split(train_val_df, test_size=0.2, random_state=config.RANDOM_STATE) # 0.2 of (0.8) = 0.16 for val\n",
    "\n",
    "    logger.info(f\"Dataset split: Train={len(train_df)}, Val={len(val_df)}, Test={len(test_df)}\")\n",
    "    \n",
    "    # --- Define CNN Transforms ---\n",
    "    normalize_transform = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    train_transforms = transforms.Compose([\n",
    "        transforms.Resize(config.CNN_INPUT_IMG_SIZE),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(20),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        normalize_transform\n",
    "    ])\n",
    "    val_test_transforms = transforms.Compose([\n",
    "        transforms.Resize(config.CNN_INPUT_IMG_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "        normalize_transform\n",
    "    ])\n",
    "\n",
    "    # before caching\n",
    "    # # --- Create Datasets ---\n",
    "    # dataset_args = {\n",
    "    #     \"imagery_dir\": config.IMAGERY_DIR,\n",
    "    #     \"processed_dir\": config.PROCESSED_SEGMENTED_DIR,\n",
    "    #     \"rgb_filename\": config.RGB_IMAGE_FILENAME,\n",
    "    #     \"cnn_input_size\": config.CNN_INPUT_IMG_SIZE,\n",
    "    #     \"segmentation_model\": u2net_segmentation_model if config.PRECOMPUTE_SEGMENTATION else None,\n",
    "    #     \"segmentation_input_size\": config.SEGMENTATION_INPUT_SIZE if config.PRECOMPUTE_SEGMENTATION else None,\n",
    "    #     \"precompute_segmentation\": config.PRECOMPUTE_SEGMENTATION,\n",
    "    #     \"overwrite_segmentation\": config.OVERWRITE_EXISTING_SEGMENTATION,\n",
    "    #     \"target_column\": config.TARGET_COLUMN,\n",
    "    #     \"device\": config.DEVICE\n",
    "    # }\n",
    "\n",
    "    # train_dataset = Nutrition5kDataset(metadata_df=train_df.reset_index(drop=True), transform=train_transforms, **dataset_args)\n",
    "    # val_dataset = Nutrition5kDataset(metadata_df=val_df.reset_index(drop=True), transform=val_test_transforms, **dataset_args)\n",
    "    # test_dataset = Nutrition5kDataset(metadata_df=test_df.reset_index(drop=True), transform=val_test_transforms, **dataset_args) # Create test_dataset\n",
    "\n",
    "    # # --- Create DataLoaders ---\n",
    "    # train_loader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, shuffle=True,\n",
    "    #                           num_workers=config.NUM_WORKERS, pin_memory=config.PIN_MEMORY, persistent_workers=(config.NUM_WORKERS > 0),\n",
    "    #                           drop_last=True)\n",
    "    # val_loader = DataLoader(val_dataset, batch_size=config.BATCH_SIZE, shuffle=False,\n",
    "    #                         num_workers=config.NUM_WORKERS, pin_memory=config.PIN_MEMORY, persistent_workers=(config.NUM_WORKERS > 0))\n",
    "    # test_loader = DataLoader(test_dataset, batch_size=config.BATCH_SIZE, shuffle=False, \n",
    "    #                          num_workers=config.NUM_WORKERS, pin_memory=config.PIN_MEMORY, persistent_workers=(config.NUM_WORKERS > 0)) # Create test_loader\n",
    "\n",
    "    # --- Create Base Datasets ---\n",
    "    dataset_args = {\n",
    "        \"imagery_dir\": config.IMAGERY_DIR,\n",
    "        \"processed_dir\": config.PROCESSED_SEGMENTED_DIR,\n",
    "        \"rgb_filename\": config.RGB_IMAGE_FILENAME,\n",
    "        \"cnn_input_size\": config.CNN_INPUT_IMG_SIZE,\n",
    "        \"segmentation_model\": u2net_segmentation_model if config.PRECOMPUTE_SEGMENTATION else None,\n",
    "        \"segmentation_input_size\": config.SEGMENTATION_INPUT_SIZE if config.PRECOMPUTE_SEGMENTATION else None,\n",
    "        \"precompute_segmentation\": config.PRECOMPUTE_SEGMENTATION,\n",
    "        \"overwrite_segmentation\": config.OVERWRITE_EXISTING_SEGMENTATION,\n",
    "        \"target_column\": config.TARGET_COLUMN,\n",
    "        \"device\": config.DEVICE # Pass main device, seg model uses this. Original dataset still produces CPU tensors first.\n",
    "    }\n",
    "\n",
    "    base_train_dataset = Nutrition5kDataset(metadata_df=train_df.reset_index(drop=True), transform=train_transforms, **dataset_args)\n",
    "    base_val_dataset = Nutrition5kDataset(metadata_df=val_df.reset_index(drop=True), transform=val_test_transforms, **dataset_args)\n",
    "    base_test_dataset = Nutrition5kDataset(metadata_df=test_df.reset_index(drop=True), transform=val_test_transforms, **dataset_args)\n",
    "\n",
    "    # --- GPU Caching Logic ---\n",
    "    train_dataset, val_dataset, test_dataset = base_train_dataset, base_val_dataset, base_test_dataset\n",
    "    can_cache_all = False\n",
    "    if config.USE_GPU_CACHING and config.DEVICE.type == 'cuda':\n",
    "        # Get sample tensor shapes for memory estimation (assumes datasets are not empty)\n",
    "        sample_img_shape, sample_label_shape = (3, *config.CNN_INPUT_IMG_SIZE), (1,) # Default if datasets are empty\n",
    "        if len(base_train_dataset) > 0:\n",
    "            sample_img, sample_label = base_train_dataset[0]\n",
    "            sample_img_shape = sample_img.shape\n",
    "            sample_label_shape = sample_label.shape\n",
    "        \n",
    "        total_samples_to_cache = len(base_train_dataset) + len(base_val_dataset) + len(base_test_dataset)\n",
    "        can_cache_all = check_gpu_memory_for_caching(total_samples_to_cache, \n",
    "                                                     sample_img_shape, sample_label_shape,\n",
    "                                                     config.DEVICE, config.GPU_CACHE_SAFETY_MARGIN)\n",
    "        if can_cache_all:\n",
    "            logger.info(\"Attempting to cache all datasets to GPU.\")\n",
    "            if len(base_train_dataset) > 0:\n",
    "                train_dataset = CachedDataset(base_train_dataset, config.DEVICE, \"TrainSet\")\n",
    "            if len(base_val_dataset) > 0:\n",
    "                val_dataset = CachedDataset(base_val_dataset, config.DEVICE, \"ValSet\")\n",
    "            else: val_dataset = None # Ensure val_dataset can be None\n",
    "            if len(base_test_dataset) > 0:\n",
    "                test_dataset = CachedDataset(base_test_dataset, config.DEVICE, \"TestSet\")\n",
    "        else:\n",
    "            logger.warning(\"Insufficient GPU VRAM to cache all datasets. Proceeding without GPU caching.\")\n",
    "            config.USE_GPU_CACHING = False # Explicitly turn off if check fails\n",
    "    \n",
    "    # Adjust DataLoader parameters if caching is active and successful\n",
    "    loader_num_workers = config.NUM_WORKERS\n",
    "    loader_pin_memory = config.PIN_MEMORY\n",
    "    if config.USE_GPU_CACHING and can_cache_all: # only if caching was successful\n",
    "        loader_num_workers = 0  # Data is already on GPU, no parallel loading from CPU needed\n",
    "        loader_pin_memory = False # Not needed when data is on GPU\n",
    "        logger.info(\"Using 0 num_workers and pin_memory=False for DataLoaders due to GPU caching.\")\n",
    "\n",
    "\n",
    "    # --- Create DataLoaders ---\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, shuffle=True,\n",
    "                              num_workers=loader_num_workers, pin_memory=loader_pin_memory, \n",
    "                              persistent_workers=(loader_num_workers > 0),\n",
    "                              drop_last=True)\n",
    "    \n",
    "    val_loader = None\n",
    "    if val_dataset and len(val_dataset) > 0: # Check if val_dataset is not None and not empty\n",
    "        val_loader = DataLoader(val_dataset, batch_size=config.BATCH_SIZE, shuffle=False,\n",
    "                                num_workers=loader_num_workers, pin_memory=loader_pin_memory, \n",
    "                                persistent_workers=(loader_num_workers > 0))\n",
    "    \n",
    "    test_loader = DataLoader(test_dataset, batch_size=config.BATCH_SIZE, shuffle=False, \n",
    "                             num_workers=loader_num_workers, pin_memory=loader_pin_memory, \n",
    "                             persistent_workers=(loader_num_workers > 0))\n",
    "\n",
    "\n",
    "    # --- Initialize Model and Trainer ---\n",
    "    calorie_cnn_model = CaloriePredictorCNN(num_outputs=1, pretrained=True) # This is the model variable we need\n",
    "    trainer = ModelTrainer(train_loader, val_loader, config_params=config, model_name=calorie_cnn_model.name)\n",
    "    \n",
    "    # --- Train and Evaluate (on validation set during training) ---\n",
    "    training_results = trainer.train_and_evaluate_model(calorie_cnn_model) # calorie_cnn_model is updated in-place\n",
    "    trainer.plot_training_history()\n",
    "    trainer.display_final_metrics() # Displays metrics on validation set (or train if val empty)\n",
    "\n",
    "    logger.info(\"Training pipeline finished.\")\n",
    "    \n",
    "    # --- Final Evaluation and Visualization on the TEST SET ---\n",
    "    logger.info(\"\\n--- Evaluating on dedicated Test Set ---\")\n",
    "    if test_loader and len(test_dataset) > 0:\n",
    "        # The model (calorie_cnn_model) has been trained and best weights loaded by ModelTrainer\n",
    "        test_eval_results = evaluate_model_on_test_set(calorie_cnn_model, test_loader, config.DEVICE, criterion=nn.MSELoss())\n",
    "        \n",
    "        logger.info(\"Test Set Metrics:\")\n",
    "        for metric_name, value in test_eval_results[\"metrics\"].items():\n",
    "            logger.info(f\"  {metric_name.upper()}: {value:.3f}\")\n",
    "\n",
    "        logger.info(\"\\n--- Visualizing some Test Set Predictions ---\")\n",
    "        visualize_test_predictions(calorie_cnn_model, \n",
    "                                   test_dataset, # Pass the dataset for easy sample access\n",
    "                                   config.DEVICE, \n",
    "                                   num_samples=5, \n",
    "                                   target_column_name=config.TARGET_COLUMN.capitalize(),\n",
    "                                   config_params=config)\n",
    "\n",
    "        logger.info(\"\\n--- Plotting Test Set Predictions vs. Truth ---\")\n",
    "        plot_predictions_vs_truth_scatter(test_eval_results[\"predictions\"],\n",
    "                                          test_eval_results[\"ground_truth\"],\n",
    "                                          target_column_name=config.TARGET_COLUMN.capitalize())\n",
    "    else:\n",
    "        logger.info(\"Test set is empty or test_loader not created. Skipping final test set evaluation and visualization.\")\n",
    "\n",
    "    # --- Visualize samples with highest prediction errors ---\n",
    "    if test_loader and len(test_dataset) > 0:\n",
    "        logger.info(\"\\n--- Visualizing samples with highest prediction errors ---\")\n",
    "        \n",
    "        # Get all test predictions with their dish IDs\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        all_indices = []\n",
    "        \n",
    "        model = calorie_cnn_model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i in range(len(test_dataset)):\n",
    "                img, label = test_dataset[i]\n",
    "                img_tensor = img.unsqueeze(0).to(config.DEVICE)\n",
    "                pred = model(img_tensor).cpu().item()\n",
    "                all_preds.append(pred)\n",
    "                all_labels.append(label.item())\n",
    "                all_indices.append(i)\n",
    "        \n",
    "        # Calculate errors and get dish_ids\n",
    "        dish_ids = test_df['dish_id'].values\n",
    "        errors = np.abs(np.array(all_preds) - np.array(all_labels))\n",
    "        error_data = list(zip(all_indices, errors, all_preds, all_labels, dish_ids))\n",
    "        \n",
    "        # Sort by error (descending) and get top 5\n",
    "        error_data.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_error_samples = error_data[:5]\n",
    "        \n",
    "        # Create visualization\n",
    "        fig, axes = plt.subplots(1, 5, figsize=(20, 6))\n",
    "        \n",
    "        denormalize = transforms.Compose([\n",
    "            transforms.Normalize(mean=[0., 0., 0.], std=[1/0.229, 1/0.224, 1/0.225]),\n",
    "            transforms.Normalize(mean=[-0.485, -0.456, -0.406], std=[1., 1., 1.]),\n",
    "            transforms.ToPILImage()\n",
    "        ])\n",
    "        \n",
    "        for i, (idx, error, pred, true, dish_id) in enumerate(top_error_samples):\n",
    "            img, _ = test_dataset[idx]\n",
    "            img_pil = denormalize(img.cpu())\n",
    "            \n",
    "            axes[i].imshow(img_pil)\n",
    "            axes[i].set_title(f\"Dish ID: {dish_id}\\nTrue: {true:.1f}\\nPred: {pred:.1f}\\nError: {error:.1f}\")\n",
    "            axes[i].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"highest_error_samples.png\")\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    # done outlier viz\n",
    "\n",
    "    logger.info(\"--- End of Pipeline ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main_pipeline()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
