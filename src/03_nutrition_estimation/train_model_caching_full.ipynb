{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import pickle\n",
    "import concurrent.futures\n",
    "import random\n",
    "import gc # Add import gc at the top of your file\n",
    "\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Argument Parsing\n",
    "# ----------------------------------------------------------------------------\n",
    "def parse_arguments():\n",
    "    parser = argparse.ArgumentParser(description=\"Train nutrition estimation models.\")\n",
    "    parser.add_argument('--model_name', type=str, required=True,\n",
    "                        choices=['SimpleConvNet', 'DeepConvNet', 'MobileNetLike', 'ResNetFromScratch', 'ResNetPretrained'],\n",
    "                        help='Name of the model to train.')\n",
    "    parser.add_argument('--base_dir', type=str, default=\"/Data/nutrition5k\", help='Base directory for the dataset.')\n",
    "    parser.add_argument('--output_dir', type=str, default=\".\", help='Directory to save models, history, and plots.')\n",
    "    parser.add_argument('--epochs', type=int, default=100, help='Number of training epochs.')\n",
    "    parser.add_argument('--lr', type=float, default=1e-3, help='Learning rate.')\n",
    "    parser.add_argument('--batch_size', type=int, default=32, help='Batch size for training and validation.')\n",
    "    parser.add_argument('--num_workers', type=int, default=0, help='Number of workers for DataLoader. Forced to 0 if GPU caching is active.')\n",
    "    parser.add_argument('--save_plots', action='store_true', help='Save plots to files instead of displaying them.')\n",
    "    parser.add_argument('--gpu_caching', action='store_true', help='Enable caching of dataset images into GPU VRAM. Forces num_workers=0.')\n",
    "    parser.add_argument('--include_side_angles', action='store_true', help='Include side angle images.')\n",
    "    parser.add_argument('--num_side_angles_per_dish', type=int, default=20, help='Max random side angle frames per dish if included (0 for all).')\n",
    "    parser.add_argument('--epoch_data_fraction', type=float, default=1.0, help='Fraction of training AND validation data to use per epoch (0.0 to 1.0). Requires >0.')\n",
    "    parser.add_argument('--early_stopping_patience', type=int, default=10, help='Patience for early stopping (epochs). 0 to disable.')\n",
    "\n",
    "    if any('jupyter' in arg for arg in sys.argv) or 'ipykernel_launcher.py' in sys.argv[0]:\n",
    "        print(\"Running in interactive mode. Using default/test args.\")\n",
    "        default_args_list = [\n",
    "            '--model_name', 'SimpleConvNet', '--epochs', '20', # Test with more epochs for early stopping\n",
    "            '--output_dir', 'interactive_test_final_optimized', '--save_plots',\n",
    "            '--include_side_angles', '--num_side_angles_per_dish', '2',\n",
    "            '--gpu_caching', \n",
    "            '--epoch_data_fraction', '0.5', \n",
    "            '--early_stopping_patience', '5' # Test early stopping\n",
    "        ]\n",
    "        current_args_str = \" \".join(sys.argv)\n",
    "        if '--model_name' not in current_args_str: args = parser.parse_args(default_args_list)\n",
    "        else:\n",
    "            try: args = parser.parse_args()\n",
    "            except SystemExit: args = parser.parse_args(default_args_list)\n",
    "    else:\n",
    "        print(\"Running as a script. Parsing command-line arguments.\")\n",
    "        args = parser.parse_args()\n",
    "\n",
    "    if not (0.0 < args.epoch_data_fraction <= 1.0):\n",
    "        parser.error(\"--epoch_data_fraction must be > 0.0 and <= 1.0.\")\n",
    "    if args.early_stopping_patience < 0:\n",
    "        parser.error(\"--early_stopping_patience must be >= 0.\")\n",
    "    return args\n",
    "\n",
    "args = parse_arguments()\n",
    "os.makedirs(args.output_dir, exist_ok=True)\n",
    "print(f\"Running with arguments: {args}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Variables and Constants\n",
    "# ----------------------------------------------------------------------------\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "LOCAL_BASE_DIR=args.base_dir; BATCH_SIZE=args.batch_size; LEARNING_RATE=args.lr; NUM_EPOCHS=args.epochs\n",
    "MODEL_NAME_ARG=args.model_name; NUM_WORKERS_ARG=args.num_workers; OUTPUT_DIR=args.output_dir\n",
    "SAVE_PLOTS=args.save_plots; GPU_CACHING_REQUESTED_ARG=args.gpu_caching\n",
    "INCLUDE_SIDE_ANGLES_ARG=args.include_side_angles; NUM_SIDE_ANGLES_PER_DISH_ARG=args.num_side_angles_per_dish\n",
    "EPOCH_DATA_FRACTION_ARG=args.epoch_data_fraction; EARLY_STOPPING_PATIENCE_ARG=args.early_stopping_patience\n",
    "\n",
    "IMAGERY_BASE_DIR=os.path.join(LOCAL_BASE_DIR,\"imagery\"); OVERHEAD_IMAGERY_DIR=os.path.join(IMAGERY_BASE_DIR,\"realsense_overhead\")\n",
    "SIDE_ANGLES_IMAGERY_DIR=os.path.join(IMAGERY_BASE_DIR,\"side_angles\");\n",
    "SIDE_ANGLES_SUBDIR_NAME=\"extracted_frames\"\n",
    "# SIDE_ANGLES_SUBDIR_NAME=\"extracted_framesss\"\n",
    "METADATA_FILE_CAFE1=os.path.join(LOCAL_BASE_DIR,\"metadata/dish_metadata_cafe1.csv\")\n",
    "METADATA_FILE_CAFE2=os.path.join(LOCAL_BASE_DIR,\"metadata/dish_metadata_cafe2.csv\")\n",
    "\n",
    "\n",
    "# Example outlier filtering (adjust thresholds as needed)\n",
    "MAX_CAL_100G = 600  # Max reasonable calories per 100g\n",
    "MAX_FAT_100G = 70   # Max reasonable fat per 100g (e.g. oil is 100, but few foods are pure oil)\n",
    "MAX_CARBS_100G = 100 # Pure sugar/starch\n",
    "MAX_PROT_100G = 50  # Dried protein powder might be higher, but whole foods rarely\n",
    "\n",
    "assert os.path.exists(LOCAL_BASE_DIR), f\"LOCAL_BASE_DIR not found: {LOCAL_BASE_DIR}\"\n",
    "assert os.path.exists(OVERHEAD_IMAGERY_DIR), f\"OVERHEAD_IMAGERY_DIR not found: {OVERHEAD_IMAGERY_DIR}\"\n",
    "if INCLUDE_SIDE_ANGLES_ARG: assert os.path.exists(SIDE_ANGLES_IMAGERY_DIR), f\"SIDE_ANGLES_IMAGERY_DIR not found: {SIDE_ANGLES_IMAGERY_DIR}\"\n",
    "assert os.path.exists(METADATA_FILE_CAFE1), f\"METADATA_FILE_CAFE1 not found: {METADATA_FILE_CAFE1}\"\n",
    "assert os.path.exists(METADATA_FILE_CAFE2), f\"METADATA_FILE_CAFE2 not found: {METADATA_FILE_CAFE2}\"\n",
    "\n",
    "# RGB_IMAGE_FILENAME=\"rgb.png\"; TARGET_COLUMNS=['calories_per_100g','fat_per_100g','carbs_per_100g','protein_per_100g']\n",
    "RGB_IMAGE_FILENAME=\"rgb.jpg\"; TARGET_COLUMNS=['calories_per_100g','fat_per_100g','carbs_per_100g','protein_per_100g']\n",
    "print(f\"Device: {DEVICE}\"); print(f\"Selected Model: {MODEL_NAME_ARG}\")\n",
    "print(f\"GPU Caching Requested: {GPU_CACHING_REQUESTED_ARG}\"); print(f\"Epoch Data Fraction: {EPOCH_DATA_FRACTION_ARG}\")\n",
    "print(f\"Early Stopping Patience: {EARLY_STOPPING_PATIENCE_ARG if EARLY_STOPPING_PATIENCE_ARG > 0 else 'Disabled'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Definitions\n",
    "# ----------------------------------------------------------------------------\n",
    "class SimpleConvNet(nn.Module):\n",
    "    def __init__(self,num_outputs=4):\n",
    "        super().__init__()\n",
    "        self.conv1=nn.Conv2d(3,32,3,padding=1)\n",
    "        self.bn1=nn.BatchNorm2d(32)\n",
    "        self.conv2=nn.Conv2d(32,64,3,padding=1)\n",
    "        self.bn2=nn.BatchNorm2d(64)\n",
    "        self.conv3=nn.Conv2d(64,128,3,padding=1)\n",
    "        self.bn3=nn.BatchNorm2d(128)\n",
    "        self.conv4=nn.Conv2d(128,256,3,padding=1)\n",
    "        self.bn4=nn.BatchNorm2d(256)\n",
    "        self.pool=nn.MaxPool2d(2,2)\n",
    "        self.dropout=nn.Dropout(0.5)\n",
    "        self.fc1=nn.Linear(256*14*14,512)\n",
    "        self.fc2=nn.Linear(512,256)\n",
    "        self.fc3=nn.Linear(256,num_outputs)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x=self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x=self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "        x=self.pool(F.relu(self.bn4(self.conv4(x))))\n",
    "        x=x.view(x.size(0),-1)\n",
    "        x=self.dropout(F.relu(self.fc1(x)))\n",
    "        x=self.dropout(F.relu(self.fc2(x)))\n",
    "        return self.fc3(x)\n",
    "    \n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out += self.shortcut(identity) # Add skip connection\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "    \n",
    "class DeepConvNet(nn.Module):\n",
    "    def __init__(self,num_outputs=4):\n",
    "        super().__init__()\n",
    "        self.conv1=nn.Conv2d(3,64,7,stride=2,padding=3)\n",
    "        self.bn1=nn.BatchNorm2d(64)\n",
    "        self.pool1=nn.MaxPool2d(3,stride=2,padding=1)\n",
    "        self.res_block1 = BasicBlock(64, 128, stride=2)\n",
    "        self.res_block2 = BasicBlock(128, 256, stride=2)\n",
    "        self.res_block3 = BasicBlock(256, 512, stride=2)\n",
    "        self.res_block4 = BasicBlock(512, 512, stride=2)\n",
    "        # self.res_block1=self._make_residual_block(64,128,stride=2)\n",
    "        # self.res_block2=self._make_residual_block(128,256,stride=2)\n",
    "        # self.res_block3=self._make_residual_block(256,512,stride=2)\n",
    "        self.avgpool=nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.dropout = nn.Dropout(0.5) \n",
    "        self.fc=nn.Linear(512,num_outputs)\n",
    "\n",
    "    def _make_residual_block(self,in_channels,out_channels,stride=1):\n",
    "        # This creates a sequence of layers, but a true ResNet block adds input to output.\n",
    "        # If stride != 1 or in_channels != out_channels, a projection shortcut is needed.\n",
    "        layers = []\n",
    "        layers.append(nn.Conv2d(in_channels, out_channels, 3, stride=stride, padding=1, bias=False))\n",
    "        layers.append(nn.BatchNorm2d(out_channels))\n",
    "        layers.append(nn.ReLU(inplace=True))\n",
    "        layers.append(nn.Conv2d(out_channels, out_channels, 3, padding=1, bias=False))\n",
    "        layers.append(nn.BatchNorm2d(out_channels))\n",
    "        \n",
    "        # Shortcut connection for ResNet\n",
    "        shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "        # This needs to be integrated into a block class that does x = self.layers(x) + self.shortcut(x)\n",
    "        # For simplicity with your current structure, this _make_residual_block is just a sequence.\n",
    "        # To make it a true ResNet block, you'd need a custom nn.Module for the block.\n",
    "        return nn.Sequential(*layers)    \n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
    "        x=self.res_block1(x)\n",
    "        x=self.res_block2(x)\n",
    "        x=self.res_block3(x)\n",
    "        x=self.res_block4(x)\n",
    "        x=self.avgpool(x)\n",
    "        x=x.view(x.size(0),-1)\n",
    "        x=self.dropout(x)\n",
    "        return self.fc(x)\n",
    "    \n",
    "class MobileNetLike(nn.Module):\n",
    "    def __init__(self,num_outputs=4):super().__init__();ds_conv=lambda i,o,s=1:nn.Sequential(nn.Conv2d(i,i,3,s,1,groups=i,bias=False),nn.BatchNorm2d(i),nn.ReLU(True),nn.Conv2d(i,o,1,bias=False),nn.BatchNorm2d(o),nn.ReLU(True));self.conv1=nn.Conv2d(3,32,3,stride=2,padding=1,bias=False);self.bn1=nn.BatchNorm2d(32);self.dw2=ds_conv(32,64,2);self.dw3=ds_conv(64,128,2);self.dw4=ds_conv(128,256,2);self.dw5=ds_conv(256,512,2);self.avgpool=nn.AdaptiveAvgPool2d((1,1));self.fc=nn.Linear(512,num_outputs)\n",
    "    def forward(self,x):x=F.relu(self.bn1(self.conv1(x)));x=self.dw2(x);x=self.dw3(x);x=self.dw4(x);x=self.dw5(x);x=self.avgpool(x);x=x.view(x.size(0),-1);return self.fc(x)\n",
    "class ResNetFromScratch(nn.Module):\n",
    "    def __init__(self,num_outputs=4,use_pretrained=False):super().__init__();weights=models.ResNet34_Weights.IMAGENET1K_V1 if use_pretrained else None;self.backbone=models.resnet34(weights=weights);n_feat=self.backbone.fc.in_features;self.backbone.fc=nn.Sequential(nn.Linear(n_feat,256),nn.ReLU(),nn.Dropout(0.5),nn.Linear(256,num_outputs))\n",
    "    def forward(self,x):return self.backbone(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function\n",
    "# ----------------------------------------------------------------------------\n",
    "class MultiTaskLoss(nn.Module): # Kept for potential future use, L1Loss used by default\n",
    "    def __init__(self,task_weights=None):\n",
    "        super().__init__()\n",
    "        self.w=torch.ones(len(TARGET_COLUMNS))if task_weights is None else torch.tensor(task_weights,dtype=torch.float32)\n",
    "    def forward(self,p,t):L=torch.abs(p-t);W=self.w.to(p.device)if self.w.device!=p.device else self.w;return(L*W).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing (Metadata)\n",
    "# ----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing (Metadata)\n",
    "# ----------------------------------------------------------------------------\n",
    "def parse_nutrition_csv(file_path):\n",
    "    dishes=[]\n",
    "    try:\n",
    "        with open(file_path,'r')as f:\n",
    "            for line in f:\n",
    "                parts=line.strip().split(',')\n",
    "                if not parts[0].startswith('dish_'):continue\n",
    "                try:did,cal,wt,fat,car,pro=parts[0],float(parts[1]),float(parts[2]),float(parts[3]),float(parts[4]),float(parts[5])\n",
    "                except(ValueError,IndexError):continue\n",
    "                if wt <= 0: continue # Ensure weight is positive here\n",
    "                dishes.append({'dish_id':did,'calories':cal,'weight':wt,'fat':fat,'carbs':car,'protein':pro,\n",
    "                               'calories_per_100g':(cal/wt)*100,\n",
    "                               'fat_per_100g':(fat/wt)*100,\n",
    "                               'carbs_per_100g':(car/wt)*100,\n",
    "                               'protein_per_100g':(pro/wt)*100})\n",
    "    except FileNotFoundError: print(f\"Error: Metadata file {file_path} not found.\")\n",
    "    return pd.DataFrame(dishes)\n",
    "\n",
    "raw_dish_metadata_df=pd.concat([parse_nutrition_csv(METADATA_FILE_CAFE1),parse_nutrition_csv(METADATA_FILE_CAFE2)],ignore_index=True)\n",
    "\n",
    "# Initial cleaning\n",
    "all_dish_metadata_df_unfiltered = raw_dish_metadata_df.replace([np.inf,-np.inf],np.nan).dropna(\n",
    "    subset=TARGET_COLUMNS+['dish_id','weight'] # Ensure these are present before filtering\n",
    ").set_index('dish_id')\n",
    "\n",
    "print(f\"Unfiltered metadata rows: {len(all_dish_metadata_df_unfiltered)}\")\n",
    "if not all_dish_metadata_df_unfiltered.empty:\n",
    "    print(\"Unfiltered metadata stats:\\n\", all_dish_metadata_df_unfiltered[TARGET_COLUMNS].describe())\n",
    "\n",
    "# --- APPLY OUTLIER FILTERING HERE ---\n",
    "MAX_CAL_100G = 600    # Example: Max reasonable calories per 100g\n",
    "MIN_CAL_100G = 5      # Min reasonable calories per 100g\n",
    "MAX_FAT_100G = 80     # Example: Max reasonable fat per 100g (oil is 100, but most foods less)\n",
    "MIN_FAT_100G = 1\n",
    "MAX_CARBS_100G = 100  # Pure sugar/starch\n",
    "MIN_CARBS_100G = 1\n",
    "MAX_PROT_100G = 60    # Dried protein powder might be higher, but whole foods often less\n",
    "MIN_PROT_100G = 1\n",
    "\n",
    "# Ensure that 'weight' is positive as well, although parse_nutrition_csv should handle it.\n",
    "# This check is redundant if parse_nutrition_csv already ensures wt > 0.\n",
    "# all_dish_metadata_df_unfiltered = all_dish_metadata_df_unfiltered[all_dish_metadata_df_unfiltered['weight'] > 0]\n",
    "\n",
    "\n",
    "all_dish_metadata_df_filtered = all_dish_metadata_df_unfiltered[\n",
    "    (all_dish_metadata_df_unfiltered['calories_per_100g'] >= MIN_CAL_100G) & (all_dish_metadata_df_unfiltered['calories_per_100g'] <= MAX_CAL_100G) &\n",
    "    (all_dish_metadata_df_unfiltered['fat_per_100g'] >= MIN_FAT_100G) & (all_dish_metadata_df_unfiltered['fat_per_100g'] <= MAX_FAT_100G) &\n",
    "    (all_dish_metadata_df_unfiltered['carbs_per_100g'] >= MIN_CARBS_100G) & (all_dish_metadata_df_unfiltered['carbs_per_100g'] <= MAX_CARBS_100G) &\n",
    "    (all_dish_metadata_df_unfiltered['protein_per_100g'] >= MIN_PROT_100G) & (all_dish_metadata_df_unfiltered['protein_per_100g'] <= MAX_PROT_100G)\n",
    "]\n",
    "print(f\"Filtered metadata rows: {len(all_dish_metadata_df_filtered)}\")\n",
    "if not all_dish_metadata_df_filtered.empty:\n",
    "    print(\"Filtered metadata stats (this will be used for dataset construction):\\n\", all_dish_metadata_df_filtered[TARGET_COLUMNS].describe())\n",
    "else:\n",
    "    print(\"CRITICAL: All data was filtered out by outlier removal. Check filter thresholds and data.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Now, use all_dish_metadata_df_filtered to build dataset_items\n",
    "dataset_items=[]\n",
    "all_labels_list=[]\n",
    "dish_id_to_label_idx={} # This will map dish_id to an index in all_labels_list\n",
    "\n",
    "# --- REVISED LOGIC FOR ROBUST LABEL INDEXING WITH IMAGE VERIFICATION ---\n",
    "print(\"Starting revised dataset item construction with image verification...\")\n",
    "\n",
    "candidate_image_info = [] # List of (dish_id, image_path, image_type) for images that EXIST\n",
    "for dish_id, row_data in tqdm(all_dish_metadata_df_filtered.iterrows(), total=len(all_dish_metadata_df_filtered), desc=\"Phase 1: Identifying existing images\"):\n",
    "    overhead_img_path = os.path.join(OVERHEAD_IMAGERY_DIR, dish_id, RGB_IMAGE_FILENAME)\n",
    "    if os.path.exists(overhead_img_path):\n",
    "        candidate_image_info.append({'dish_id': dish_id, 'image_path': overhead_img_path, 'image_type': 'overhead'})\n",
    "    \n",
    "    if INCLUDE_SIDE_ANGLES_ARG:\n",
    "        side_angle_base = os.path.join(SIDE_ANGLES_IMAGERY_DIR, dish_id, SIDE_ANGLES_SUBDIR_NAME)\n",
    "        if os.path.isdir(side_angle_base):\n",
    "            available_frames=[os.path.join(side_angle_base,f)for f in os.listdir(side_angle_base)if f.startswith(\"camera_\")and f.endswith(\".jpg\")] # or .png\n",
    "            if available_frames:\n",
    "                n_to_select = NUM_SIDE_ANGLES_PER_DISH_ARG if NUM_SIDE_ANGLES_PER_DISH_ARG > 0 else len(available_frames)\n",
    "                selected_frames = np.random.choice(available_frames, min(len(available_frames), n_to_select), replace=False).tolist()\n",
    "                for frame_path in selected_frames:\n",
    "                    candidate_image_info.append({'dish_id': dish_id, 'image_path': frame_path, 'image_type': 'side_angle'})\n",
    "\n",
    "verified_image_info = []\n",
    "corrupted_image_paths = []\n",
    "print(f\"Phase 2: Verifying {len(candidate_image_info)} potential image items...\")\n",
    "for item_info in tqdm(candidate_image_info, desc=\"Verifying image readability\"):\n",
    "    try:\n",
    "        with Image.open(item_info['image_path']) as img:\n",
    "            img.verify() \n",
    "        verified_image_info.append(item_info)\n",
    "    except Exception as e: # Catching a broader range of PIL errors\n",
    "        print(f\"WARNING: Skipping corrupted/unreadable image: {item_info['image_path']} ({type(e).__name__}: {e})\")\n",
    "        corrupted_image_paths.append(item_info['image_path'])\n",
    "\n",
    "if corrupted_image_paths:\n",
    "    print(f\"INFO: Skipped {len(corrupted_image_paths)} corrupted/unreadable images during dataset construction.\")\n",
    "\n",
    "if not verified_image_info:\n",
    "    print(\"CRITICAL: No valid and readable images found. Exiting.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Now build labels ONLY for dishes that have at least one verified image\n",
    "final_dish_ids_with_verified_images = sorted(list(set(item['dish_id'] for item in verified_image_info)))\n",
    "\n",
    "all_labels_list = []\n",
    "dish_id_to_label_idx = {}\n",
    "for dish_id in final_dish_ids_with_verified_images:\n",
    "    # It's guaranteed that dish_id is in all_dish_metadata_df_filtered if it's in verified_image_info\n",
    "    row_data = all_dish_metadata_df_filtered.loc[dish_id] \n",
    "    dish_id_to_label_idx[dish_id] = len(all_labels_list)\n",
    "    all_labels_list.append(row_data[TARGET_COLUMNS].values.astype(np.float32))\n",
    "    \n",
    "all_labels_array = np.array(all_labels_list, dtype=np.float32) if all_labels_list else np.array([])\n",
    "\n",
    "dataset_items = []\n",
    "for item_info in verified_image_info:\n",
    "    dish_id = item_info['dish_id']\n",
    "    if dish_id in dish_id_to_label_idx: # Ensure the dish made it into the final label mapping\n",
    "        label_idx = dish_id_to_label_idx[dish_id]\n",
    "        dataset_items.append({\n",
    "            'dish_id': dish_id, \n",
    "            'label_idx': label_idx, \n",
    "            'image_type': item_info['image_type'], \n",
    "            'image_path': item_info['image_path']\n",
    "        })\n",
    "print(f\"Total dataset items (views) after verification and label mapping: {len(dataset_items)}\")\n",
    "\n",
    "\n",
    "all_labels_array = np.array(all_labels_list, dtype=np.float32) if all_labels_list else np.array([])\n",
    "\n",
    "# This `filtered_metadata_for_eval` is now mainly for reference if you need the original weights later.\n",
    "# The actual labels come from `all_labels_array` which is built from filtered data.\n",
    "final_dish_ids_in_dataset = sorted(list(set(item['dish_id'] for item in dataset_items))) # These are dish_ids from filtered data\n",
    "# For evaluation, we might still want original weights, so get them from the initial unfiltered or partially cleaned df\n",
    "# but ensure we only look at dishes that actually made it into our final dataset.\n",
    "_temp_metadata_for_weights = all_dish_metadata_df_unfiltered.loc[all_dish_metadata_df_unfiltered.index.isin(final_dish_ids_in_dataset)].copy()\n",
    "if _temp_metadata_for_weights.index.name == 'dish_id':\n",
    "    _temp_metadata_for_weights.reset_index(inplace=True)\n",
    "# This is used later in final evaluation to get 'weight' for absolute metrics.\n",
    "# Ensure it's named clearly if you keep it, or integrate weight into dataset_items if preferred.\n",
    "# For now, let's rename the variable used later in the eval section to avoid confusion.\n",
    "# metadata_for_final_eval_weights = _temp_metadata_for_weights \n",
    "\n",
    "\n",
    "if not all_dish_metadata_df_filtered.empty: # This was the one used for creating dataset_items\n",
    "    print(\"\\nPer-100g stats for dishes IN THE FINAL DATASET (after filtering):\\n\", all_dish_metadata_df_filtered.loc[final_dish_ids_in_dataset][TARGET_COLUMNS].describe())\n",
    "\n",
    "if not all_dish_metadata_df_unfiltered.empty and all_dish_metadata_df_unfiltered.index.name == 'dish_id':\n",
    "    # Select only the dishes that are in our final dataset_items\n",
    "    metadata_for_final_eval_weights = all_dish_metadata_df_unfiltered.loc[\n",
    "        all_dish_metadata_df_unfiltered.index.isin(final_dish_ids_in_dataset)\n",
    "    ].copy()\n",
    "    # Ensure 'dish_id' is a column if it was the index, for easier use later if needed,\n",
    "    # though pd.Series can work directly with the index.\n",
    "    # metadata_for_final_eval_weights.reset_index(inplace=True) # Optional: reset if you prefer dish_id as column\n",
    "else:\n",
    "    print(\"Warning: Could not properly create metadata_for_final_eval_weights. Weight data might be affected.\")\n",
    "    # Create an empty DataFrame or handle appropriately\n",
    "    metadata_for_final_eval_weights = pd.DataFrame(columns=['dish_id', 'weight']) \n",
    "    if 'dish_id' in all_dish_metadata_df_unfiltered.columns: # if dish_id was already a column\n",
    "         metadata_for_final_eval_weights = all_dish_metadata_df_unfiltered.loc[\n",
    "            all_dish_metadata_df_unfiltered['dish_id'].isin(final_dish_ids_in_dataset)\n",
    "        ].copy()\n",
    "\n",
    "# raw_dish_metadata_df=pd.concat([parse_nutrition_csv(METADATA_FILE_CAFE1),parse_nutrition_csv(METADATA_FILE_CAFE2)],ignore_index=True)\n",
    "# all_dish_metadata_df=raw_dish_metadata_df.replace([np.inf,-np.inf],np.nan).dropna(subset=TARGET_COLUMNS+['dish_id','weight']).set_index('dish_id')\n",
    "# print(\"Original metadata stats:\\n\", all_dish_metadata_df[TARGET_COLUMNS].describe())\n",
    "# all_dish_metadata_df = all_dish_metadata_df[\n",
    "#     (all_dish_metadata_df['calories_per_100g'] >= 0) & (all_dish_metadata_df['calories_per_100g'] <= MAX_CAL_100G) &\n",
    "#     (all_dish_metadata_df['fat_per_100g'] >= 0) & (all_dish_metadata_df['fat_per_100g'] <= MAX_FAT_100G) &\n",
    "#     (all_dish_metadata_df['carbs_per_100g'] >= 0) & (all_dish_metadata_df['carbs_per_100g'] <= MAX_CARBS_100G) &\n",
    "#     (all_dish_metadata_df['protein_per_100g'] >= 0) & (all_dish_metadata_df['protein_per_100g'] <= MAX_PROT_100G)\n",
    "# ]\n",
    "# # Crucially, also ensure weight is positive if not already handled by parse_nutrition_csv\n",
    "# all_dish_metadata_df = all_dish_metadata_df[all_dish_metadata_df['weight'] > 0]\n",
    "\n",
    "# print(\"Filtered metadata stats:\\n\", all_dish_metadata_df[TARGET_COLUMNS].describe())\n",
    "# # Now proceed with creating dataset_items from this filtered DataFrame\n",
    "# dataset_items=[];all_labels_list=[];dish_id_to_label_idx={}\n",
    "# print(\"Scanning for available images and preparing dataset items...\")\n",
    "# for dish_id,row_data in tqdm(all_dish_metadata_df.iterrows(),total=len(all_dish_metadata_df),desc=\"Processing dishes\"):\n",
    "#     if dish_id not in dish_id_to_label_idx:dish_id_to_label_idx[dish_id]=len(all_labels_list);all_labels_list.append(row_data[TARGET_COLUMNS].values.astype(np.float32))\n",
    "#     label_idx=dish_id_to_label_idx[dish_id]\n",
    "#     overhead_img_path=os.path.join(OVERHEAD_IMAGERY_DIR,dish_id,RGB_IMAGE_FILENAME)\n",
    "#     if os.path.exists(overhead_img_path):dataset_items.append({'dish_id':dish_id,'label_idx':label_idx,'image_type':'overhead','image_path':overhead_img_path})\n",
    "#     if INCLUDE_SIDE_ANGLES_ARG:\n",
    "#         side_angle_base=os.path.join(SIDE_ANGLES_IMAGERY_DIR,dish_id,SIDE_ANGLES_SUBDIR_NAME)\n",
    "#         if os.path.isdir(side_angle_base):\n",
    "#             # available_frames=[os.path.join(side_angle_base,f)for f in os.listdir(side_angle_base)if f.startswith(\"camera_\")and f.endswith(\".png\")]\n",
    "#             available_frames=[os.path.join(side_angle_base,f)for f in os.listdir(side_angle_base)if f.startswith(\"camera_\")and f.endswith(\".jpg\")]\n",
    "#             if available_frames:\n",
    "#                 n_to_select=NUM_SIDE_ANGLES_PER_DISH_ARG if NUM_SIDE_ANGLES_PER_DISH_ARG>0 else len(available_frames)\n",
    "#                 selected_frames=np.random.choice(available_frames,min(len(available_frames),n_to_select),replace=False).tolist()\n",
    "#                 for frame_path in selected_frames:dataset_items.append({'dish_id':dish_id,'label_idx':label_idx,'image_type':'side_angle','image_path':frame_path})\n",
    "# if not dataset_items:print(f\"CRITICAL: No dataset items found after scanning imagery directories like {OVERHEAD_IMAGERY_DIR}. Check paths and data structure.\");sys.exit(1)\n",
    "# print(f\"Total dataset items (views): {len(dataset_items)}\")\n",
    "# all_labels_array=np.array(all_labels_list,dtype=np.float32)if all_labels_list else np.array([])\n",
    "# final_dish_ids_in_dataset=sorted(list(set(item['dish_id']for item in dataset_items)))\n",
    "# filtered_metadata_for_eval=all_dish_metadata_df.loc[all_dish_metadata_df.index.isin(final_dish_ids_in_dataset)].copy()\n",
    "# if filtered_metadata_for_eval.index.name=='dish_id':filtered_metadata_for_eval.reset_index(inplace=True)\n",
    "# if not filtered_metadata_for_eval.empty:print(\"\\nPer-100g stats for final dataset dishes:\\n\",filtered_metadata_for_eval[TARGET_COLUMNS].describe())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset and DataLoader with Enhanced GPU Caching\n",
    "# ----------------------------------------------------------------------------\n",
    "def check_memory_for_caching(num_unique_images, image_size_wh=224, device=None, return_estimate=False, subtract_from_free_mb=0.0, context=\"\"):\n",
    "    if device is None or device.type!='cuda' or not torch.cuda.is_available():return(False,0.0)if return_estimate else False\n",
    "    try:\n",
    "        img_mem_mb=image_size_wh*image_size_wh*3*4/(1024*1024);total_est_mb_this_set=num_unique_images*img_mem_mb\n",
    "        props=torch.cuda.get_device_properties(device);total_gpu_mem_mb=props.total_memory/(1024*1024)\n",
    "        free_mem_bytes,_=torch.cuda.mem_get_info(device);current_free_gpu_mem_mb=free_mem_bytes/(1024*1024)\n",
    "        effective_free_gpu_mem_mb=current_free_gpu_mem_mb-subtract_from_free_mb\n",
    "        if effective_free_gpu_mem_mb<0:effective_free_gpu_mem_mb=0\n",
    "        # This factor (0.5) is for deciding if a NEW SET of images can be cached in currently free VRAM.\n",
    "        # It's different from the overall cache utilization cap.\n",
    "        usable_cache_mb_this_set=effective_free_gpu_mem_mb*0.8\n",
    "        if context: # Print only if context is provided\n",
    "            print(f\"MemCheck({context}): EstCacheThisSet={total_est_mb_this_set:.1f}MB({num_unique_images} imgs).\")\n",
    "            print(f\"  CurrentFreeGPU={current_free_gpu_mem_mb:.1f}MB,SubtractedUsed={subtract_from_free_mb:.1f}MB,EffectiveFree={effective_free_gpu_mem_mb:.1f}MB.\")\n",
    "            print(f\"  UsableEstForThisSet={usable_cache_mb_this_set:.1f}MB,TotalGPU={total_gpu_mem_mb:.1f}MB.\")\n",
    "        can_cache_this_set=False\n",
    "        if total_est_mb_this_set==0:can_cache_this_set=True\n",
    "        elif total_est_mb_this_set > total_gpu_mem_mb:\n",
    "            if context: print(f\"  ✗ EstCacheThisSet > TotalGPU. Cannot cache.\")\n",
    "        elif usable_cache_mb_this_set >= total_est_mb_this_set:\n",
    "            if context: print(\"  ✓ Sufficient VRAM estimate for this set.\");can_cache_this_set=True\n",
    "        else:\n",
    "            if context: print(f\"  ✗ Insufficient VRAM estimate for this set (needs {total_est_mb_this_set:.1f}MB, usable {usable_cache_mb_this_set:.1f}MB).\")\n",
    "        return(can_cache_this_set,total_est_mb_this_set)if return_estimate else can_cache_this_set\n",
    "    except Exception as e:\n",
    "        if context: print(f\"Err MemCheck({context}):{e}\")\n",
    "        return(False,0.0)if return_estimate else False\n",
    "\n",
    "class NutritionDataset(Dataset):\n",
    "    def __init__(self, items_list, all_labels_array_source, transform=None,\n",
    "                 user_requests_gpu_caching=False, attempt_initial_full_cache_for_this_set=False,\n",
    "                 max_cache_utilization_factor=0.3): # Cap cache at 60% of total GPU VRAM by default\n",
    "        self.items_list = items_list\n",
    "        self.all_labels_array_source = all_labels_array_source\n",
    "        self.transform = transform\n",
    "        self.image_cache = {} # Stores path: tensor_on_gpu\n",
    "        self.device_to_cache_to = DEVICE # Assuming DEVICE is globally defined\n",
    "        self.caching_is_active_for_retrieval = False\n",
    "        \n",
    "        self.total_gpu_memory_mb = 0\n",
    "        self.max_cache_memory_mb = 0 # Max memory the cache should occupy\n",
    "        self.max_cache_utilization_factor = max_cache_utilization_factor\n",
    "        self.current_epoch_image_paths = set() # Image paths for the current epoch's subset\n",
    "\n",
    "        # For multithreaded loading for _cache_image_paths_to_gpu\n",
    "        self.num_cache_workers = min(8, os.cpu_count() // 2 if os.cpu_count() else 1, len(items_list) // 100 if len(items_list) > 100 else 1)\n",
    "        # self.num_cache_workers = 16\n",
    "        self.num_cache_workers = max(1, self.num_cache_workers)\n",
    "\n",
    "        if user_requests_gpu_caching and self.device_to_cache_to.type == 'cuda':\n",
    "            self.caching_is_active_for_retrieval = True\n",
    "            try:\n",
    "                props = torch.cuda.get_device_properties(self.device_to_cache_to)\n",
    "                self.total_gpu_memory_mb = props.total_memory / (1024 * 1024)\n",
    "                self.max_cache_memory_mb = self.total_gpu_memory_mb * self.max_cache_utilization_factor\n",
    "                print(f\"Dataset '{self.__class__.__name__}': Max GPU cache size target: {self.max_cache_memory_mb:.1f}MB ({self.max_cache_utilization_factor*100:.0f}% of total {self.total_gpu_memory_mb:.1f}MB VRAM).\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not get GPU properties to set max cache size: {e}. Disabling GPU caching for safety.\")\n",
    "                self.caching_is_active_for_retrieval = False\n",
    "\n",
    "            if self.caching_is_active_for_retrieval and attempt_initial_full_cache_for_this_set:\n",
    "                unique_paths = list(set(item['image_path'] for item in self.items_list))\n",
    "                print(f\"Attempting initial full GPU cache for {len(unique_paths)} unique images in this dataset instance ({self.__class__.__name__})...\")\n",
    "                \n",
    "                # For initial load, all unique paths are considered \"current\" for purpose of not evicting them immediately if they fit.\n",
    "                initial_load_paths_set = set(unique_paths)\n",
    "                self._cache_image_paths_to_gpu(unique_paths, \"Initial caching\")\n",
    "                # Manage capacity after the initial load attempt\n",
    "                self._manage_cache_capacity(current_epoch_image_paths_set=initial_load_paths_set)\n",
    "        \n",
    "        elif user_requests_gpu_caching:\n",
    "            print(\"GPU caching requested, but device not CUDA. Caching disabled.\")\n",
    "\n",
    "    def _calculate_cache_memory_usage_mb(self):\n",
    "        if not self.image_cache:\n",
    "            return 0.0\n",
    "        current_cache_size_bytes = 0\n",
    "        for tensor in self.image_cache.values():\n",
    "            current_cache_size_bytes += tensor.nelement() * tensor.element_size()\n",
    "        return current_cache_size_bytes / (1024 * 1024)\n",
    "\n",
    "    def _manage_cache_capacity(self, current_epoch_image_paths_set):\n",
    "        if not self.caching_is_active_for_retrieval or self.max_cache_memory_mb <= 0:\n",
    "            return\n",
    "\n",
    "        current_cache_usage_mb = self._calculate_cache_memory_usage_mb()\n",
    "\n",
    "        if current_cache_usage_mb > self.max_cache_memory_mb:\n",
    "            num_items_before_eviction = len(self.image_cache)\n",
    "            print(f\"Cache Info ({self.__class__.__name__}): Usage {current_cache_usage_mb:.1f}MB (Items: {num_items_before_eviction}) > Target {self.max_cache_memory_mb:.1f}MB. Evicting non-epoch images...\")\n",
    "            \n",
    "            evictable_paths = [\n",
    "                p for p in self.image_cache.keys() if p not in current_epoch_image_paths_set\n",
    "            ]\n",
    "            np.random.shuffle(evictable_paths) # Evict randomly among non-epoch items\n",
    "            \n",
    "            num_evicted = 0\n",
    "            evicted_mem_mb = 0.0\n",
    "\n",
    "            for path_to_evict in evictable_paths:\n",
    "                if current_cache_usage_mb <= self.max_cache_memory_mb:\n",
    "                    break # Target met\n",
    "\n",
    "                if path_to_evict in self.image_cache:\n",
    "                    tensor_to_evict = self.image_cache.pop(path_to_evict) # Remove and get tensor\n",
    "                    tensor_size_bytes = tensor_to_evict.nelement() * tensor_to_evict.element_size()\n",
    "                    del tensor_to_evict # Explicitly delete reference\n",
    "                    \n",
    "                    current_cache_usage_mb -= tensor_size_bytes / (1024 * 1024)\n",
    "                    evicted_mem_mb += tensor_size_bytes / (1024 * 1024)\n",
    "                    num_evicted += 1\n",
    "                \n",
    "            if num_evicted > 0:\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache() # Try to free memory after deletions\n",
    "                print(f\"Cache Eviction ({self.__class__.__name__}): Evicted {num_evicted} non-epoch images (freed ~{evicted_mem_mb:.1f}MB). New cache: {current_cache_usage_mb:.1f}MB (Items: {len(self.image_cache)}).\")\n",
    "            \n",
    "            if current_cache_usage_mb > self.max_cache_memory_mb:\n",
    "                 # This implies all remaining images are for the current epoch, or no evictable images were found.\n",
    "                print(f\"Warning ({self.__class__.__name__}): Cache usage {current_cache_usage_mb:.1f}MB still > target {self.max_cache_memory_mb:.1f}MB. \"\n",
    "                      f\"{len(self.image_cache)} items remain, potentially all for current epoch ({len(current_epoch_image_paths_set)} paths). \"\n",
    "                      f\"Consider reducing epoch_data_fraction, batch_size, or max_cache_utilization_factor if OOMs persist.\")\n",
    "        # else:\n",
    "            # print(f\"Cache Info ({self.__class__.__name__}): Current usage {current_cache_usage_mb:.1f}MB (Items: {len(self.image_cache)}) is within target {self.max_cache_memory_mb:.1f}MB.\")\n",
    "\n",
    "\n",
    "    def _load_and_transform_image_cpu(self, image_path):\n",
    "        # (This method remains the same as in your original code)\n",
    "        try:\n",
    "            img_pil = Image.open(image_path).convert(\"RGB\")\n",
    "            img_tensor_cpu = self.transform(img_pil) if self.transform else transforms.ToTensor()(img_pil)\n",
    "            return image_path, img_tensor_cpu\n",
    "        except FileNotFoundError:\n",
    "            return image_path, None\n",
    "        except Exception: # as e: print(f\"W:Err CPU pre-load {image_path}:{e}.\")\n",
    "            return image_path, None\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.items_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.items_list[idx]\n",
    "        image_path = item['image_path']\n",
    "        label_idx = item['label_idx']\n",
    "        img_tensor = None\n",
    "        \n",
    "        if self.caching_is_active_for_retrieval and image_path in self.image_cache:\n",
    "            img_tensor_gpu = self.image_cache[image_path] # This tensor is on self.device_to_cache_to (e.g., CUDA)\n",
    "            \n",
    "            # Defensive check: ensure it's on the intended cache device first\n",
    "            if img_tensor_gpu.device != self.device_to_cache_to:\n",
    "                img_tensor_gpu = img_tensor_gpu.to(self.device_to_cache_to)\n",
    "            \n",
    "            # Move to CPU before returning, so collate_fn gets consistent CPU tensors\n",
    "            img_tensor = img_tensor_gpu.cpu() \n",
    "        else:\n",
    "            # Fallback: Load from disk if not cached or caching disabled\n",
    "            try:\n",
    "                img_pil = Image.open(image_path).convert(\"RGB\")\n",
    "                img_tensor = self.transform(img_pil) if self.transform else transforms.ToTensor()(img_pil) # Already on CPU\n",
    "            except Exception as e:\n",
    "                # It's good practice to log which image failed\n",
    "                print(f\"WARNING: Error loading image {image_path} in __getitem__: {e}. Returning zero tensor.\")\n",
    "                img_tensor = torch.zeros((3, 224, 224), dtype=torch.float32) # Already on CPU\n",
    "        \n",
    "        # Labels are typically created on CPU and moved to GPU later with the batch\n",
    "        label_tensor = torch.tensor(self.all_labels_array_source[label_idx], dtype=torch.float32) # Already on CPU\n",
    "        \n",
    "        return img_tensor, label_tensor\n",
    "    \n",
    "    def clear_cache_aggressively(self, reason=\"\"): # Added reason for logging\n",
    "        if not self.caching_is_active_for_retrieval: return\n",
    "        num_items = len(self.image_cache)\n",
    "        if num_items == 0: return\n",
    "\n",
    "        print(f\"Aggressively clearing image cache ({num_items} items, {self._calculate_cache_memory_usage_mb():.1f} MB) due to: {reason}...\")\n",
    "        \n",
    "        paths_to_remove = list(self.image_cache.keys()) \n",
    "        for p in paths_to_remove:\n",
    "            if p in self.image_cache:\n",
    "                tensor = self.image_cache.pop(p)\n",
    "                del tensor\n",
    "        \n",
    "        self.image_cache.clear() \n",
    "\n",
    "        print(f\"Cache dictionary cleared. Python-level items: {len(self.image_cache)}\")\n",
    "        \n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache() \n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        free_after_clear, total_mem = torch.cuda.mem_get_info(self.device_to_cache_to)\n",
    "        used_after_clear = (total_mem - free_after_clear) / (1024 * 1024)\n",
    "        reserved_pytorch = torch.cuda.memory_reserved(self.device_to_cache_to) / (1024 * 1024)\n",
    "        allocated_pytorch = torch.cuda.memory_allocated(self.device_to_cache_to) / (1024 * 1024)\n",
    "        print(f\"Aggressive clear: PyTorch Used: {used_after_clear:.1f}MB, Allocated: {allocated_pytorch:.1f}MB, Reserved: {reserved_pytorch:.1f}MB\")\n",
    "    \n",
    "    def _cache_image_paths_to_gpu(self, image_paths_to_cache, desc_prefix=\"Caching\"):\n",
    "        if not self.caching_is_active_for_retrieval or not image_paths_to_cache: \n",
    "            return False # Return a status indicating if an OOM and clear occurred\n",
    "\n",
    "        # --- Step 1: Determine paths that genuinely need caching (not already in self.image_cache) ---\n",
    "        # This is important because image_paths_to_cache might be the full set for the current epoch.\n",
    "        # If we cleared the cache, ALL of them will need caching.\n",
    "        # If we didn't clear, only some might.\n",
    "        paths_genuinely_needing_cache = [p for p in image_paths_to_cache if p not in self.image_cache]\n",
    "        \n",
    "        if not paths_genuinely_needing_cache:\n",
    "            # print(f\"Debug: All {len(image_paths_to_cache)} requested paths already in cache.\")\n",
    "            return False # No OOM, no new caching done\n",
    "\n",
    "        num_newly_cached = 0\n",
    "        oom_triggered_clear = False # Flag to indicate if OOM caused a full cache clear\n",
    "        \n",
    "        # --- Step 2: Parallel CPU load and transform for paths_genuinely_needing_cache ---\n",
    "        cpu_processed_images = {}\n",
    "        actual_workers = min(self.num_cache_workers, len(paths_genuinely_needing_cache))\n",
    "        if actual_workers <= 0: actual_workers = 1\n",
    "\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=actual_workers) as executor:\n",
    "            future_to_path = {executor.submit(self._load_and_transform_image_cpu, path): path for path in paths_genuinely_needing_cache}\n",
    "            iterator = concurrent.futures.as_completed(future_to_path)\n",
    "            if len(paths_genuinely_needing_cache) > actual_workers * 5:\n",
    "                 iterator = tqdm(iterator, total=len(paths_genuinely_needing_cache), \n",
    "                                 desc=f\"{desc_prefix} CPU PreProc ({self.__class__.__name__})\", \n",
    "                                 leave=False, mininterval=1.0)\n",
    "            for future in iterator:\n",
    "                path = future_to_path[future]\n",
    "                try: \n",
    "                    _, img_tensor_cpu = future.result()\n",
    "                    if img_tensor_cpu is not None:\n",
    "                        cpu_processed_images[path] = img_tensor_cpu\n",
    "                except Exception as exc:\n",
    "                     print(f\"W: Path {path} generated an exception during CPU pre-processing: {exc}\")\n",
    "\n",
    "\n",
    "        # --- Step 3: Sequential GPU transfer ---\n",
    "        # Iterate over the original image_paths_to_cache because these are what the current epoch needs.\n",
    "        # We will try to load them if they were successfully preprocessed on CPU.\n",
    "        paths_to_attempt_gpu_transfer = [p for p in image_paths_to_cache if p in cpu_processed_images]\n",
    "\n",
    "        gpu_transfer_pbar_desc = f\"{desc_prefix} GPU Transfer ({self.__class__.__name__})\"\n",
    "        gpu_transfer_disable_pbar = len(paths_to_attempt_gpu_transfer) < 10\n",
    "        \n",
    "        for image_path in tqdm(paths_to_attempt_gpu_transfer, desc=gpu_transfer_pbar_desc, leave=False, mininterval=1.0, disable=gpu_transfer_disable_pbar):\n",
    "            if oom_triggered_clear: # If a clear already happened in this call, stop trying to add more.\n",
    "                                  # The caller (prime_cache_for_indices) will recall this function.\n",
    "                break \n",
    "\n",
    "            if image_path not in cpu_processed_images: continue # Should not happen if logic is correct\n",
    "\n",
    "            img_tensor_cpu = cpu_processed_images[image_path]\n",
    "            \n",
    "            # Pre-emptive check against overall cache limit\n",
    "            # This check is crucial before attempting the .to(device)\n",
    "            # if self.max_cache_memory_mb > 0:\n",
    "            #     current_cache_usage_mb_before_add = self._calculate_cache_memory_usage_mb()\n",
    "            #     new_img_mem_mb = (img_tensor_cpu.nelement() * img_tensor_cpu.element_size()) / (1024 * 1024)\n",
    "                \n",
    "            #     if current_cache_usage_mb_before_add + new_img_mem_mb > self.max_cache_memory_mb and len(self.image_cache) > 0 : # Check if cache is not empty\n",
    "            #         print(f\"W: Pre-emptive limit reached before caching {image_path}. \"\n",
    "            #               f\"Current cache ({current_cache_usage_mb_before_add:.1f}MB) + new ({new_img_mem_mb:.1f}MB) \"\n",
    "            #               f\"would exceed target ({self.max_cache_memory_mb:.1f}MB).\")\n",
    "            #         self.clear_cache_aggressively(reason=\"Pre-emptive limit in _cache_image_paths_to_gpu\")\n",
    "            #         oom_triggered_clear = True # Signal that a clear happened\n",
    "            #         # After clearing, we need to restart the caching process for the current epoch's items.\n",
    "            #         # So we break here, and the caller (prime_cache_for_indices) will handle re-calling.\n",
    "            #         break \n",
    "\n",
    "            try:\n",
    "                with torch.no_grad():\n",
    "                    self.image_cache[image_path] = img_tensor_cpu.to(self.device_to_cache_to, non_blocking=True)\n",
    "                num_newly_cached += 1\n",
    "            except RuntimeError as e:\n",
    "                if \"out of memory\" in str(e).lower():\n",
    "                    print(f\"W: GPU OOM during {desc_prefix} for {image_path}. Newly cached in this attempt before OOM: {num_newly_cached}.\")\n",
    "                    if image_path in self.image_cache: del self.image_cache[image_path] # Clean up failed current attempt\n",
    "                    # Don't call torch.cuda.empty_cache() yet, clear_cache_aggressively will do it more thoroughly\n",
    "                    \n",
    "                    self.clear_cache_aggressively(reason=f\"OOM for {image_path} in _cache_image_paths_to_gpu\")\n",
    "                    oom_triggered_clear = True # Signal that a clear happened\n",
    "                    break # Stop this GPU transfer loop. Caller will re-initiate.\n",
    "                else:\n",
    "                    print(f\"W: RuntimeErr during GPU transfer for {image_path}: {e}.\")\n",
    "            except Exception as e:\n",
    "                print(f\"W: General Err during GPU transfer for {image_path}: {e}.\")\n",
    "        \n",
    "        # --- Step 4: Summary Logging ---\n",
    "        # This logging might be a bit complex if a clear happened mid-way.\n",
    "        # The num_newly_cached would refer to items added *before* any clear in this specific call.\n",
    "        if num_newly_cached > 0 or oom_triggered_clear or len(cpu_processed_images) < len(paths_genuinely_needing_cache):\n",
    "            current_cache_total_mb = self._calculate_cache_memory_usage_mb()\n",
    "            print(f\"{desc_prefix} Summary ({self.__class__.__name__}): \"\n",
    "                  f\"{len(cpu_processed_images)}/{len(paths_genuinely_needing_cache)} CPU pre-processed. \"\n",
    "                  f\"{num_newly_cached} new to GPU (this attempt). \" # Corrected wording\n",
    "                  f\"Total GPU cache now: {len(self.image_cache)} items ({current_cache_total_mb:.1f}MB).\")\n",
    "        \n",
    "        if oom_triggered_clear:\n",
    "            print(\"Caching process in _cache_image_paths_to_gpu was interrupted by OOM/limit, full cache clear performed.\")\n",
    "        \n",
    "        return oom_triggered_clear # Return status\n",
    "\n",
    "    def prime_cache_for_indices(self, indices):\n",
    "        if not self.caching_is_active_for_retrieval:\n",
    "            self.current_epoch_image_paths = set()\n",
    "            return\n",
    "        \n",
    "        valid_indices = [i for i in indices if 0 <= i < len(self.items_list)]\n",
    "        new_epoch_image_paths = set()\n",
    "        if valid_indices:\n",
    "            new_epoch_image_paths = set(self.items_list[i]['image_path'] for i in valid_indices)\n",
    "\n",
    "        # Option 1: Evict based on previous epoch's paths first (your current logic)\n",
    "        # This is less relevant if _cache_image_paths_to_gpu itself will clear on OOM.\n",
    "        # self._manage_cache_capacity(current_epoch_image_paths_set=self.current_epoch_image_paths) \n",
    "\n",
    "        self.current_epoch_image_paths = new_epoch_image_paths # Update to current epoch's needs\n",
    "        \n",
    "        if self.current_epoch_image_paths:\n",
    "            print(f\"Priming cache for {len(self.current_epoch_image_paths)} unique paths for current epoch/subset...\")\n",
    "            # First attempt to cache\n",
    "            oom_and_cleared = self._cache_image_paths_to_gpu(list(self.current_epoch_image_paths), desc_prefix=\"Epoch subset priming (Attempt 1)\")\n",
    "            \n",
    "            if oom_and_cleared:\n",
    "                # If an OOM occurred AND the cache was cleared from within _cache_image_paths_to_gpu,\n",
    "                # we need to re-attempt caching the current epoch's items into the now (hopefully) emptier cache.\n",
    "                print(\"Retrying cache population for current epoch after aggressive clear...\")\n",
    "                # The self.image_cache is now empty or much smaller.\n",
    "                # self.current_epoch_image_paths still holds the paths we need for *this* epoch.\n",
    "                self._cache_image_paths_to_gpu(list(self.current_epoch_image_paths), desc_prefix=\"Epoch subset priming (Attempt 2 Post-Clear)\")\n",
    "        \n",
    "        # After all attempts, ensure the overall cache policy is still met.\n",
    "        # This handles cases where the second attempt might still overfill if max_cache_memory_mb is extremely small.\n",
    "        self._manage_cache_capacity(current_epoch_image_paths_set=self.current_epoch_image_paths)\n",
    "\n",
    "\n",
    "train_transform=transforms.Compose([transforms.Resize((256,256)),transforms.RandomCrop(224),transforms.RandomHorizontalFlip(),transforms.ColorJitter(0.1,0.1,0.1),transforms.ToTensor(),transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])])\n",
    "val_transform=transforms.Compose([transforms.Resize((224,224)),transforms.ToTensor(),transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])])\n",
    "\n",
    "\n",
    "train_loader,val_loader=None,None\n",
    "train_dataset_instance,val_dataset_instance=None,None\n",
    "actual_num_workers=NUM_WORKERS_ARG\n",
    "\n",
    "\n",
    "if dataset_items and all_labels_array.size > 0:\n",
    "    unique_dish_ids = sorted(list(set(item['dish_id'] for item in dataset_items)))\n",
    "    train_dish_ids_split, val_dish_ids_split = (train_test_split(unique_dish_ids, test_size=0.2, random_state=42)\n",
    "                                                if len(unique_dish_ids) >= 2 else (unique_dish_ids, []))\n",
    "    train_items_final = [item for item in dataset_items if item['dish_id'] in train_dish_ids_split]\n",
    "    val_items_final = [item for item in dataset_items if item['dish_id'] in val_dish_ids_split]\n",
    "\n",
    "    # --- RESTRUCTURED LOGIC ---\n",
    "    user_wants_caching_on_cuda = GPU_CACHING_REQUESTED_ARG and DEVICE.type == 'cuda'\n",
    "    actual_num_workers = NUM_WORKERS_ARG  # Start with the argument value\n",
    "\n",
    "    attempt_initial_cache_train, attempt_initial_cache_val = False, False\n",
    "    train_cache_estimate_mb = 0.0\n",
    "\n",
    "    if user_wants_caching_on_cuda:\n",
    "        print(\"\\n--- GPU Caching Pre-check for Initial Full Cache Attempts ---\")\n",
    "        if NUM_WORKERS_ARG > 0:\n",
    "            print(f\"INFO: GPU caching is active, forcing num_workers from {NUM_WORKERS_ARG} to 0.\")\n",
    "            actual_num_workers = 0 # Force to 0 if GPU caching is active\n",
    "\n",
    "        if train_items_final:\n",
    "            num_unique_train = len(set(i['image_path'] for i in train_items_final))\n",
    "            if num_unique_train > 0:\n",
    "                can_cache_train_ind, train_cache_est_actual = check_memory_for_caching(\n",
    "                    num_unique_train, device=DEVICE, return_estimate=True, context=\"Train Set(Individual)\")\n",
    "                if can_cache_train_ind:\n",
    "                    attempt_initial_cache_train = True\n",
    "                    train_cache_estimate_mb = train_cache_est_actual\n",
    "            else: # num_unique_train is 0\n",
    "                attempt_initial_cache_train = True\n",
    "\n",
    "        if attempt_initial_cache_train and val_items_final:\n",
    "            num_unique_val = len(set(i['image_path'] for i in val_items_final))\n",
    "            if num_unique_val > 0:\n",
    "                attempt_initial_cache_val = check_memory_for_caching(\n",
    "                    num_unique_val, device=DEVICE, return_estimate=False,\n",
    "                    subtract_from_free_mb=train_cache_estimate_mb, context=\"Val Set(w/Train)\")\n",
    "            else: # num_unique_val is 0\n",
    "                attempt_initial_cache_val = True\n",
    "        # else: Val caching not attempted if train caching not attempted or no val items\n",
    "        # attempt_initial_cache_val remains False if train caching failed/not attempted\n",
    "\n",
    "        print(f\"Initial cache decision: Train={attempt_initial_cache_train}, Val={attempt_initial_cache_val} (Val depends on Train fitting).\")\n",
    "        print(\"--- End GPU Caching Pre-check ---\\n\")\n",
    "    elif GPU_CACHING_REQUESTED_ARG: # Caching requested but device not CUDA\n",
    "        print(\"INFO: GPU caching requested, but DEVICE is not CUDA. All caching disabled.\")\n",
    "        # actual_num_workers remains NUM_WORKERS_ARG in this case\n",
    "\n",
    "    # Now, determine pin_memory_setting based on the final actual_num_workers\n",
    "    pin_memory_setting = (DEVICE.type == 'cuda' and actual_num_workers > 0)\n",
    "\n",
    "    # Create Datasets and DataLoaders (this part is now outside the caching pre-check conditional)\n",
    "    if train_items_final:\n",
    "        train_dataset_instance = NutritionDataset(\n",
    "            train_items_final, all_labels_array, train_transform,\n",
    "            user_requests_gpu_caching=user_wants_caching_on_cuda,\n",
    "            attempt_initial_full_cache_for_this_set=attempt_initial_cache_train,\n",
    "            max_cache_utilization_factor=0.7 # Or make this an argparse parameter\n",
    "        )\n",
    "        \n",
    "        # train_dataset_instance = NutritionDataset(\n",
    "        #     train_items_final, all_labels_array, train_transform,\n",
    "        #     user_requests_gpu_caching=user_wants_caching_on_cuda, # Pass the flag for dataset's internal logic\n",
    "        #     attempt_initial_full_cache_for_this_set=attempt_initial_cache_train\n",
    "        # )\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset_instance, batch_size=BATCH_SIZE,\n",
    "            shuffle=(EPOCH_DATA_FRACTION_ARG == 1.0),\n",
    "            num_workers=actual_num_workers,\n",
    "            pin_memory=pin_memory_setting\n",
    "        )\n",
    "        print(f\"Base Train DataLoader created with {len(train_dataset_instance)} items. Workers: {actual_num_workers}, Pin memory: {pin_memory_setting}\")\n",
    "\n",
    "    if val_items_final:\n",
    "        val_dataset_instance = NutritionDataset(\n",
    "            val_items_final, all_labels_array, val_transform,\n",
    "            user_requests_gpu_caching=user_wants_caching_on_cuda,\n",
    "            attempt_initial_full_cache_for_this_set=attempt_initial_cache_val,\n",
    "            max_cache_utilization_factor=0.7 # Or make this an argparse parameter\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset_instance, batch_size=BATCH_SIZE,\n",
    "            shuffle=False,\n",
    "            num_workers=actual_num_workers,\n",
    "            pin_memory=pin_memory_setting\n",
    "        )\n",
    "        print(f\"Base Val DataLoader created with {len(val_dataset_instance)} items. Workers: {actual_num_workers}, Pin memory: {pin_memory_setting}\")\n",
    "\n",
    "else:\n",
    "    print(\"CRITICAL: Skipping DataLoader creation: no data items or labels.\")\n",
    "    # Consider sys.exit(1) here if DataLoaders are essential\n",
    "    train_loader, val_loader = None, None # Ensure they are defined as None if not created\n",
    "    train_dataset_instance, val_dataset_instance = None, None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Selection and Optimizer\n",
    "# ----------------------------------------------------------------------------\n",
    "model_configs = {\n",
    "    'SimpleConvNet':SimpleConvNet,\n",
    "    'DeepConvNet':DeepConvNet,\n",
    "    'MobileNetLike':MobileNetLike,\n",
    "    'ResNetFromScratch': lambda num_outputs: ResNetFromScratch(num_outputs=num_outputs, use_pretrained=False),\n",
    "    'ResNetPretrained': lambda num_outputs: ResNetFromScratch(num_outputs=num_outputs, use_pretrained=True),\n",
    "    }\n",
    "model=model_configs[MODEL_NAME_ARG](num_outputs=len(TARGET_COLUMNS)).to(DEVICE)\n",
    "print(f\"Selected model: {MODEL_NAME_ARG}, Trainable Params: {sum(p.numel()for p in model.parameters()if p.requires_grad):,}\")\n",
    "criterion=nn.L1Loss()\n",
    "optimizer=optim.Adam(model.parameters(),lr=LEARNING_RATE);scheduler=optim.lr_scheduler.ReduceLROnPlateau(optimizer,'min',patience=max(1,EARLY_STOPPING_PATIENCE_ARG//2),factor=0.5,verbose=True) # Scheduler patience related to early stopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Validation Loop\n",
    "# ----------------------------------------------------------------------------\n",
    "def train_epoch(model,loader,criterion,optimizer,device):\n",
    "    model.train();total_loss,count=0,0;pbar=tqdm(loader,desc='Training',leave=False,disable=len(loader)==0,mininterval=0.5)\n",
    "    for img,lbl in pbar:img,lbl=img.to(device),lbl.to(device);optimizer.zero_grad();out=model(img);loss=criterion(out,lbl);loss.backward();optimizer.step();total_loss+=loss.item();count+=1;pbar.set_postfix({'loss':f'{loss.item():.4f}'})\n",
    "    return total_loss/count if count>0 else float('nan')\n",
    "def validate(model,loader,criterion,device, desc=\"Validating\"): # Added desc\n",
    "    model.eval();total_loss,count=0,0;all_preds,all_lbls=[],[]\n",
    "    if not loader or len(loader)==0:return float('nan'),{c:float('nan')for c in TARGET_COLUMNS},np.empty((0,len(TARGET_COLUMNS))),np.empty((0,len(TARGET_COLUMNS)))\n",
    "    pbar=tqdm(loader,desc=desc,leave=False,disable=len(loader)==0,mininterval=0.5)\n",
    "    with torch.no_grad():\n",
    "        for img,lbl_b in pbar:img,lbl_b=img.to(device),lbl_b.to(device);out=model(img);loss=criterion(out,lbl_b);total_loss+=loss.item();count+=1;all_preds.append(out.cpu().numpy());all_lbls.append(lbl_b.cpu().numpy());pbar.set_postfix({'loss':f'{loss.item():.4f}'})\n",
    "    avg_loss=total_loss/count if count>0 else float('nan')\n",
    "    if not all_preds:return avg_loss,{c:float('nan')for c in TARGET_COLUMNS},np.empty((0,len(TARGET_COLUMNS))),np.empty((0,len(TARGET_COLUMNS)))\n",
    "    preds_np,lbls_np=np.concatenate(all_preds),np.concatenate(all_lbls);errors={}\n",
    "    for i,col in enumerate(TARGET_COLUMNS):\n",
    "        if lbls_np.shape[0]>0:mae=mean_absolute_error(lbls_np[:,i],preds_np[:,i]);mean_val=np.mean(lbls_np[:,i]);errors[col]=(mae/(np.abs(mean_val)+1e-9))*100 if mean_val!=0 else(float('inf')if mae>1e-9 else 0.0)\n",
    "        else:errors[col]=float('nan')\n",
    "    return avg_loss,errors,preds_np,lbls_np\n",
    "\n",
    "if train_loader and train_dataset_instance:\n",
    "    best_val_loss=float('inf');epochs_no_improve=0;history={'train_loss':[],'val_loss':[],'percentage_errors':[],'lr':[]}\n",
    "    MODEL_SAVE_PATH=os.path.join(OUTPUT_DIR,f'best_nutrition_model_{MODEL_NAME_ARG}.pth');HISTORY_SAVE_PATH=os.path.join(OUTPUT_DIR,f'training_history_{MODEL_NAME_ARG}.pkl')\n",
    "    print(\"=\"*60+f\"\\nSTARTING TRAINING - {MODEL_NAME_ARG}\\n\"+\"=\"*60)\n",
    "    try:\n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            epoch_start_time=time.time();current_lr=optimizer.param_groups[0]['lr']\n",
    "            print(f\"\\nEPOCH {epoch+1}/{NUM_EPOCHS} | LR: {current_lr:.6f} | Best Val Loss: {best_val_loss:.4f} | Epochs no improve: {epochs_no_improve}\" + \"-\"*20)\n",
    "            current_epoch_train_loader=train_loader\n",
    "            # if epoch > 0 and epoch % 10 == 0: # Example: every 10 epochs\n",
    "            #     if hasattr(train_dataset_instance, 'clear_cache_aggressively') and train_dataset_instance.caching_is_active_for_retrieval:\n",
    "            #         print(f\"\\n--- Performing aggressive cache clear for train_dataset_instance at epoch {epoch+1} ---\")\n",
    "            #         train_dataset_instance.clear_cache_aggressively()\n",
    "            #     if hasattr(val_dataset_instance, 'clear_cache_aggressively') and val_dataset_instance.caching_is_active_for_retrieval:\n",
    "            #         print(f\"\\n--- Performing aggressive cache clear for val_dataset_instance at epoch {epoch+1} ---\")\n",
    "            #         val_dataset_instance.clear_cache_aggressively()\n",
    "            #     # After clearing, you MUST re-prime if you expect items to be in cache for the current epoch\n",
    "            #     # The existing priming logic in your loop should handle this.\n",
    "            if EPOCH_DATA_FRACTION_ARG<1.0 and len(train_dataset_instance)>0:\n",
    "                full_len=len(train_dataset_instance);samples_count=max(1,int(EPOCH_DATA_FRACTION_ARG*full_len))\n",
    "                epoch_indices=torch.randperm(full_len).tolist()[:samples_count]\n",
    "                if train_dataset_instance.caching_is_active_for_retrieval:train_dataset_instance.prime_cache_for_indices(epoch_indices)\n",
    "                current_epoch_train_loader=DataLoader(train_dataset_instance,batch_size=BATCH_SIZE,sampler=SubsetRandomSampler(epoch_indices),num_workers=actual_num_workers,pin_memory=pin_memory_setting)\n",
    "            elif train_dataset_instance.caching_is_active_for_retrieval:train_dataset_instance.prime_cache_for_indices(list(range(len(train_dataset_instance))))\n",
    "            train_loss=train_epoch(model,current_epoch_train_loader,criterion,optimizer,DEVICE)\n",
    "\n",
    "            current_epoch_val_loader=val_loader;val_loss,percentage_errors=float('nan'),{c:float('nan')for c in TARGET_COLUMNS}\n",
    "            if val_dataset_instance and len(val_dataset_instance)>0:\n",
    "                if EPOCH_DATA_FRACTION_ARG<1.0:\n",
    "                    full_len_val=len(val_dataset_instance);samples_count_val=max(1,int(EPOCH_DATA_FRACTION_ARG*full_len_val))\n",
    "                    epoch_indices_val=torch.randperm(full_len_val).tolist()[:samples_count_val]\n",
    "                    if val_dataset_instance.caching_is_active_for_retrieval:val_dataset_instance.prime_cache_for_indices(epoch_indices_val)\n",
    "                    current_epoch_val_loader=DataLoader(val_dataset_instance,batch_size=BATCH_SIZE,sampler=SubsetRandomSampler(epoch_indices_val),num_workers=actual_num_workers,pin_memory=pin_memory_setting)\n",
    "                elif val_dataset_instance.caching_is_active_for_retrieval:val_dataset_instance.prime_cache_for_indices(list(range(len(val_dataset_instance))))\n",
    "                if current_epoch_val_loader and len(current_epoch_val_loader)>0:val_loss,percentage_errors,_,_=validate(model,current_epoch_val_loader,criterion,DEVICE, desc=\"Validating (Epoch Subset)\")\n",
    "            elif val_loader and len(val_loader)>0:val_loss,percentage_errors,_,_=validate(model,val_loader,criterion,DEVICE, desc=\"Validating (Full Set)\")\n",
    "            \n",
    "            if not np.isnan(val_loss):scheduler.step(val_loss)\n",
    "            history['train_loss'].append(train_loss);history['val_loss'].append(val_loss);history['percentage_errors'].append(percentage_errors);history['lr'].append(current_lr)\n",
    "            print(f\"\\n{'='*20} EPOCH {epoch+1} RESULTS {'='*20}\")\n",
    "            print(f\"Train Loss: {train_loss:.4f} | Val Loss (epoch subset): {val_loss:.4f} | Time: {time.time()-epoch_start_time:.2f}s\")\n",
    "            print(\"Percentage Errors (on epoch val subset):\");[print(f\"  {n:20s}:{e_v:6.2f}%\")for n,e_v in percentage_errors.items()]\n",
    "            if not np.isnan(val_loss):\n",
    "                if val_loss<best_val_loss:best_val_loss=val_loss;epochs_no_improve=0;torch.save({'epoch':epoch,'model_state_dict':model.state_dict(),'optimizer_state_dict':optimizer.state_dict(),'scheduler_state_dict':scheduler.state_dict(),'best_val_loss':best_val_loss,'model_name':MODEL_NAME_ARG,'history':history},MODEL_SAVE_PATH);print(f\"✓ NEW BEST MODEL (epoch val) SAVED to {MODEL_SAVE_PATH}\")\n",
    "                else:epochs_no_improve+=1\n",
    "            if EARLY_STOPPING_PATIENCE_ARG > 0 and epochs_no_improve >= EARLY_STOPPING_PATIENCE_ARG:print(f\"\\nEarly stopping triggered after {EARLY_STOPPING_PATIENCE_ARG} epochs without improvement on validation loss.\");break\n",
    "            print(\"=\"*60)\n",
    "    except KeyboardInterrupt:print(f\"\\n⚠️ Training interrupted by user.\")\n",
    "    except Exception as e:print(f\"\\n❌ Error during training:{e}\");import traceback;traceback.print_exc()\n",
    "    finally:\n",
    "        print(\"\\n\" + \"=\" * 20 + f\" TRAINING SUMMARY - {MODEL_NAME_ARG} \" + \"=\" * 20)\n",
    "        if history['train_loss']:\n",
    "            final_tl = history['train_loss'][-1]\n",
    "            final_vl = history['val_loss'][-1] if history['val_loss'] else float('nan')\n",
    "            bvl_disp = best_val_loss if best_val_loss != float('inf') else float('nan')\n",
    "            print(f\"Final Train Loss: {final_tl:.4f}\")\n",
    "            print(f\"Final Val Loss (epoch subset): {final_vl:.4f}\")\n",
    "            print(f\"Best Val Loss (epoch subset): {bvl_disp:.4f}\")\n",
    "            with open(HISTORY_SAVE_PATH, 'wb') as f:\n",
    "                pickle.dump(history, f)\n",
    "            print(f\"History saved: {HISTORY_SAVE_PATH}\")\n",
    "else:print(\"Training skipped:train_loader or train_dataset_instance unavailable.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Training History\n",
    "# ----------------------------------------------------------------------------\n",
    "if 'history' in locals() and history['train_loss'] and SAVE_PLOTS:\n",
    "    fig,(ax1,ax2)=plt.subplots(1,2,figsize=(16,6)) # Removed squeeze=False as it's 1x2\n",
    "\n",
    "    # Plotting Training and Validation Loss on ax1\n",
    "    ax1.plot(history['train_loss'],label='Train Loss',color='royalblue',linewidth=2)\n",
    "    \n",
    "    # Ensure val_loss has same length or is handled properly if shorter (e.g. early stopping)\n",
    "    # Extract valid validation loss points for plotting\n",
    "    val_loss_series = pd.Series(history['val_loss']).dropna()\n",
    "    if not val_loss_series.empty:\n",
    "        ax1.plot(val_loss_series.index, val_loss_series.values, \n",
    "                 label='Val Loss(Epoch Subset)',color='darkorange',linestyle='--',linewidth=2)\n",
    "\n",
    "    ax1.set_xlabel('Epoch',fontsize=12)\n",
    "    ax1.set_ylabel('Loss',fontsize=12)\n",
    "    ax1.set_title(f'Training & Validation Loss ({MODEL_NAME_ARG})',fontsize=14)\n",
    "    if ax1.has_data(): # Check if any lines were plotted before adding legend\n",
    "        ax1.legend(fontsize=10)\n",
    "    ax1.grid(True,linestyle=':',alpha=0.7)\n",
    "\n",
    "    # Corrected plotting for percentage errors on ax2:\n",
    "    if history['percentage_errors']: # Check if the key exists\n",
    "        # Filter out epochs where percentage_errors might be None or not a dict\n",
    "        valid_percentage_errors = [item for item in history['percentage_errors'] if isinstance(item, dict)]\n",
    "        if valid_percentage_errors: # Proceed only if we have valid dicts\n",
    "            df_errors = pd.DataFrame(valid_percentage_errors) \n",
    "            # No need for .dropna(how='all') here if we pre-filtered, \n",
    "            # but it's fine if you want to keep it as an extra safeguard for fully NaN rows.\n",
    "            # df_errors = df_errors.dropna(how='all', axis=0) \n",
    "\n",
    "            if not df_errors.empty:\n",
    "                something_plotted_on_ax2 = False\n",
    "                for col in df_errors.columns:  # Iterate through each nutrient column\n",
    "                    series_col = df_errors[col].dropna() # Get data for the current nutrient, drop NaNs\n",
    "                    if not series_col.empty: # Check if there's any data left for this nutrient\n",
    "                        valid_error_indices = series_col.index.tolist()\n",
    "                        valid_error_values = series_col.values.tolist()\n",
    "                        \n",
    "                        ax2.plot(valid_error_indices, valid_error_values, \n",
    "                                 label=f\"{col.split('_')[0]} %Err\", alpha=0.8, linewidth=1.5)\n",
    "                        something_plotted_on_ax2 = True\n",
    "                \n",
    "                ax2.set_xlabel('Epoch', fontsize=12)\n",
    "                ax2.set_ylabel('Percentage Error(%)', fontsize=12)\n",
    "                ax2.set_title(f'Nutrient % Errors(Epoch Val Subset)', fontsize=14)\n",
    "                if something_plotted_on_ax2: # Add legend only if something was plotted\n",
    "                    ax2.legend(fontsize=9)\n",
    "                ax2.grid(True, linestyle=':', alpha=0.7)\n",
    "                \n",
    "                # Determine y_lim dynamically or use a robust fixed one\n",
    "                if something_plotted_on_ax2 and any(line.get_ydata().size > 0 for line in ax2.lines):\n",
    "                    current_ylim_top = ax2.get_ylim()[1]\n",
    "                    ax2.set_ylim(bottom=0, top=min(200, current_ylim_top if current_ylim_top > 0 else 200))\n",
    "                else:\n",
    "                    ax2.set_ylim(bottom=0, top=200) # Default if no data\n",
    "            else:\n",
    "                ax2.set_title('Nutrient % Errors - No Data')\n",
    "                ax2.axis('off')\n",
    "        else:\n",
    "            ax2.set_title('Nutrient % Errors - No Valid Error Data')\n",
    "            ax2.axis('off')\n",
    "    else: # history['percentage_errors'] key doesn't exist or is empty\n",
    "        ax2.set_title('Nutrient % Errors - History Key Missing')\n",
    "        ax2.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plot_path=os.path.join(OUTPUT_DIR,f\"plot_training_history_{MODEL_NAME_ARG}.png\")\n",
    "    plt.savefig(plot_path)\n",
    "    print(f\"Plot saved:{plot_path}\")\n",
    "    plt.close(fig)\n",
    "\n",
    "elif 'history'in locals()and history['train_loss']:print(\"SAVE_PLOTS False or history empty.Skip training history plot.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation on FULL Validation Set\n",
    "# ----------------------------------------------------------------------------\n",
    "MODEL_SAVE_PATH_EVAL = os.path.join(OUTPUT_DIR, f'best_nutrition_model_{MODEL_NAME_ARG}.pth')\n",
    "results_df = pd.DataFrame()\n",
    "print(\"\\n\" + \"=\"*30 + f\" FINAL EVALUATION ON FULL VALIDATION SET - {MODEL_NAME_ARG} \" + \"=\"*30)\n",
    "if os.path.exists(MODEL_SAVE_PATH_EVAL) and val_loader and val_dataset_instance and len(val_loader) > 0:\n",
    "    print(f\"Loading best model from {MODEL_SAVE_PATH_EVAL} for final evaluation...\")\n",
    "    checkpoint = torch.load(MODEL_SAVE_PATH_EVAL, map_location=DEVICE, weights_only=False)\n",
    "    state_dict = checkpoint.get('model_state_dict', checkpoint)\n",
    "    if all(k.startswith('module.') for k in state_dict.keys()):\n",
    "        state_dict = {k[7:]: v for k, v in state_dict.items()}\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "    print(f\"Model {MODEL_NAME_ARG} (epoch {checkpoint.get('epoch', 'N/A')}, best val loss {checkpoint.get('best_val_loss', 'N/A')}) loaded.\")\n",
    "    if val_dataset_instance.caching_is_active_for_retrieval:\n",
    "        print(\"Priming cache for full validation set...\")\n",
    "        val_dataset_instance.prime_cache_for_indices(list(range(len(val_dataset_instance))))\n",
    "    _, _, predictions_np_eval, labels_np_eval = validate(model, val_loader, criterion, DEVICE, desc=\"Validating (FULL SET)\")\n",
    "    if predictions_np_eval.shape[0] > 0:\n",
    "        num_eval_samples = len(predictions_np_eval)\n",
    "        eval_item_details = val_dataset_instance.items_list[:num_eval_samples]\n",
    "        all_dish_ids_eval = [item['dish_id'] for item in eval_item_details]\n",
    "        weight_map = metadata_for_final_eval_weights['weight'].to_dict()\n",
    "        all_weights_eval = [weight_map.get(did, np.nan) for did in all_dish_ids_eval]\n",
    "        results_df_data = {'dish_id': all_dish_ids_eval, 'weight': all_weights_eval}\n",
    "        for i, col_name in enumerate(TARGET_COLUMNS):\n",
    "            results_df_data[f'{col_name}_pred'] = predictions_np_eval[:, i]\n",
    "            results_df_data[f'{col_name}_true'] = labels_np_eval[:, i]\n",
    "        results_df = pd.DataFrame(results_df_data)\n",
    "        for nutr_root in ['calories', 'fat', 'carbs', 'protein']:\n",
    "            nutr_col = next((tc for tc in TARGET_COLUMNS if tc.startswith(nutr_root)), None)\n",
    "            if nutr_col and f'{nutr_col}_pred' in results_df.columns and 'weight' in results_df.columns:\n",
    "                results_df[f'{nutr_root}_abs_pred'] = results_df[f'{nutr_col}_pred'] * results_df['weight'] / 100\n",
    "                results_df[f'{nutr_root}_abs_true'] = results_df[f'{nutr_col}_true'] * results_df['weight'] / 100\n",
    "        print(f\"Final evaluation predictions completed for {len(results_df)} views on FULL validation set.\")\n",
    "        \n",
    "        # Save the results table to CSV\n",
    "        results_file = os.path.join(OUTPUT_DIR, f\"evaluation_results_{MODEL_NAME_ARG}.csv\")\n",
    "        results_df.to_csv(results_file, index=False, float_format='%.3f')\n",
    "        print(f\"Results saved to {results_file}\")\n",
    "    else:\n",
    "        print(\"Final evaluation skipped: No predictions from validate on full validation set.\")\n",
    "else:\n",
    "    if not os.path.exists(MODEL_SAVE_PATH_EVAL):\n",
    "        print(f\"Skip final eval: Model {MODEL_SAVE_PATH_EVAL} not found.\")\n",
    "    elif not (val_loader and val_dataset_instance and len(val_loader) > 0):\n",
    "        print(\"Skip final eval: Full val_loader unavailable/empty.\")\n",
    "\n",
    "# Metrics Calculation (on full validation set results)\n",
    "if not results_df.empty:\n",
    "    def calculate_metrics(df, target_cols_list, per_100g=True):\n",
    "        metrics_list = []\n",
    "        for ncol_key in target_cols_list:\n",
    "            true_c, pred_c = f'{ncol_key}_true', f'{ncol_key}_pred'\n",
    "            if not per_100g:\n",
    "                base_n = ncol_key.split('_per_100g')[0]\n",
    "                true_c, pred_c = f'{base_n}_abs_true', f'{base_n}_abs_pred'\n",
    "            if true_c not in df.columns or pred_c not in df.columns:\n",
    "                continue\n",
    "            valid_idx = df[[true_c, pred_c]].dropna().index\n",
    "            if len(valid_idx) == 0:\n",
    "                continue\n",
    "            true_v, pred_v = df.loc[valid_idx, true_c].values, df.loc[valid_idx, pred_c].values\n",
    "            R2_val = r2_score(true_v, pred_v) if len(true_v) >= 2 else np.nan\n",
    "            mae = mean_absolute_error(true_v, pred_v)\n",
    "            rmse = np.sqrt(mean_squared_error(true_v, pred_v))\n",
    "            mean_t = np.mean(true_v)\n",
    "            perc_err = (mae / (np.abs(mean_t) + 1e-9)) * 100 if mean_t != 0 else (float('inf') if mae > 1e-9 else 0.0)\n",
    "            metrics_list.append({\n",
    "                'Nutrient': ncol_key if per_100g else base_n.capitalize(),\n",
    "                'MAE': mae,\n",
    "                'RMSE': rmse,\n",
    "                'R²': R2_val,\n",
    "                '% Err': perc_err,\n",
    "                'Mean True': mean_t,\n",
    "                'Mean Pred': np.mean(pred_v)\n",
    "            })\n",
    "        return pd.DataFrame(metrics_list)\n",
    "    \n",
    "    print(\"\\nMetrics on FULL Validation Set (Per 100g):\")\n",
    "    metrics_df_100g = calculate_metrics(results_df, TARGET_COLUMNS, per_100g=True)\n",
    "    if not metrics_df_100g.empty:\n",
    "        print(metrics_df_100g.to_string(index=False, float_format='%.3f'))\n",
    "        # Save metrics per 100g to CSV\n",
    "        metrics_100g_file = os.path.join(OUTPUT_DIR, f\"metrics_per_100g_{MODEL_NAME_ARG}.csv\")\n",
    "        metrics_df_100g.to_csv(metrics_100g_file, index=False, float_format='%.3f')\n",
    "        print(f\"Metrics (per 100g) saved to {metrics_100g_file}\")\n",
    "    \n",
    "    print(\"\\nMetrics on FULL Validation Set (Absolute):\")\n",
    "    metrics_df_abs = calculate_metrics(results_df, TARGET_COLUMNS, per_100g=False)\n",
    "    if not metrics_df_abs.empty:\n",
    "        print(metrics_df_abs.to_string(index=False, float_format='%.3f'))\n",
    "        # Save absolute metrics to CSV\n",
    "        metrics_abs_file = os.path.join(OUTPUT_DIR, f\"metrics_absolute_{MODEL_NAME_ARG}.csv\")\n",
    "        metrics_df_abs.to_csv(metrics_abs_file, index=False, float_format='%.3f')\n",
    "        print(f\"Metrics (absolute) saved to {metrics_abs_file}\")\n",
    "else:\n",
    "    print(\"Skipping metrics: results_df from final evaluation empty.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics Calculation (on full validation set results)\n",
    "# ----------------------------------------------------------------------------\n",
    "if not results_df.empty:\n",
    "    def calculate_metrics(df,target_cols_list,per_100g=True):\n",
    "        metrics_list=[];\n",
    "        for ncol_key in target_cols_list:\n",
    "            true_c,pred_c=f'{ncol_key}_true',f'{ncol_key}_pred'\n",
    "            if not per_100g:base_n=ncol_key.split('_per_100g')[0];true_c,pred_c=f'{base_n}_abs_true',f'{base_n}_abs_pred'\n",
    "            if true_c not in df.columns or pred_c not in df.columns:continue\n",
    "            valid_idx=df[[true_c,pred_c]].dropna().index;\n",
    "            if len(valid_idx)==0:continue\n",
    "            true_v,pred_v=df.loc[valid_idx,true_c].values,df.loc[valid_idx,pred_c].values\n",
    "            R2_val=r2_score(true_v,pred_v)if len(true_v)>=2 else np.nan\n",
    "            mae=mean_absolute_error(true_v,pred_v);rmse=np.sqrt(mean_squared_error(true_v,pred_v));mean_t=np.mean(true_v);perc_err=(mae/(np.abs(mean_t)+1e-9))*100\n",
    "            metrics_list.append({'Nutrient':ncol_key if per_100g else base_n.capitalize(),'MAE':mae,'RMSE':rmse,'R²':R2_val,'% Err':perc_err,'Mean True':mean_t,'Mean Pred':np.mean(pred_v)})\n",
    "        return pd.DataFrame(metrics_list)\n",
    "    print(\"\\nMetrics on FULL Validation Set(Per 100g):\");metrics_df_100g=calculate_metrics(results_df,TARGET_COLUMNS,per_100g=True)\n",
    "    if not metrics_df_100g.empty:print(metrics_df_100g.to_string(index=False,float_format='%.3f'))\n",
    "    print(\"\\nMetrics on FULL Validation Set(Absolute):\");metrics_df_abs=calculate_metrics(results_df,TARGET_COLUMNS,per_100g=False)\n",
    "    if not metrics_df_abs.empty:print(metrics_df_abs.to_string(index=False,float_format='%.3f'))\n",
    "else:print(\"Skipping metrics:results_df from final evaluation empty.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Predictions vs Actual & Error Distributions (on full validation set results)\n",
    "# ----------------------------------------------------------------------------\n",
    "if not results_df.empty and SAVE_PLOTS:\n",
    "    # Predictions vs True Values plots\n",
    "    num_targets=len(TARGET_COLUMNS);ncols=2\n",
    "    nrows_pvsa=(num_targets+ncols-1)//ncols\n",
    "    fig_pvsa,axes_pvsa=plt.subplots(nrows_pvsa,ncols,figsize=(7*ncols,6*nrows_pvsa),squeeze=False)\n",
    "    axes_pvsa=axes_pvsa.flatten()\n",
    "    for i,nutr_key in enumerate(TARGET_COLUMNS):\n",
    "        ax=axes_pvsa[i]\n",
    "        true_col,pred_col=f'{nutr_key}_true',f'{nutr_key}_pred'\n",
    "        if true_col not in results_df.columns or pred_col not in results_df.columns:\n",
    "            ax.set_title(f'{nutr_key.replace(\"_per_100g\",\"\").capitalize()} - No Data') # Indicate missing data\n",
    "            ax.axis('off') # Turn off axis if no data\n",
    "            continue\n",
    "        plot_df=results_df[[true_col,pred_col]].dropna()\n",
    "        if plot_df.empty:\n",
    "            ax.set_title(f'{nutr_key.replace(\"_per_100g\",\"\").capitalize()} - No Valid Data') # Indicate no valid data after dropna\n",
    "            ax.axis('off')\n",
    "            continue\n",
    "        x_p,y_p=plot_df[true_col].values,plot_df[pred_col].values\n",
    "        ax.scatter(x_p,y_p,alpha=0.6,s=35,edgecolors='k',linewidth=0.5,color='cornflowerblue')\n",
    "        min_v_data,max_v_data=min(x_p.min(),y_p.min()),max(x_p.max(),y_p.max()) # Renamed to avoid conflict with potential plt.min/max\n",
    "        # Ensure min_v and max_v define a valid range for the identity line\n",
    "        if np.isfinite(min_v_data) and np.isfinite(max_v_data) and min_v_data <= max_v_data:\n",
    "             ax.plot([min_v_data,max_v_data],[min_v_data,max_v_data],'r--',lw=2.5)\n",
    "        else: # Handle cases with NaN or single point after dropna (though plot_df.empty should catch most)\n",
    "            # Fallback: plot identity line based on some reasonable default or just skip it\n",
    "            print(f\"Warning: Could not determine valid range for identity line for {nutr_key}.\")\n",
    "\n",
    "        r2_val=r2_score(x_p,y_p)if len(x_p)>1 else float('nan')\n",
    "        disp_n=nutr_key.replace('_per_100g','').replace('_',' ').capitalize()\n",
    "        ax.set_xlabel(f'True {disp_n}',fontsize=11)\n",
    "        ax.set_ylabel(f'Pred {disp_n}',fontsize=11)\n",
    "        ax.set_title(f'{disp_n} (R²={r2_val:.3f})',fontsize=13)\n",
    "        ax.grid(True,alpha=0.4,linestyle='--')\n",
    "    for j in range(num_targets,len(axes_pvsa)): # Delete unused subplots\n",
    "        fig_pvsa.delaxes(axes_pvsa[j])\n",
    "    plt.tight_layout(rect=[0,0,1,0.96])\n",
    "    fig_pvsa.suptitle(f'Predictions vs True Values (Full Val Set, {MODEL_NAME_ARG})',fontsize=16)\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR,f\"plot_final_preds_vs_actual_{MODEL_NAME_ARG}.png\"))\n",
    "    plt.close(fig_pvsa)\n",
    "    print(f\"Plot saved: plot_final_preds_vs_actual_{MODEL_NAME_ARG}.png\")\n",
    "\n",
    "    # Error Distribution plots\n",
    "    nrows_err=(num_targets+ncols-1)//ncols\n",
    "    fig_err,axes_err=plt.subplots(nrows_err,ncols,figsize=(9*ncols,5*nrows_err),squeeze=False)\n",
    "    axes_err=axes_err.flatten()\n",
    "    for i,nutr_key in enumerate(TARGET_COLUMNS):\n",
    "        ax=axes_err[i]\n",
    "        true_col,pred_col=f'{nutr_key}_true',f'{nutr_key}_pred'\n",
    "        if true_col not in results_df.columns or pred_col not in results_df.columns:\n",
    "            ax.set_title(f'{nutr_key.replace(\"_per_100g\",\"\").capitalize()} Error - No Data')\n",
    "            ax.axis('off')\n",
    "            continue\n",
    "        err_df=results_df[[true_col,pred_col]].dropna()\n",
    "        if err_df.empty:\n",
    "            ax.set_title(f'{nutr_key.replace(\"_per_100g\",\"\").capitalize()} Error - No Valid Data')\n",
    "            ax.axis('off')\n",
    "            continue\n",
    "        \n",
    "        errors_abs=err_df[pred_col]-err_df[true_col] # Absolute errors\n",
    "        # Relative errors, handle potential division by zero or near-zero in true values\n",
    "        rel_errors_raw = np.zeros_like(errors_abs, dtype=float)\n",
    "        valid_denom_mask = np.abs(err_df[true_col].values) > 1e-9 # Mask for valid denominators\n",
    "        rel_errors_raw[valid_denom_mask] = (errors_abs[valid_denom_mask] / err_df[true_col].values[valid_denom_mask]) * 100\n",
    "        rel_errors_raw[~valid_denom_mask] = np.nan # Set to NaN if denominator is too small\n",
    "        \n",
    "        rel_err_clean=pd.Series(rel_errors_raw).dropna() # Convert to Series to use .quantile() and handle NaNs\n",
    "        rel_err_clean=rel_err_clean[np.isfinite(rel_err_clean)] # Ensure finite values\n",
    "\n",
    "        if len(rel_err_clean)==0:\n",
    "            ax.set_title(f'{nutr_key.replace(\"_per_100g\",\"\").capitalize()} Error - No Finite Rel. Errors')\n",
    "            ax.axis('off')\n",
    "            continue\n",
    "            \n",
    "        # Clip outliers for histogram visualization (robust range)\n",
    "        q_low,q_high=rel_err_clean.quantile(0.02),rel_err_clean.quantile(0.98)\n",
    "        # Ensure q_low <= q_high, can happen if data is very sparse or mostly identical\n",
    "        if q_low > q_high: q_low, q_high = q_high, q_low \n",
    "        filt_errs=rel_err_clean[(rel_err_clean>=q_low)&(rel_err_clean<=q_high)]\n",
    "        \n",
    "        if len(filt_errs)==0: # If filtering removes all data (e.g., all values are identical outside quantiles)\n",
    "            filt_errs=rel_err_clean # Use original cleaned errors\n",
    "\n",
    "        if len(filt_errs) > 0: # Check again if there's anything to plot\n",
    "            ax.hist(filt_errs,bins=40,alpha=0.75,color='mediumseagreen',edgecolor='black')\n",
    "            ax.axvline(x=0,color='red',linestyle='--',lw=2,label='Zero Err')\n",
    "            ax.axvline(x=filt_errs.mean(),color='darkblue',linestyle='-',lw=2,label=f'Mean:{filt_errs.mean():.1f}%')\n",
    "            ax.legend()\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, \"No data for histogram after filtering\", ha='center', va='center')\n",
    "\n",
    "\n",
    "        disp_n=nutr_key.replace('_per_100g','').replace('_',' ').capitalize()\n",
    "        ax.set_xlabel('Relative Error (%)',fontsize=11)\n",
    "        ax.set_ylabel('Frequency',fontsize=11)\n",
    "        ax.set_title(f'{disp_n} Error Distribution',fontsize=13)\n",
    "        ax.grid(True,alpha=0.4,linestyle='--')\n",
    "    for j in range(num_targets,len(axes_err)): # Delete unused subplots\n",
    "        fig_err.delaxes(axes_err[j])\n",
    "    plt.tight_layout(rect=[0,0,1,0.96])\n",
    "    fig_err.suptitle(f'Error Distribution (Full Val Set, {MODEL_NAME_ARG})',fontsize=16)\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR,f\"plot_final_error_dist_{MODEL_NAME_ARG}.png\"))\n",
    "    plt.close(fig_err)\n",
    "    print(f\"Plot saved: plot_final_error_dist_{MODEL_NAME_ARG}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Sample Predictions with Images (on full validation set results)\n",
    "# ----------------------------------------------------------------------------\n",
    "if not results_df.empty and SAVE_PLOTS and val_dataset_instance and len(results_df)>=1:\n",
    "    def show_predictions_with_images(n_samples=6):\n",
    "        actual_n=min(n_samples,len(results_df));\n",
    "        if actual_n==0:print(\"No samples for image predictions plot.\");return\n",
    "        sample_indices_df=np.random.choice(len(results_df),actual_n,replace=False);ncols=min(3,actual_n);nrows=(actual_n+ncols-1)//ncols\n",
    "        fig,axes=plt.subplots(nrows,ncols,figsize=(6*ncols,6*nrows),squeeze=False);axes=axes.flatten()\n",
    "        for i_ax,ax_plot in enumerate(axes):\n",
    "            if i_ax>=actual_n:ax_plot.axis('off');continue\n",
    "            df_row_idx=sample_indices_df[i_ax];item_info=val_dataset_instance.items_list[df_row_idx];dish_id_p,img_path_p=item_info['dish_id'],item_info['image_path']\n",
    "            if not os.path.exists(img_path_p):ax_plot.text(0.5,0.5,f\"ImgNF:\\n{os.path.basename(img_path_p)}\",ha='center',va='center',fontsize=9);ax_plot.axis('off');continue\n",
    "            try:img=Image.open(img_path_p)\n",
    "            except Exception as e_img:ax_plot.text(0.5,0.5,f\"ErrLoadImg:\\n{os.path.basename(img_path_p)}\\n{e_img}\",ha='center',va='center',fontsize=9);ax_plot.axis('off');continue\n",
    "            ax_plot.imshow(img);ax_plot.axis('off');pred_txt,true_txt=\"Pred:\\n\",\"Actual(Err%):\\n\";row=results_df.iloc[df_row_idx]\n",
    "            for nutr_col in TARGET_COLUMNS:pred_v,true_v=row.get(f'{nutr_col}_pred',np.nan),row.get(f'{nutr_col}_true',np.nan);err_pct=abs(pred_v-true_v)/(abs(true_v)+1e-9)*100 if not(np.isnan(pred_v)or np.isnan(true_v))else np.nan;disp_n=nutr_col.split('_')[0].capitalize()[:3];pred_txt+=f\"{disp_n}:{pred_v:.0f}\\n\";true_txt+=f\"{disp_n}:{true_v:.0f}({err_pct:.1f}%)\\n\"\n",
    "            ax_plot.text(0.02,0.98,pred_txt,transform=ax_plot.transAxes,va='top',ha='left',fontsize=9,bbox=dict(boxstyle='round,pad=0.4',fc='powderblue',alpha=0.85));ax_plot.text(0.98,0.98,true_txt,transform=ax_plot.transAxes,va='top',ha='right',fontsize=9,bbox=dict(boxstyle='round,pad=0.4',fc='lightgoldenrodyellow',alpha=0.85));ax_plot.set_title(f\"Dish:{dish_id_p}\\n{os.path.basename(img_path_p)}\",fontsize=10,y=1.02) # Adjust title y\n",
    "        for j in range(actual_n,len(axes)):fig.delaxes(axes[j])\n",
    "        plt.tight_layout(rect=[0,0,1,0.95]);plt.suptitle(f'Sample Predictions (Full Val Set, {MODEL_NAME_ARG})',fontsize=16,y=0.99);sample_plot_path=os.path.join(OUTPUT_DIR,f\"plot_final_sample_preds_{MODEL_NAME_ARG}.png\");plt.savefig(sample_plot_path);plt.close(fig);print(f\"Plot saved:{sample_plot_path}\")\n",
    "    show_predictions_with_images(min(6,len(results_df)))\n",
    "elif not results_df.empty:print(\"SAVE_PLOTS False.Skip final sample predictions plot.\")\n",
    "elif results_df.empty and SAVE_PLOTS:print(\"Results_df empty.Skip final sample predictions plot.\")\n",
    "\n",
    "print(\"\\n--- Script Finished ---\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
